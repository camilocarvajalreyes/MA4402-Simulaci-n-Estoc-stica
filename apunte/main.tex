% Template:     Informe/Reporte LaTeX
% Documento:    Archivo principal
% Versión:      6.6.0 (19/09/2019)
% Codificación: UTF-8
%
% Autor: Pablo Pizarro R.
%        Facultad de Ciencias Físicas y Matemáticas
%        Universidad de Chile
%        pablo@ppizarror.com
%
% Manual template: [https://latex.ppizarror.com/informe]
% Licencia MIT:    [https://opensource.org/licenses/MIT]

% CREACIÓN DEL DOCUMENTO
\documentclass[letterpaper,11pt]{article} % Articulo tamaño carta, 11pt
\usepackage[utf8]{inputenc} % Codificación UTF-8


% INFORMACIÓN DEL DOCUMENTO
\def\titulodelinforme {Apuntes Simulación Estocástica}
\def\temaatratar {MA4402 - Simulación Estocástica: Teoría y Laboratorio}

\def\autordeldocumento {}%{Camilo Carvajal Reyes}
\def\nombredelcurso {Simulación Estocástica: Teoría y Laboratorio}
\def\codigodelcurso {MA4402}

\def\nombreuniversidad {Departamento de Ingeniería Matemática}
\def\nombrefacultad {}
\def\departamentouniversidad {}
\def\imagendepartamento {dim}
\def\imagendepartamentoescala {0.2}
\def\localizacionuniversidad {}

% INTEGRANTES, PROFESORES Y FECHAS
\def\tablaintegrantes {
\begin{tabular}{ll}
	{}
	& \begin{tabular}[t]{l}
		{}%Camilo Carvajal Reyes
	\end{tabular} \\
	Profesor:
	%& \begin{tabular}[t]{l}
		Joaquín Fontbona
	%\end{tabular} \\
	%& \\
	\multicolumn{2}{l}{\today} \\
	\multicolumn{2}{l}{\localizacionuniversidad}
\end{tabular}}{
}

% CONFIGURACIONES
\input{lib/config}

% IMPORTACIÓN DE LIBRERÍAS
\input{lib/env/imports}

% IMPORTACIÓN DE FUNCIONES Y ENTORNOS
\input{lib/cmd/all}

% IMPORTACIÓN DE ESTILOS
\input{lib/style/all}

% CONFIGURACIÓN INICIAL DEL DOCUMENTO
\input{lib/cfg/init}

\usepackage{amsthm}

\def\espacio{\hspace{.25cm}\,}

% leftbar
\newlength{\leftbarwidth}
\setlength{\leftbarwidth}{1pt}
\newlength{\leftbarsep}
\setlength{\leftbarsep}{10pt}

\newcommand*{\leftbarcolorcmd}{\color{leftbarcolor}}% as a command to be more flexible
\colorlet{leftbarcolor}{black}

\renewenvironment{leftbar}{%
    \def\FrameCommand{{\leftbarcolorcmd{\vrule width \leftbarwidth\relax\hspace {\leftbarsep}}}}%
    \MakeFramed {\advance \hsize -\width \FrameRestore }%
}{%
    \endMakeFramed
}

%\theoremstyle{definition}
\newtheoremstyle{defbreak}%
    {}{}%
    {}{}%
    {\bfseries}{}% % Note that final punctuation is omitted.
    {\newline}
    {\thmname{#1}\thmnumber{ #2}\thmnote{ #3}}
\theoremstyle{defbreak}
\newtheorem{definition}{Definición}[subsection]
\newtheorem{example}{Ejemplo}[subsection]

\newtheoremstyle{propbreak}%
    {}{}%
    {}{}%
    {\bfseries}{}% % Note that final punctuation is omitted.
    {\newline}{}
\theoremstyle{propbreak}
\newtheorem{property}{Propiedad}[subsection]


\theoremstyle{remark}
\newtheorem{remark}{Observación}[subsection]
\newtheorem*{notation}{Notación}

%\theoremstyle{plain}
%\newtheorem{property}{Proposición}[subsection]
%\newtheorem{corolary}{Corolario}[subsection]
%\newtheorem{result}{Resultado}[subsection]
%\newtheorem{theorem}{Teorema}[subsection]
%\newtheorem{lemma}{Lema}[subsection]

\newtheoremstyle{break}%
    {}{}%
    {\itshape}{}%
    {\bfseries}{}% % Note that final punctuation is omitted.
    {\newline}
    {\thmname{#1}\thmnumber{ #2}\thmnote{ #3}}

\theoremstyle{break}
\newtheorem{theorem}{Teorema}[subsection]
\newtheorem{proposition}{Proposición}[subsection]
\newtheorem{corolary}{Corolario}[subsection]
\newtheorem{result}{Resultado}[subsection]
\newtheorem{lemma}{Lema}[subsection]
\newtheorem{algorithm}{Algoritmo}[subsection]
\def\KwIn{\textbf{Dado}:\espacio}

\renewcommand{\qedsymbol}{}

%---------------------------------
\usepackage{tikz}
\usetikzlibrary{positioning,calc}

%\usepackage{mathbbol}
%+\mathbb{1}+
%\def\bbone{\mathbb{1}}
\usepackage{unicode-math}

\usepackage{centernot}

\definecolor{mygray}{gray}{0.25}

\iffalse
\section{Definiciones latex}
%%%%%%%%%%%%%% cosas recurrentes %%%%%%%%%%%%%%%%%%%
%Sea $X:\Omega \longrightarrow E $ v.a., 
% $f:(E,\Sigma)\longrightarrow (\mathbb{R},\mathcal{B(\mathbb{R})})$
% $f\in L^1(E,\Sigma,\mu)$
% $$ \mathbb{E}(f(X)) = \langle \mu,f \rangle, \forall f \in L^1(\mu)$$
% Sea $(\Omega,\mathcal{F},\mathbb{P})$ e.d.p.
% $\mu_n \mbox{ }\substack{\Longrightarrow \\n \to \infty}\mbox{ } \mu$ convergencia débil
% \mbox{ }\substack{\longrightarrow \\ n\to\infty}\mbox{ } convergencia normal
% $X_n\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X$ conv en ley
% \color{red} RELLENAR \color{mygray}
\fi

% definiciones
\def\ssi{\Longleftrightarrow}
\def\R{\mathbb{R}}
\def\Rd{\mathbb{R}^d}
\def\E{\mathbb{E}}
\def\P{\mathbb{P}}
\def\N{\mathbb{N}}
\def\normal{\mathcal{N}}
\def\unif{\mathbb{U}([0,1])}
\def\var{Var}
\def\cov{Cov}
\newcommand{\indep}{\perp \!\!\! \perp}
\def\vas{X_1,X_2,\dots,X_n,\dots}
\def\conv{\mbox{ }\substack{\longrightarrow \\ n\to\infty}\mbox{ }}
\def\convt{\mbox{ }\substack{\longrightarrow \\ t\to\infty}\mbox{ }}
\def\convtzero{\mbox{ }\substack{\longrightarrow \\ t\to0}\mbox{ }}
\def\convley{\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }}
\def\convdebil{\mbox{ }\substack{\Longrightarrow \\n \to \infty}\mbox{ }}
\def\convcs{\mbox{ }\overset{c.s.}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }}
\def\convldos{\mbox{ }\overset{L^2}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }}
\def\convldos{\mbox{ }\overset{L^1}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }}
\def\convcst{\mbox{ }\overset{c.s.}{\substack{\longrightarrow \\t \to \infty}}\mbox{ }}
\def\convp{\mbox{ }\overset{\mathbb{P}}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }}
\def\searroweps{\,\substack{\searrow \\ \tiny{\epsilon\to\infty}}\,}
\def\nearroweps{\,\substack{\nearrow \\ \tiny{\epsilon\to\infty}}\,}
\def\nearrown{\,\substack{\nearrow \\ \tiny{n\to\infty}}\,}
\def\nearrowk{\,\substack{\nearrow \\ \tiny{k\to\infty}}\,}
\def\searrown{\,\substack{\searrow \\ \tiny{n\to\infty}}\,}
\def\searrowk{\,\substack{\searrow \\ \tiny{k\to\infty}}\,}
\def\edp{(\Omega,\mathcal{F},\mathbb{P})}
\def\rellenar{\color{red}RELLENAR\color{black}}
\def\rellenardem{\color{red}RELLENAR\color{mygray}}
\def\apunte{\color{teal}En apuntes físicos\color{black}}
\def\ejercicio{\color{blue}Ejercicio\color{black}}
\def\sumfx{\displaystyle\frac{1}{n}\sum^n_{k=1}f(X_k)}
\def\mean{\displaystyle\frac{1}{n}\sum^n_{k=1}}
\def\iid{\mbox{ i.i.d. }}
\def\tq{\mbox{ tal que }}
\def\beforeitemize{\leavevmode \vspace{-0.5\baselineskip}}
\def\gris{\color{mygray}}
\def\negro{\color{black}}
\def\rojo{\color{red}}
\def\demejercicio{\begin{proof}\ejercicio\end{proof}}
\def\findem{\null\hfill\color{white}a\color{black}_\square}
\def\nmc{\displaystyle\frac{\sigma^2 Z^2_{\frac{\alpha}{2}}}{\epsilon^2}}
\def\Finvgen{F^-_{\mbox{ }X}}
\def\Finv{F^{-1}_{\mbox{ }X}}
\def\igualley{\mbox{ }\overset{ley}{=}\mbox{ }}
\def\xcm{(X_n)_{n\in N}}
\def\cm{CM(\lambda,P)}
\def\xhat{\hat{X}}
%ml
\def\samples{(x_1,y_1),\dots,(x_n,y_n),\dots}

\newenvironment{rcases}
  {\left.\begin{aligned}}
  {\end{aligned}\right\rbrace}
% source : https://tex.stackexchange.com/questions/47560/how-to-put-a-brace-on-the-right-not-left-to-group-cases

%\usepackage{hyperref}

% INICIO DE LAS PÁGINAS
\begin{document}
	
% PORTADA
\input{lib/page/portrait} % Se puede borrar

% CONFIGURACIÓN DE PÁGINA Y ENCABEZADOS
\input{lib/cfg/page}

% TABLA DE CONTENIDOS - ÍNDICE
\input{lib/page/index} % Se puede borrar

% CONFIGURACIONES FINALES
\input{lib/cfg/final}

% ======================= INICIO DEL DOCUMENTO =======================
% \textbf{Prefacio}: Este documento corresponde a mis apuntes del curso Simulación Estocástica, dictado en Primavera 2021. Correo contacto: \href{mailto:ccarvajal@dim.uchile.cl}{ccarvajal@dim.uchile.cl}
% \newline \textbf{Camilo Carvajal Reyes}
\section{Repaso y preliminares}
\subsection{Repaso Probabilidades}
\subsubsection{Ley y Esperanza}
Consideraremos $(\Omega,\mathcal{F},\mathbb{P})$ espacio de probabilidad (e.d.p.), $(E,\Sigma)$ espacio medible y $X: \Omega \longrightarrow E$ variable aleatoria (función medible).

\begin{definition}[Ley de X]
La ley de X es la medida de probabilidad:
$\mu := \mathbb{P} \circ X^{-1}: \Sigma \longrightarrow [0,1]$, $A \in \Sigma \longmapsto \mu(A)=\mathbb{P}(X \in A)=\mathbb{P}(X^{-1}(A))$

Corresponde a la medida inducida por $\mathbb{P}$ y $X$ en $\Sigma$.
\end{definition}

\begin{notation}
\beforeitemize
\begin{itemize}
    \item $\mu:=Ley(X)$ o $X\thicksim \mu$.
    \item $\langle \mu, f \rangle = \int f(x) d\mu(x) = \int f(x)\mu(dx)$ $\forall f \in L^1(E,\Sigma,\mu)$.
\end{itemize}
\end{notation}

\begin{definition}[Esperanza]
Si $Y:\Omega \longrightarrow \mathbb{R}$ es variable aleatoria e $Y\in L^1(\Omega,\mathcal{F},\mathbb{P}), Y\geq1$, $\mathbb{E(Y)}$ denota la integral de Lebesgue de $Y$ con respecto a $\mathbb{P}$ y se define la esperanza de $Y$ como sigue:

\begin{itemize}
    \item $\mathbb{E}(Y)=\mathbb{P}(B)$ cuando $Y=\mathbf{1}_B$ con $B \in \mathcal{F}$
    \item $\displaystyle\mathbb{E}(Y)=\sum^n_{i=1} b_i \mathbb{P}(B_i)$ cuando $\displaystyle Y=\sum^n_{i=1}b_i\mathbf{1}_{B_i}$ con $B_i \in \mathcal{F}$, i.e., $Y$ es una función simple.
    % %%% arreglar acá !!!
    \item Para $Y\geq1$, $\mathbb{E}(Y) = \displaystyle \lim_{n\rightarrow \infty} \nearrow \mathbb{E}(Y_n)$ con $(Y_n)_{n \in \mathbb{N}}$ sucesión creciente de funciones simples tal que $Y_n \displaystyle \nearrown Y$  % arreglar nearrow
    %$$ Y_n \,\substack{\nearrow \\ \tiny{n\to\infty}}\, Y \espacio \mbox{\rojo borrar esto en seguida\negro}$$
    % $$ Y_n \,\underset{n\to\infty}{\nearrow}\, Y \espacio \mbox{\rojo borrar esto en seguida\negro}$$
    \item $\mathbb{E}(Y) = \mathbb{E}(Y_+)-\mathbb{E}(Y_-)$ para $Y\in L^1$
\end{itemize}
\end{definition}

\begin{proposition}
Sea $X:\Omega \longrightarrow E $ v.a., $f:(E,\Sigma)\longrightarrow (\mathbb{R},\mathcal{B(\mathbb{R})})$ medida $\geq 0$ $f\in L^1(E,\Sigma,\mu) =_\mu Ley(X)$, entonces
$$ \mathbb{E}(f(X)) = \langle \mu,f \rangle, \forall f \in L^1(\mu)$$
Más aún, $f\in L^1(\mu)$ si y sólo si $f(X) \in L^1(\Omega,\mathcal{F},\mathbb{P})$
\end{proposition}
\begin{proof}
\ejercicio

Indicación: demostrar primero para indicatrices de conjuntos medibles, luego para funciones simples, positivas y finalmente concluir el caso general. 
\end{proof}

\begin{remark}
\beforeitemize
\begin{enumerate}
    \item Si $X$ es v.a. real, ``discreta'' tenemos que:
    $$ \mu=Ley(X) = \sum_x p_x\delta_x$$
    $$\mathbb{E}(f(X)) = \int f(x)\mu(dx) = \sum_x f(x)p_x \, .$$
    En lo anterior, $\delta_x$ son masas de Dirac, $\sum_xp_x = 1$ y las sumas son finitas o numerables.
    \item Si $X$ es v.a., (absolutamente) ``continua'':
    $$ \mu(dx) = f_X(x)dx \, ,$$ 
    $$ \mathbb{E}(\varphi(X))=\int \varphi(x)\mu(dx)=\int\varphi(x) f_X(x)dx \, ,$$
    donde $f_X$ es la densidad de $X$.
\end{enumerate}
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Esperanza Condicional} %clase 2
Sea $(\Omega,\F,P)$ un espacio de probabilidad completo y $G\subset \F$ una sub-$\sigma$-álgebra. Consideramos las siguientes interpretaciones:
\begin{itemize}
    \item $\omega\in\Omega$ serán los ``estados posibles de la naturaleza''
    \item $\F$ conjuntos cuya ocurrencia somos capaces de distinguir: dado $\omega \in\Omega$ y $B\in\F$, podemos responder si $\omega\in B$ (eventos), y podemos ``medir'', i.e., calcular $\P(B)$ \\
    $\therefore\F$ representa a qué información tenemos acceso.
    \item $\G$, al ser una sub-$\sigma$-álgebra, posee menos información (reconoce menos eventos).
\end{itemize}
\begin{definition}[Esperanza condicional, caso $L^2$]
Sea $X\in L^2\edp, \G\subset\F$ una sub-$\sigma$-álgebra. Se define la esperanza condicional de $X$ dado $\G$ como la \textbf{proyección ortogonal} desde $L^2\edp$ de $X$ en el subespacio vectorial $L^2(X,\G\P)$.
\\ Esto lo denotaremos $\E(X|\G)$
\end{definition}
\begin{remark}
\beforeitemize
\begin{itemize}
    \item $L^2(X,\G,\P)$ es cerrado en $L^2\edp$.
    \\ En efecto $Z_n\convldos Z$ con $Z_n\in L^2(X,\G,\P)$ implica que existe una subsucesión que converge a $Z$ casi seguramente. Por lo tanto $Z\in\G$
    \item $\E(X|\G)$ es $\G$-medible.
    \item $\E(\cdot|\G):\L^2\edp\mapsto L^2(\Omega,\G,\P)$ es bilineal y continua.
\end{itemize}
\end{remark}
\begin{remark}[Propiedad fundamental]
$\E(X|\G)$ queda caracterizada como la única variable aleatoria tal que:
\begin{enumerate}
    \item $\E(X|\G)\in L^2(\Omega,\G,\P)$
    \item $\E(\E(X|\G)|Z) = \E(X|Z) \espacio \forall Z\in L^2(\Omega,\G,\P)$
    \vspace{.4cm}\\ En particular tenemos $\E(\E(X|\G)) = \E(X)$ y si además $X\in L^2(\Omega,\G,\P)$, $E(X|\G)=X$
\end{enumerate}
\end{remark}
%\vspace{1cm}  % haciendo que la propiedad empieze en la próxima página
La siguiente propiedad es un ejercicio fácil.
\begin{property}
\beforeitemize
\begin{enumerate}
    \item $Z=\E(X|\G)$ minimiza $Z\in L^2(\Omega,\G,\P)\mapsto\E((Z-X)^2)$.
    \item Como consecuencia de lo anterior, tenemos la siguiente \textbf{interpretación estadística}: La v.a. $\mathcal{G}$-medible $\E(X|\G)$ es el \textbf{mejor estimador de $X$} en el sentido de tener \textbf{menor error cuadrático medio} (en inglés \textit{MSE}) usando la \textbf{información accesible} para la $\sigma$-álgebra $\G$.
    %\item Si $X$ es $\G$-medible, la mejor estimación (menor error cuadrático) de $X$ usando la información en $\G$ es $\E(X|\G)=X$
    \item Si $\G$ es la tribu trivial ($G=\{\emptyset,\Omega\}$), toda función $\G$-medible es constante. Luego $\E(X|\G)$ es constante tal que $\E(E(X|\G))=\E(X)\implies\E(X|\G)=\E(X)$. \\ Dicho de otro modo, la mejor estimación es ``trivial'' y no usa información.
\end{enumerate}
\end{property}
%\demejercicio

\begin{lemma}
Sea $Y:\Omega\mapsto E$ v.a. y $Z:\Omega\mapsto \R$ v.a. medible con respecto a $\G=\sigma(Y):=\{Y^{-1}(A):A\in \Sigma \}\subset \mathcal{F}$ (con $\Sigma$ $\sigma$-álgebra de $E$), entonces existe $h:E\mapsto \R$ medible tal que $Z=h(Y)$.
\end{lemma}
\begin{remark}
\beforeitemize
\begin{itemize}
    \item En particular para $Z=\E(X|\G)$, $\G=\sigma(Y)$ escribimos $\E(X|Y=y):=h(y)$, de modo que
    $$ \E(X|Y) = \E(X|\sigma(Y)) = h(Y) = \E(X|Y=y)|_{y=Y} \, .$$
    \item Si $Y=(Y_1,\dots,Y_d)\in\R^d$, $\E(X|\sigma(Y_1,\dots,Y_d)$ se denota $\E(X|Y_1,\dots,Y_d)$. Por lo anterior, es una función de ($Y_1,\dots,Y_d$)\, .
\end{itemize}
\end{remark}
\begin{proof}
(del Lema)

\begin{itemize} \gris
    \item Primero asumimos que $Z=\mathbf{1}_B$ con $B\in\sigma(Y)$, es decir $B=Y^{-1}(A)$ para cieto  $A\in\Sigma$. \\ Entonces $\mathbf{1}_B=\mathbf{1}_{Y^{-1}(A)}=\mathbf{1}_A(Y)$, luego $Z=h(Y)$ con $h(y)=\mathbf{1}_A(y) \, .$
    \item Ahora tomemos $Z=\displaystyle \sum^n_{i=1}b_i\mathbf{1}_{B_i}$ con $B_i=Y^{-1}(A_i)$, $A_i\in\Sigma$, $\forall i\in\{1,\dots,n\} \, .$
    \\ As\'i,  $Z=\displaystyle \sum^n_{i=1}b_i\mathbf{1}_{A_i}(Y)$, luego $Z=h(Y)$ con $h(y)=\displaystyle\sum^n_{i=1}b_i\mathbf{1}_{A_i}(y)$.
    \item Sea $Z\geq 0$, entonces existe una sucesión $(Z_k)_k$, todos $\G-medibles$ y $h^k:E\mapsto\R$ medibles tal que $h^k(Y)=Z_k \displaystyle \nearrowk Z$ (puntualmente) c.s. . Entonces podemos definir:
    $$ h(y) = \begin{cases} \displaystyle\limsup_{k\to \infty}h^k(y)  & \mbox{ si }y\in Y(\Omega)\\
                            0 & \mbox{ si }y\notin Y(\Omega)  \end{cases} \, .$$
    % Se puede probar que $h$ es medible. 
    Luego dado que $Z_K \nearrowk Z$, queda que $h(Y)=\displaystyle\limsup_{k\to \infty}Z_k$\, . % $Z=\displaystyle\lim_{k\to\infty}Z_k=\lim_{k\to\infty}h^k(Y)=h(Y)$ c.s. 
    \item El caso general se deduce de lo anterior y  queda propuesto.
\end{itemize} \findem \negro
\end{proof}
\vspace{2cm}  % haciendo que el teorema empiece en la próxima página
\begin{theorem}[Esperanza condicional, caso general $L^1$]
Sean $X\in L^1\edp$ v.a. y $\G\subset\mathcal{F}$ sub-$\sigma$-álgebra. Entonces existe una única variable aleatoria $Z\in L^1\edp$ tal que:
\begin{itemize}
    \item $Z\in L^1(\Omega,\G,\P)$
    \item $\E(XH)=\E(ZH) \hspace{.5cm} \forall H\in L^\infty(\Omega,\G,\P)$ \espacio (propiedad fundamental)
\end{itemize}
\end{theorem}
\begin{notation}
Denotamos $Z$  como $\E(X|\G)$ y la llamamos \textbf{Esperanza condicional de $X$ % $\in L^1$
dado $\G$}
\end{notation}
\begin{remark}
\beforeitemize
\begin{itemize}
    \item $\E(\E(X|\G))=\E(X)$ (con $H=1$)
    \item La propiedad fundamental equivale a $\E(X \mathbf{1}_A)=\E(\E(X|\G)\mathbf{1}_A) \forall A\in\G$
    \\ Esto se demuestra usando aproximación por funciones simples y T.C.M. (\ejercicio)
\end{itemize}
\end{remark}
\begin{proof}
(del Teorema)

\ejercicio \gris \, Para la existencia cuando $X\geq0$ considerar $X_n:=\min(X,n) \, \forall n\in\N$ y ver que $\E(X_{n+1}|\G)\geq\E(X_n|\G)$ c.s. . Por T.C.D. se verifica que $X_n\convluno X$ y entonces tomando $Z=\displaystyle\lim_{n\to\infty}\nearrow\E(X_n|\G)$ se prueba que $\E(XH)=\E(\E(ZH))\,\forall H\in \L^\infty(\G)$. 

Para unicidad primero ver que si tomamos $Z,Z'$ tal que satisfacen la propiedad fundamental entonces $\E((Z-Z')_{\{Z<Z'\}})=0$. Notar que lo anterior es simétrico y usarlo para concluir que $Z=Z'$ c.s. .
\negro
% \color{red} completar indicación \color{black}  % 10/14
\end{proof}
\begin{property}
\beforeitemize
\begin{enumerate}
    \item[(i)] $\E(\cdot|\G):L^1\edp\mapsto L^1(\Omega,\G,\P)$ es una aplicación lineal continua
    \item[(ii)] Si $X\in L^1(\Omega,\G,\P)$ entonces $\E(X|\G)=X$
    \item[(iii)] Si $F\in L^\infty(\Omega,\G,\P)$ entonces $\E(XF|\G)=F\E(X|\G)$
    \item[(iv)] Si $\mathcal{H}\subseteq \G\subseteq\mathcal{F}$ $\sigma$-álgebras entonces
    $ \E(\E(X|\G)|\mathcal{H}) = \E(\E(X|\mathcal{H})|\G)=\E(X|\mathcal{H})$
\end{enumerate}
\end{property}
\begin{proof}
\beforeitemize
\begin{enumerate} \gris
    \item[(i)] \ejercicio \gris
    \item[(ii)] \ejercicio \gris
    \item[(iii)] Sean $F$, $H\in L^\infty(\G)$, $\E((X F) H)=\E(X(FH))=\E(\E(X|\G)F H)$
    \\ Como $\E(X|\G)F \in L^1(\Omega,\G,\P)$, por (ii), y tomando $H=1$,  $\E(X F)=\E(\E(X|\G)F)=\E(X|\G)F$.
    \item[(iv)] La segunda igualdad es directa pues $\E(X|\mathcal{H})$ es en particular $\G$-medible. Para la primera tomemos $H\in L^\infty(\H)$, como $H\in L^\infty$ tenemos $\E(XH)=\E(\E(X|\G)H)=\E(\E(\E(X|\G)|\mathcal{H})H)$ pues $\E(\E(X|\G)|\mathcal{H})$ es $\mathcal{H}$-medible, entonces $\E(X|\mathcal{H})=\E(\E(X|\G)|\mathcal{H})$, pero como tenemos que $\E(X|\mathcal{H})=\E(\E(X|\mathcal{H})|\G)$ (segunda igualdad), entonces concluimos que $\E(\E(X|\mathcal{H})|\G)=\E(\E(X|\G)|\mathcal{H})$.
    % que es igual a $\E(\E(X|\mathcal{H})H|\G)$ pues $\E(\E(X|\G)|\mathcal{H})\in L^1(\mathcal{H})$ \, .
\end{enumerate}
\findem
\end{proof}
% \vspace{2cm} \\
\begin{example}[Aterrizando el concepto]
\beforeitemize
\begin{itemize}
    \item Sean $(B_n)_{n\in\N}\subset\F$ partición de $\Omega$ y $\G:=\sigma((B_n)_n)$
    \\ Se puede probar que $\G=\{\cup_{j\in J}B_j:J\subseteq\N\mbox{ numerable o finito }\}\cup\{\emptyset\}$ (\ejercicio).
    \\ Sea $X\in L^1$, la esperanza condicional está dada por:
    $$ \E(X|\G)=\displaystyle\sum_{n\in\N}\mathbf{1}_{B_n}\E(X|B_n) \, .$$
    \begin{proof} \gris
    $\displaystyle\sum_{n\in\N}\mathbf{1}_{B_n}\E(X|B_n)$ es $\G$-medible y está en $L^1$. Por otro lado, $\forall A\in\G$, $\exists (B_n)_{n\in\N}$ $A=\displaystyle \dot\cup_{j\in J}B_j$, luego se tiene
    $$ \E((\displaystyle\sum_{n\in\N}\mathbf{1}_{B_n}\E(X|B_n))\mathbf{1}_A)=\E(\sum_{j\in J}\mathbf{1}_{B_j}\E(X|B_j))=\E(X \mathbf{1}_A) \, .$$
    \negro \end{proof}
    \item Sean $(X,Y)\in\R^2$ par aleatorio continuo con densidad $f_{(X,Y)}$. La densidad condicional de $X|Y=y$ se define como $\displaystyle f_{X|Y}(x|y) = \frac{f_{(X,Y)(x,y)}}{f_Y(y)}\mathbf{1}_{\{f_Y(y)>0\}}$.
    \\ \ejercicio: demostrar que 
    $$ \E(X|Y)(\omega) = [\displaystyle\int x f_{X|Y}(x|y)dx]|_{y=Y(\omega)} \, .$$
\end{itemize}
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Convergencia en Ley de variables aleatorias}
Parte de esta subsección está basada en el libro de Billingsley \cite{billing}.
\begin{notation}
\beforeitemize
\begin{itemize}
    \item $(E,d)$ espacio métrico, $\mathcal{B}$ tribu boreliana
    \item $\mathcal{M}(E)$ medidas finitas $\geq0$ sobre $E$
    \item $\mathcal{P}(E)$ medidas de probabilidad, $\mathcal{M}_s(E)$ medidas con sigma finitas %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
    \item $\mathcal{C}_b(E)$ funciones continuas acotadas %%%%%%%%%%%%%%%%%%%%5
    \item $BL(E)$ funciones Lipschitz acotadas
    \item Integral de $f$ con respecto a $\mu \in \mathcal{M}(E)$ (con $f$ medible y acotada): $$ \langle \mu, f \rangle = \int f(x)\mu(dx) $$
\end{itemize}
\end{notation}

\subsubsection{Definición de convergencia débil y en ley}
\begin{remark}
$\nu \in \mathcal{M}(E)$ queda caracterizada por $\langle \nu,f \rangle$, $f \in \mathcal{C}_b(E)$
\end{remark}
\begin{proof}\gris Sea $F\subset E$ cerrado y $\epsilon>0$. Consideramos $f_\epsilon(x) = \max (0,1-\frac{d(x,F)}{\epsilon})$ con $d(x,F)=\displaystyle\inf_{y\in F}(x,y)$. Notemos que esta función es $\frac{1}{\epsilon}$-Lipschitz y que $\mathbf{1}_F\leq f_\epsilon\leq\mathbf{1}_{F^\epsilon}\searroweps\mathbf{1}_F$, donde $F^\epsilon = \{x \in E : d(x,F)<\epsilon\}$. Entonces por teorema de convergencia dominada, 
$$ \langle \nu,f_\epsilon \rangle \searroweps \nu(F) \, . $$
Sea $\displaystyle\mathcal{H}=\{A\subset E \mbox{ tal que }\mu(A)=\sup_{F\subset A cerrado}\mu(F)=\mu(A)=\inf_{F\subset A\mbox{ abierto}}\mu(F)\}$. $\mathcal{H}$ es $\sigma$-álgebra y $\{F\subset A : F\mbox{ cerrado}\}\subset\mathcal{H}$. Entonces $\mathcal{B}=\sigma(cerrados)\subset\mathcal{H}$ y por lo tanto $\displaystyle\forall A\in\mathcal(B)(E),A=\sup_{F cerrado \subset A}\mu(F),\Rightarrow \mu$ queda caracterizada por los cerrados y entonces por $\langle \mu,f \rangle, f\in\mathcal{C}_b(E)$. 
\negro \findem
\end{proof}

\begin{definition}[Convergencia Débil]
Sean $\mu\in \mathcal{M}(E)$ y $(\mu_n)_{n \in \mathbb{N}}\subset \mathcal{M}(E)$ medidas finitas mayores o iguales a $0$. Decimos que $(\mu_n)$ converge débilmente a $\mu$ si 
$$\langle \mu_n,f\rangle \mbox{ }\substack{\longrightarrow \\ n\to\infty}\mbox{ } \langle \mu,f \rangle, \espacio \forall f \in \mathcal{C}_b(E), .$$
Esto se denota $\mu_n \mbox{ }\substack{\Longrightarrow \\n \to \infty}\mbox{ } \mu$.
\end{definition}
\begin{remark}
$\mathcal{P}(E)\subset \mathcal{M}(E) \subset \mathcal{M}_s(E) \subset\mathcal{C}_b(E)^*$ \espacio y \espacio$\mu_n \mbox{ }\substack{\Longrightarrow \\n \to \infty}\mbox{ } \mu$ equivale a $\displaystyle \mu_n \mbox{ }\overset{\ast}{\substack{\rightharpoonup\\n \to\infty}} \mbox{ }\mu$

La inclusión $\mathcal{M}_s(E) \subset\mathcal{C}_b(E)^*$ se demuestra como sigue: para cada $\nu\in\mathcal{M}_s(E)$, la aplicación $f\in\mathcal{C}_b(E)\mapsto\langle \nu,f\rangle:=\langle \nu_+,f\rangle-\langle \nu_-,f\rangle$ es lineal. Además es continua: $|\langle\nu,f\rangle|\leq (\langle \nu_+,f\rangle+\langle \nu_-,f\rangle)\|f\|_{unif}$. Entonces $\nu\in\mathcal{C}_b(E)^*$.
\end{remark}

\begin{example}[\label{ejemplo:1_2_1}] 
Consideremos $E=\mathbb{R}$.
\begin{itemize} % % completar en clase
    \item[(i)] $\mu_n=\delta_{\frac{1}{n}} \mbox{ }\substack{\Longrightarrow \\n \to \infty}\mbox{ } \delta_0$ 
    \item[(ii)] $\displaystyle\mu_n(dx) = \frac{n}{2}\mathbf{1}_{[-\frac{1}{n},\frac{1}{n}]}(x)dx \mbox{ }\substack{\Longrightarrow \\n \to \infty}\mbox{ } \delta_0$
    \item[(iii)] $\displaystyle\mu_n=\frac{1}{n}\sum_{n=0}^{n-1}\delta_{\frac{k}{n}} \mbox{ }\substack{\Longrightarrow \\n \to \infty}\mbox{ } \mu$ %$$ = \mbox{Lebesgue en }[0,1]$
    \\ Donde $\mu$ es la medida de Lebesgue en $[0,1]$
\end{itemize}
\end{example}
\begin{proof}
\gris Sea $f\in C_b(E)$, %\beforeitemize
\begin{itemize}
    \item[(i)] $\langle \mu_n,f\rangle=f(\frac{1}{n})\conv f(0)=\langle \delta_0,f\rangle$
    \item[(ii)] Observemos que 
    $\displaystyle\int f(x)\mu_n(dx)=\frac{n}{2}\int^{\frac{1}{n}}_{-\frac{1}{n}}f(x)dt = \frac{n}{2}\cdot\frac{2}{n}\cdot f(\xi_n)$,
    
    donde en la última igualdad usamos el teorema del valor medio para integrales y $\xi_n\in[\frac{-1}{n},\frac{1}{n}]$.
    \\ Como $f$ es continua, $\frac{n}{2}\cdot\frac{2}{n}\cdot f(\xi_n) \conv f(0)=\langle \delta_0,f \rangle$
    \item[(iii)] Tenemos que $\displaystyle\int f(x)\mu_n(dx) = \frac{1}{n}\sum^n_{k=0}f\bigg(\frac{k}{n}\bigg)$. El lado derecho es una suma de Riemann con paso $\frac{1}{n}$. Como $f$ es continua entonces es Riemann integrable y luego $\displaystyle \frac{1}{n}\sum^n_{k=0}f\bigg(\frac{k}{n}\bigg) \conv \int_0^1 f(x)dx$.
\end{itemize} 
\findem
\negro \end{proof}

\begin{definition}[Convergencia en Ley]
Sean $(\Omega_n,\mathcal{F}_n,\mathbb{P}_n), (\Omega,\mathcal{F},\mathbb{P})$ espacios de probabilidad, Sean $X_n:\Omega_n \longrightarrow E, n\in\mathbb{N}$, y $X:\Omega \longrightarrow E$ v.a., decimos que $X_n$ converge en ley o en distribución a $X$ si 
$$\mu_n:=Ley(X_n) \mbox{ }\substack{\Longrightarrow \\n \to \infty}\mbox{ } \mu:=Ley(X)\mbox{ en }\mathcal{P}(E) \, .$$
Equivalentemente, $\forall f \in \mathcal{C}_b(E)$ $$ \langle \mu_n,f \rangle \mbox{ }\substack{\longrightarrow \\ n\to\infty}\mbox{ } \langle \mu,f,\rangle\, \, ,$$
y 
$$ \mathbb{E}_n(f(X_n)) \mbox{ }\substack{\longrightarrow \\ n\to\infty}\mbox{ } \mathbb{E}(f(X))\, .$$
\end{definition}
\begin{notation}
La convergencia en ley se denota del siguiente modo:
$$X_n \,\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\,X \, .$$
Equivalentemente podemos denotarla \espacio
%$$ X_n \mbox{ }\overset{\mathcal{L}}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X \mbox{ o } \mbox{ }\overset{d}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X \, .$$
$ X_n \,\overset{\mathcal{L}}{\substack{\longrightarrow \\n \to \infty}}\,X \hspace{.5cm}\mbox{ o }\hspace{.5cm} X_n\,\overset{d}{\substack{\longrightarrow \\n \to \infty}}\,X \, .$
\end{notation}

\begin{example}
Gracias al Ejemplo \ref{ejemplo:1_2_1} tenemos lo siguiente:
\begin{itemize}
    \item Sean $X_n \thicksim \frac{1}{n}\displaystyle\sum_{k=0}^{n-1}\delta_{\frac{k}{n}}$ y $X \thicksim \mathbb{U}[[0,1]$, entonces $X_n \mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X$ .
    \item Sean $X_n \thicksim \mathbb{U}([-\frac{1}{n},\frac{1}{n}])$ y $X \thicksim \delta_0 (X \equiv 0)$ entonces $X_n\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X$ .
\end{itemize}
\end{example}

\begin{remark}
\beforeitemize
\begin{itemize}
    \item $X_n\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X \centernot\implies \mathbb{E}_n(f(X_n)) \mbox{ }\substack{\longrightarrow \\ n\to\infty}\mbox{ } \mathbb{E}(f(X))$, para toda $f$ medible acotada.
    %\item $X_n\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X \nRightarrow \mathbb{P}(X_n\leq x) \longrightarrow \mathbb{P}(X\leq x) \forall x \in \mathbb{R}$
    \item $X_n\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X \centernot\implies \mathbb{P}(X_n\leq x) \conv \mathbb{P}(X\leq x) \forall x \in \mathbb{R}$.
\end{itemize}

% En ambos casos necesitamos que sea continua y acotada
\end{remark}

\begin{theorem}[Portmanteau] Sean $\mu,\mu_n \in \mathcal{P}(E)$. Son equivalentes:
\label{portmanteau}
\begin{enumerate}
    \item[(i)] $\mu_n \mbox{ }\substack{\Longrightarrow \\n \to \infty}\mbox{ } \mu$
    \item[(ii)] $\langle \mu_n,f\rangle { }\substack{\longrightarrow \\ n\to\infty}\mbox{ } \langle \mu,f\rangle, \hspace{.25cm}\, \forall f$ acotada, uniformemente continua
    \item[(iii)]$\langle \mu_n,f\rangle { }\substack{\longrightarrow \\ n\to\infty}\mbox{ } \langle \mu,f\rangle, \espacio \forall f \in BL(E)$ Lipschitz acotada
    \item[(iv)] $\displaystyle\limsup_{n\to\infty} \mu_n (F)\leq\mu(F) \espacio \forall F $ cerrado
    \item[(v)] $\displaystyle\liminf_{n\to\infty} \mu_n (G)\geq\mu(G) \espacio \forall G $ abierto
    \item[(vi)] $\displaystyle\lim_{n\to\infty}\mu_n(A) = \mu(A) \espacio \forall A \in \mathcal{B}$ tal que $\mu(\partial A)=0$ (frontera de medida nula)
    \item[(vii)] $\langle \mu_n,f\rangle { }\substack{\longrightarrow \\ n\to\infty}\mbox{ } \langle \mu,f\rangle, \espacio \forall f$ acotada, continua $\mu(dx)$-c.s.
\end{enumerate}
\end{theorem}
\begin{proof}
\gris
(vi)$\Rightarrow$(i)$\Rightarrow$(ii)$\Rightarrow$(iii) son directas.

\begin{itemize}
    \item[] (iii)$\Rightarrow$(iv) tomamos $f_\epsilon(x)=\max(0,1-\frac{d(x,F)}{\epsilon})$ y notemos que $\mathbf{1}_F\leq f_\epsilon\leq\mathbf{1}_\bar{{F^\epsilon}}$. Luego tenemos
    $$\displaystyle\limsup_{n}\mu_n(F)\leq\limsup_{n}\langle\mu_n(F),f_\epsilon\rangle=\langle\mu,f\rangle \, .$$
    A su vez $\mathbf{1}_\bar{{F^\epsilon}}$ $\,\substack{\searrow \\ \tiny{\epsilon\to 0}}\,\mathbf{1}_F$, de donde se concluye que
    $$ \displaystyle\limsup_n\mu_n(F)\leq \langle \mu,\mathbf{1}_{\bar{{F^\epsilon}}} \rangle \,\substack{\searrow \\ \tiny{\epsilon\to 0}}\, \mu(F) \, ,$$
    Donde en la última convergencia usamos T.C.D.
    \item[] (iv)$\Rightarrow$(v) Basta tomar $F=G^c$.
    \item[] (v)$\Rightarrow$(vi) $A^o \subset A\subset \bar{A}$ y como $\partial A=0$, $\mu(\bar{A})=\mu(A)=\mu(A^o)$. Ahora aplicamos (iv), $\limsup\leq\liminf$ y (v) para obtener: $$\mu(\bar{A})\geq \displaystyle \varlimsup_n \mu_n(\bar{A}) \geq \varlimsup_n \mu_n(A) \geq \varliminf_{n}\mu_n(A) \geq \varliminf_{n} \mu_n(A^0) \geq\mu(A^o) \, ,$$ donde hemos usando (v) en la primera y la última desigualdad. Concluimos que $\exists \lim_{n\to\infty}\mu_n(A)=\mu(A)$.
    \item[] (vi)$\Rightarrow$(vii) Sin perder generalidad podemos suponer que $f:E\longrightarrow[0,1]$. De otro modo basta cambiar $f$ por $\frac{f-\inf f}{\sup f - \inf f}$.
    \\ Notar que usando Fubini, para todo $g\geq0$ medible y $\nu\in\mathcal{P}(E)$ tenemos
    $$ \langle \nu,g \rangle = \displaystyle\int(\int^\infty_0 \mathbf{1}_{\{(t,x):t<g(x)\}}dt)\nu(dx)=\int^\infty_0\nu(\{g>t\})dt \, .$$
    Por otro lado, sea $C_f$ los puntos de continuidad de $f$, se puede probar que  $C_f\cap\partial\{f>t\}\subseteq\{f=t\}$ (\ejercicio \gris ).
    \\ Además, se tiene que $\mu(\{f=t\})=0$ salvo para un conjunto numerable de $t$'s. En efecto, como $\mu$ es medida finita, $\mu(\partial\{f>t\})=\mu(C_f\cap \partial\{f>t\})=0 \espacio dt\mbox{-c.t.p.} \, .$
    \\ Entonces $$ \mu_n(f>t) \conv \mu(f>t) \espacio dt\mbox{-c.t.p.} \, .$$
    Por último, por T.C.D. concluimos que $\langle \mu_n,f \rangle = \displaystyle\int^1_0 \mu_n(f>t)dt \conv \int^1_0\mu(f>t)dt = \langle \mu, f\rangle$.
\end{itemize}
\findem \negro
\end{proof}

%%%%% clase 4 %%%%%%%%
\begin{theorem}[Teorema del mapeo]
Si $X_n$ y $X$ son v.a., en $(E,d)$ tal que $X_n\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X$ y $\Phi:(E,d)\longrightarrow (E',d')$ es función continua (no necesariamente acotada) entonces $\Phi(X_n)\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }\Phi(X)$
\end{theorem}
\demejercicio

\begin{property}[Consecuencias de Teo. del mapeo]
Sea $(X_n,Y_n)\in\mathbb{R}^2d,Z_n\in\mathbb{R}$ v.a.,
\begin{itemize}
    \item $(X_n,Y_n)\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }(X,Y)\Longrightarrow X_n+Y_n\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X+Y$
    \item $(X_n,Z_n)\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }(X,Z)\Longrightarrow X_n\cdot Z_n\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }XZ$
    \item $X_n\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X \Longrightarrow (X_n^{i_1},\dots,X_n^{i_k})\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }(X^{i_1},\dots,X^{i_k})$ \\ (pues la proyección es una función continua)
\end{itemize}
\end{property}
\begin{proof}
\ejercicio
\end{proof}

\begin{proposition}
Sea $(\Omega,\mathcal{F},\mathbb{P})$ un espacio de probabilidad (no $n$ distintos como antes). Entonces:
$$\left.\begin{aligned}
X_n\mbox{ }\overset{L^p}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X \\ \mbox{o} \hspace{1cm} \\
X_n\mbox{ }\overset{c.s.}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X
\end{aligned}\right\} \Longrightarrow X_n\mbox{ }\overset{\mathbb{P}}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X \Longrightarrow X_n\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X \, .$$
%$$ X_n\mbox{ }\overset{L^p}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X \espacio \mbox{ o } \espacio X_n\mbox{ }\overset{c.s.}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X \Longrightarrow X_n\mbox{ }\overset{\mathbb{P}}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X \Longrightarrow X_n\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X$$
I.e., la convergencia en ley es más débil que las otras convergencias.
\end{proposition}
\begin{proof}
\ejercicio \gris

Indicación: usar Portmanteau con caracterización (iii) ($f\inBL(E)$). \negro
\end{proof}

\begin{remark}
En general $X_n\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X \centernot\implies X_n\mbox{ }\overset{\mathbb{P}}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X$, sin embargo
$$ X_n\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }A \mbox{ determinista } \Longrightarrow X_n\mbox{ }\overset{\mathbb{P}}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }A \mbox{ determinista }$$
\end{remark}
\begin{proof}
\ejercicio
\end{proof}

\begin{theorem}[del mapeo generalizado]
Si $X_n$ y $X$ son v.a., en $(E,d)$ tal que $X_n\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X$ y $\Phi:(E,d)\longrightarrow (E',d')$ es función continua $\mu(dx)$-c.s. en $X\in E$ con $\mu=Ley(X)$ entonces $\Phi(X_n)\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }\Phi(X)$.
\end{theorem}
\begin{proof}
\color{blue}Ejercicio (usar Portmanteau (vii)) \color{black}
\end{proof}

\begin{proposition}
Sean $X,X_n,n\in\N$ v.a., reales, son equivalentes:
$$X_n\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X \espacio \mbox{ y }\espacio F_{X_n}(x) \mbox{ }\substack{\longrightarrow \\ n\to\infty}\mbox{ } F_X(x)\, ,$$
para todo $x$ punto de continuidad de $F_X$.
\end{proposition}
\begin{proof}
\color{blue}$\Rightarrow$ Ejercicio. \gris Para $\Leftarrow$ ver Billingsley \cite{billing}. \negro
\end{proof}

\subsubsection{Una métrica D para la convergencia débil en M(E)}
\begin{definition}[Norma Lipschitz]
Sea $f\in BL(E)$ y $\mu\in \mathcal{M}_s(E)$, definimos la norma Lipschitz como:
$$\displaystyle \|f\|_{BL(E)} := \sup_x|f(x)|+\sup_{x\neq y}\frac{|f(x)-f(y)|}{d(x,y)} \, .$$
% A $\|\cdot\|_{BL(E)}$ se le llama norma Lipschitz. 
Por otro lado definimos:
%$$ \|\mu\|_{BL(E)^*}:=\displaystyle\sup_{} \dots$$
$$\|\mu\|_{BL(E)^*} := \displaystyle\sup_{f\in BL(E),\|f\|_{BL}\leq 1}|\langle \mu,f\rangle| \, .$$
\end{definition}
% \begin{remark}
%De hecho el segundo supremo 
% $\displaystyle\sup_{x\neq y}\frac{|f(x)-f(y)|}{d(x,y)}$ es la constante Lipschitz  óptima:
% $$\|\mu\|_{BL(E)^*} := \displaystyle\sup_{f\in BL(E),\|f\|_{BL}\leq 1}|\langle \mu,f\rangle| \, .$$
% \end{remark}

\begin{definition}[Distancia Lipschitz dual]
Sean $\mu,\nu\in\mathcal{M}(E)$. Definimos la distancia Lipschitz dual como: $$D(\mu,\nu):=\|\mu-\nu\|_{BL(E)^*} \, .$$
\end{definition}

\begin{theorem}
Si $E$ es separable, $D$ metriza la convergencia débil en $\mathcal{M}(E)$, esto es, para  $(\mu_n)\subseteq\mathcal{M}(E),\mu\in\mathcal{M}(E)$,
$$ \mu_n \mbox{ }\substack{\Longrightarrow \\n \to \infty}\mbox{ } \mu \espacio \mbox{ ssi } \espacio D(\mu_n,\mu)\mbox{ }\substack{\longrightarrow \\ n\to\infty}\mbox{ }0$$
Además, los e.m. $(\mathcal{M}(E),D)$ y $(\mathcal{P}(E),D)$ son separables y más aún, son polacos (separables completos) si $E$ lo es.
\end{theorem}
\begin{proof}
\gris Ver Billingsley \cite{billing} \negro
\end{proof}

\begin{remark}
\beforeitemize
\begin{itemize}
\item No es cierto en general al cambiar $\mathcal{M}(E)$ por $\mathcal{C}_b(E)^*$. (Topología débil $*$ no siempre es metrizable).
% \end{remark}
% \begin{remark}
\item $D$ es estrictamente más débil que la distancia de variación total, dada por: 
$$\|\mu-\nu\|_{TV}:=|\mu-\nu|= \displaystyle\sup_{f\in \mathcal{C}_b(E),\|f\|_{unif}\leq 1}|\langle \mu-\nu,f\rangle|=\|\mu-\nu\|_{\mathcal{C}_b(E)^*} \, . $$
En efecto,  vimos que
$$ \mu_n=\frac{1}{n}\displaystyle\sum^{n-1}_{k=0}\delta_{\frac{k}{n}}\Longrightarrow\mu=\mathbb{U}_{[0,1]} \, , $$
pero $\|\mu_n-\mu\|_{TV}=1\espacio\forall n\in\mathbb{N}$ pues $\mu_n\bot\mu$ (i.e., son singulares una con respecto a la otra). 
\\ Entonces, la distancia variación total es demasiado rígida. Por ende el uso de funciones Lipschitz acotadas.
\end{itemize}
\end{remark}

\vspace{3cm}

\subsubsection{Compacidad en (M(E),D): Tensión}
%\color{red}AGREGAR OBSERVACIÓN 10:50\color{black}
\begin{definition}[Tensión (\textit{Tightness})]
\label{def:tension}
Una familia $M\subset\mathcal{M}(E)$ se dice \textbf{tensa} si:
\begin{itemize}
    \item[(i)] $\displaystyle\sup_{\mu\in M}\mu(E)<\infty$
    \item[(ii)] $\forall \epsilon >0, \exists K_\epsilon\subset E$ compacto tal que $\displaystyle\sup_{\mu\in M}\mu(K_\epsilon^c)\leq \epsilon$
    
Dicho de otro modo, casi toda la masa de todas las $\mu\in M$ está en un mismo compacto.
\end{itemize}
\end{definition}

\begin{example} % Sea $E=\mathbb{R}$
\label{ejemplo:1_2_3}
\beforeitemize
\begin{itemize}
    \item[(a)] Sea $E=\mathbb{R}$, $M=(\delta_n)_{n\in\mathbb{N}}$ no es tensa.
    \item[(b)] Sea $E=\mathbb{R}$, $M=\}\mu_a\}_{a\in[0,R]}$ es tensa, con $\mu_a=\mathbb{U}_{[-a,a]}$.
    \item[(c)] Sea $E=\mathbb{R}^d$, $M=\{\mu\}$ con $\mu$ medida finita es tensa ($\mu$ es regular interior por compactos).
    
    (De hecho, se prueba en Billingsley \cite{billing} que $M=\{\mu\}$ es tensa si (E,d) es polaco).
    \item[(d)] Sea $E=\mathbb{R}$, $\{\mu_\sigma\}_{\sigma\in[0,L]}$ con $\mu_\sigma=\mathcal{N}(0,\sigma^2)$ es tensa
    
    De manera más general, $\{\mu_\lambda\}_{\lambda\in\Lambda}\subset \mathcal{P}(\mathbb{R})$ tal que $\exists p>0, \displaystyle\sup_{\lambda\in\Lambda}\int|x|^p\mu_\lambda(dx)=c<\infty$ es tensa. Intuición: si los momentos están acotados, no puede haber masa muy lejos.
    \item[(e)] Sea $E$ un espacio arbitrario, si $M_1,M_2,\dots,M_m$ son $m$ familias tensas entonces $M=\displaystyle\cup^m_{i=1}M_i$ es una familia tensa.
    
    En particular, las familias finitas en $\mathcal{M}(E)$ con $(E,d)$ polaco, son tensas.
\end{itemize}
\end{example}
\begin{proof}
\gris
Debemos probar los puntos (i) y (ii) de la definición \ref{def:tension} (Tensión), sin embargo el punto (i) es directo cuando las medidas son de probabilidad. Por ende en general probaremos sólo el punto (ii).
\begin{enumerate}
    \item[(a)] Siempre habrá infinitas medidas fuera del compacto que nos demos, i.e., $\forall K$ compacto, existen infinitos $n\in\N$ tal que $n\in K^c$, entonces $\displaystyle\sup_n\delta_n(K^c)=1$. Por ende $(\delta_n)_{n\in\mathbb{N}}$ no es tensa.
    \item[(b)] % (i) en la definición es directo pues $[0,R]$ es acotado. %$\displaystyle\sup_{\mu\in M}\mu(E)\leq $
    Sea $\epsilon>0$, $K_\epsilon=[-R,R]$ cumple $\mu_a(K_E^c)=0\leq\epsilon, \forall a\in[0,R]$.
    %\item[(c)]
    \item[(d)] Sea $\epsilon>0$, tenemos que: %$\mu_\lambda([-R,R]^c)=\int\mathbf{1}_{\{|x|^p>R^p\}}\mu_\lambda(dx)$
    \begin{alignat*}{2}
        \mu_\lambda([-R,R]^c) & = \int\mathbf{1}_{\{|x|^p>R^p\}}\mu_\lambda(dx) \\
         & \leq \frac{1}{R^p}\int |x|^p\mu_\lambda(dx)\\
         & \leq \frac{c}{R^p} < \epsilon \espacio \forall \lambda \in \Lambda \espacio \text{si} \espacio R>\sqrt[\leftroot{-3}\uproot{3}p]{\tfrac{c}{\epsilon}}\, , %\sqrt[\leftroot{-3}\uproot{3}p]{\frac{c}{\epsilon}}
    \end{alignat*}
    Donde usamos que $\mathbf{1}_{\{\frac{|x|^p}{R^p}>1\}}\leq\frac{|x|^p}{R^p}$.
    \item[(e)] Sea $\epsilon>0$ y $K_\epsilon^i$ tal que $\displaystyle\sup_{\mu\in M^i}\mu((K^i_\epsilon)^c)\leq\epsilon$.
    \\ Definimos $K_\epsilon$ como $K_\epsilon=K^1_\epsilon\cup\dots\cup K^m_\epsilon$, que es compacto.
    \\ Entonces, $\forall \mu\in M$ tenemos
    $ \mu(K_\epsilon^c)\leq\mu((K^i_\epsilon)^c)\leq\epsilon$ (con $i$ tal que $\mu\in M^i$).
\end{enumerate}
\findem \negro
\end{proof}

\begin{theorem}[Prokhorov]
\label{theorem:pro}
Sea $M\subseteq\mathcal{M}(E), ((E,d))$ separable.
\begin{itemize}
    \item[(i)] $M$ tensa $\Longrightarrow$ $M$ relativamente compacta
    \item[(ii)] Si además $E$ es completo, la recíproca también es cierta.
\end{itemize}
\end{theorem}
\begin{proof}
\gris Ver Billingsley \cite{billing} \negro
\end{proof}

\begin{example}
Sean $(X_n)_{n\in\mathbb{N}}$ v.a., en $\mathbb{R}$ tal que $\displaystyle\sup_n\mathbb{E}(|X_n|^2)<\infty$ (i.e., sus leyes tienen momento de orden 2 acotado uniformemente). Entonces existe subsucesión $n_k\nearrow\infty$ y $X$ v.a., en $E$ tal que $X_{n_k}\mbox{ }\overset{ley}{\substack{\longrightarrow \\k \to \infty}}\mbox{ }X$.
\end{example}
\begin{proof}
\gris En efecto, sea $M$ como sigue
$$ M:=(\mu_n:=Ley(X_n))_{n\in\N}\subseteq\mathcal{P}(\R) \, .$$
Por ejemplo \ref{ejemplo:1_2_3} tenemos que $M$ es tensa. Entonces por Teorema \ref{theorem:pro} (Prokhorov), $M$ es relativamente compacta (secuencialmente). Entonces existen $\mu$ en $\mathcal{P}(\R)$ y una subsucesión creciente a infinito $n_k$ tal que $\mu_{n_k}\mbox{ }\substack{\Longrightarrow \\k \to \infty}\mbox{ } \mu$ 
\\ Sea $\edp = (\R,\mathcal{B}(\R),\mu)$ y consideremos $X:\Omega\mapsto\R$ la identidad ($X=Id$), \\ entonces $Ley(X)=\mu$, y $X_{n_k}\mbox{ }\overset{ley}{\substack{\longrightarrow \\k \to \infty}}\mbox{ } X$
\findem
\negro
\end{proof}

\subsubsection{Convergencia débil y función característica}
Ahora veamos una aplicación: función característica.

\begin{definition}[Función Característica]
Sea $X$ v.a., en $\mathbb{R}^d$, $\mu=Ley(X)$. Su función característica es: $ \espacio \forall \xi\in\mathbb{R}^d$,
\begin{alignat*}{2}
    \varphi_\mu(\xi) & = \mathbb{E}(e^{i\langle\xi,X\rangle}) \in\mathbb{C} \\
     & =\int \cos\langle\xi,x\rangle\mu(dx) +i\int sen\langle x,\xi\rangle\mu(dx) \, .
\end{alignat*}
% $$\varphi_\mu(\xi) = \mathbb{E}(e^{i\langle\xi,X\rangle}) \in\mathbb{C}$$
% $$ =\int \cos\langle\xi,x\rangle\mu(dx) +i\int sen\langle x,\xi\rangle\mu(dx) \, .$$
También se denota $\varphi_X$. Notar que siempre está bien definida para cualquier v.a. $X$ y para todo $\xi\in\R^d$.
\end{definition} 

\vspace{2cm}
\begin{property}
\beforeitemize
\begin{itemize}
    \item[(i)] $|\varphi_\mu(\xi)|\leq 1,  \forall \xi \in\mathbb{R}^d$ y $\varphi_\mu(C)=1$.
    \item[(ii)] $\varphi_\mu$ es uniformemente continua.
    \item[(iii)] La función característica $\varphi_\mu$ caracteriza las medidas finitas $\mu$:
    $$ \varphi_\mu(\xi)=\varphi_\nu(\xi) \forall \xi\in\mathbb{R}^d\Longrightarrow\mu=\nu \, .$$
\end{itemize}
\end{property}
\begin{proof}
\gris
\beforeitemize
\begin{itemize}
    \item[(i)] Fácil.
    \item[(ii)] Sean $\xi,\zeta\in\R^d$, notemos que
    \begin{alignat*}{2}
        |\varphi_\mu(\xi)-\varphi_\mu(\zeta)| & = |\E(e^{i\langle\xi,X\rangle}-e^{i\langle\zeta,X\rangle})| \\
         & = |\E([e^{i\langle\xi-\zeta,X\rangle}-1]\cdot e^{i\langle\zeta,X\rangle})| \\
         & \leq \E(|e^{i\langle\xi-\zeta,X\rangle}-1|)\cdot 1 \, .
    \end{alignat*}
    Llamemos $u=\xi-\zeta$. Por T.C.D. $\E(|e^{i\langle\xi-\zeta,X\rangle}-1|)\,\substack{\longrightarrow \\ |u|\to 0}\mbox{ }0$
    \item[(iii)]
    Para $d=1$ el esquema de demostración es el siguiente: % \ejercicio \gris
    \\ (basta probar que $\langle \mu,f\rangle = \langle\nu,f\rangle \espacio \forall f\in\mathcal{C}_0(\R)$)
    \begin{itemize}
        \item Usando Teorema de Stone-Weirstrass, se prueba para cada $L>0$ que las combinaciones lineales de $f_n$ son densas en $\mathcal{C}_0([-L,L])$ para la convergencia uniforme.
        \item Por consiguiente, también son densas en las funciones $f_L$ $L$-periódicas en $\R$, dadas por las traslaciones de funciones $f\in\mathcal{C}_0([-L,L])$ (con respecto a la convergencia uniforme en $\R$)
        \item Para esas funciones $f_L$, la hipótesis $\varphi_\mu=\varphi_\nu$ implica que $\langle\mu,f_L\rangle=\langle\mu,f_L\rangle$ (probar esto queda de \ejercicio\gris)
        \begin{alignat*}{2}
        \therefore |\langle \mu,f\rangle-\langle\mu,f\rangle| & \leq |\langle \mu,f_L\rangle-\langle\mu,f_L\rangle| + |\langle \mu,f-f_L\rangle-\langle\mu,f-f_L\rangle| \\
         & \leq 2\cdot \|f\|_{Unif}(\mu([-L,L]^c)+\mu([-L,L]^c)) \\
         & \leq \epsilon , \espacio \mbox{para }L\mbox{ suficientemente grande} \, .
        \end{alignat*}
    \end{itemize}
    El caso $d\geq 2$ se deja de \ejercicio
\end{itemize}
\findem
\negro
\end{proof}

\vspace{3cm}
\begin{theorem}[Lévy]
\label{theorem:levy}
Sea $(\mu_n)_{n\in\mathbb{N}}\subset\mathcal{P}(\mathbb{R}^d)$,
\begin{enumerate}
    \item[(i)] Supongamos $\exists \mu\in\mathcal{P}(\R^d)$ tal que $\mu_n\mbox{ }\substack{\Longrightarrow \\ n\to\infty}\mbox{ }\mu$, luego 
    $$ \varphi_{\mu_n}(\xi)\mbox{ }\substack{\longrightarrow \\ n\to\infty}\mbox{ }\varphi_\mu(\xi), \forall\xi\in\mathbb{R^d} \, .$$
    \item[(ii)] Supongamos que $\varphi_{\mu_n}(\xi)\mbox{ }\substack{\longrightarrow \\ n\to\infty}\mbox{ }\mu(\xi), \forall\xi\in\mathbb{R}^d$ con $\varphi$ continua en $0$. Entonces, $$\exists\mu\in\mathcal{P}(\R^d)\mbox{ tal que }\mu_n\mbox{ }\substack{\Longrightarrow \\ n\to\infty}\mbox{ }\mu\mbox{ y }\varphi=\varphi_\mu \, .$$
\end{enumerate}
\end{theorem}
Este es un teorema basado en lo visto anteriormente. Para su demostración se usará lo siguiente:
\begin{lemma}
\label{lemma:levy}
Si $\varphi_{\mu_n}(\xi)\conv\varphi(\xi), \espacio \forall \xi$, con $\varphi$ continua en $0$, entonces $(\mu_n)_{n\in\mathbb{N}}$ es tensa.
\end{lemma}
\begin{proof}
\gris
\beforeitemize
\begin{itemize}
    \item Para cada $i=1,\dots,d$, sean $(\mu_n^i)_n\subseteq \mathcal{P}(\R)$ las leyes marginales de $(\mu_n)_{n\in\N}$. Es decir, si $\mu_n=Ley(X_n^1,\dots,X_n^d)$, $\mu_n^i=Ley(X_n^i)$
    \item Basta probar que cada familia $(\mu_n^i)_{n\in\N}$ es tensa.
    Esto puesto que si son tensas, $\forall \epsilon>0, \exists K_\epsilon^i\subseteq\R$ compacto tal que $\forall n\in\N$, $\mu_n^i((K_\epsilon^i)^c)\leq\epsilon$. Entonces tomando $K_\epsilon:=K\epsilon^1\times\dots\times K_\epsilon^d$, que es un compacto en $\R^d$ tenemos que
    $$ \mu_n(K^c_\epsilon)\leq \displaystyle\sum^d_{i=1}\mu_n^i((K_\epsilon^i)^c)\leq d \epsilon \, .$$
    $\therefore \, (\mu_n)_{n\in\N}$ es tensa. 
    \\ Además, por continuidad en $0$, $$\varphi_{\mu_n^i}(t)=\varphi_{\mu_n}(0,\dots,0,t,0,\dots,0)\conv \varphi(0,\dots,0,t,0,\dots,0)\, ,$$
    Con $t\in\R$ en la posición $i$-ésima. Luego basta suponer que $d=1$ y probar el resultado en este caso.
    \item Sea $\delta>0$, entonces para todo $\nu\in\mathcal{P}(\R)$
    \begin{alignat*}{2}
        \displaystyle \frac{1}{\delta}\int^\delta_0 [\int_\R 1-cos(tx)\nu(dx)]dt & = \int_\R(1-\frac{sen(x\delta)}{x\delta})\nu(dx) \\
         & \geq  \int_{|x\delta|>1}(1-\frac{sen(x\delta)}{x\delta})\nu(dx) \\
         & \geq c\nu(\{x:|x|>\frac{1}{\delta}\}) \, .
    \end{alignat*}
    % $$ \displaystyle \frac{1}{\delta}\int^\delta_0 [\int_\R 1-cos(tx)\nu(dx)]dt  = \int_\R(1-\frac{sen(x\delta)}{x\delta})\nu(dx)$$
    Donde usamos Teorema de Fubini. Luego queda que
    $$ \displaystyle \mu_n([-\frac{1}{\delta},\frac{1}{\delta}]^c)\leq \frac{c}{\delta}\int^\delta_0 1-Re(\varphi_{\mu_n}(t))dt \, .$$
    Y entonces
    $$ \displaystyle \limsup_{n\in\N} \mu_n([-\frac{1}{\delta},\frac{1}{\delta}]^c)\leq \frac{c}{\delta}\int^\delta_0 1-Re(\varphi_{\mu_n}(t))dt\mbox{ }\substack{\longrightarrow \\ \delta\to0}\mbox{ }0 \, ,$$
    donde nuevamente hemos usado la continuidad de $\varphi$ en $0$.
    \\ De este modo, $\forall \epsilon>0$, $\exists L=\frac{1}{\delta}>0$ tal que 
    $$ \limsup_{n\in\N} \mu_n([-L,L]^c)\leq \frac{\epsilon}{2} \, ,$$
    y entonces existe $n_0\in\N$ tal que $\displaystyle \sup_{n\geq n_0}\mu_n([-L,L]^c)\leq \epsilon$
    \item Como cada $(\mu_0),\dots,(\mu_{n_0-1})$ es tensa, por ejemplo \ref{ejemplo:1_2_3}, $\{(\mu_0),\dots,(\mu_{n_0-1})\}$ también es una familia tensa.
    \\ Luego, $\exists K_0\leq \R$ compacto tal que $\mu_k(K_0^c)\leq\epsilon$, $\forall k = 0,\dots,n_0-1$.
    \\ Tomando $K_\epsilon:=K_0\cup [-1,1]$ obtenemos finalmente $\displaystyle\sup_{n\in\N}\mu_n(K_\epsilon^c)\leq \epsilon$.
\end{itemize}
\findem
\negro
\end{proof}

\begin{proof} del Teorema \ref{theorem:levy} (Lévy)
\gris
\begin{itemize}
    \item[(i)] Directo, pues $x\mapsto cos(\langle\xi,x\rangle)$, $\x\mapsto sen(\langle\xi,x\rangle)$ son continuas acotadas.
    \item[(ii)] Acá usaremos el Lema \ref{lemma:levy}
    \\ En efecto, por Teorema \ref{theorem:pro} (Prokhorov), $\exists\mu\in\mathcal{P}(\R^d)$ y $(\mu_{n_k})_{k\in\N}$ subsucesión tal que $\mu_{n_k}\mbox{ }\substack{\Longrightarrow \\k \to \infty}\mbox{ }\mu$, y gracias a (i) tenemos que $\varphi_{\mu_{n_k}}(\xi)\mbox{ }\substack{\longrightarrow \\k \to \infty}\mbox{ }\varphi_{\mu}(\xi)$, y por hipótesis, $\varphi_{\mu_{n_k}}(\xi)\mbox{ }\substack{\longrightarrow \\k \to \infty}\mbox{ }\varphi(\xi)$.
    Por lo tanto $\varphi=\varphi_\mu$ es una función característica.
    
    Veamos entonces que se tiene $\mu_n\convdebil\mu$. Si suponemos lo contrario, existe una subsucesión $(\mu_{n_j})_{j\in\N}$ y un $\epsilon>0$ tal que $D(\mu_{n_j},\mu)>\epsilon$.
    \\Sin embargo, la familia $(\mu_{n_j})_{j\in\N}$ es también tensa y entonces existe $\nu\in\mathcal{P}(\R^d)$ y $(\mu_{n_{j_k}})_{k\in\N}$ subsucesión de la subsucesión que cumple $\mu_{n_{j_k}}\mbox{ }\substack{\Longrightarrow \\k \to \infty}\mbox{ }\nu$.
    \\ Luego por usando el argumento anterior, $\varphi_\nu=\varphi=\varphi_\mu$, es decir, $\mu=\nu$. Esto es una contradicción puesto que $D(\nu,\mu)\geq\epsilon>0$.
\end{itemize}
\findem
\negro 
\end{proof}

\subsection{Teorema central del límite en varias variables} %clase 5
\begin{definition}[Distribución Gaussiana multivariada]
\label{gauss}
Decimos que $Z$ es vector Gaussiano de media $\mu$ y varianza $\Gamma$ si toda combinación de sus coordenadas es v.a. Gaussiana y $\mathbb{E}(Z)=\mu$, $Cov(Z)=\Gamma$. Esto se denota $Z\sim\mathcal{N}(\mu,\Gamma)$.
\end{definition}

\begin{remark}
Si $Z\sim\mathcal{N}(\mu,\Gamma)$ entonces tenemos $\varphi_Z(\xi)=\mathbb{E}(e^{i\langle\xi,Z\rangle})=e^{i\langle\xi,Z\rangle-\frac{\xi^T\Gamma\xi}{Z}}$.
\\ Cuando el espacio es $\mathbb{R}$, esto se convierte en $\varphi_Z(t)=e^{ita-\frac{t^2\sigma^2}{Z}}$, con $a=\E(Z)$ y $\sigma=\Var(Z)$.
\end{remark}

\vspace{2cm}
\begin{theorem}[T.C.L. multivariado]
\label{tcl}
Sean $X_1,\dots,X_n,\dots$ variables aleatorias independientes idénticamente distribuidas (i.i.d.) en $L^2$ % $\convley$
($\mathbb{E}(|X_n|^2)<\infty$). En $\mathbb{R}^d$, sean $\mu:=\mathbb{E}(X_n)$ 
la media de las v.a. y $\Gamma=Cov(X_n)$ matriz de varianza-covarianza (i.e., $\Gamma=(Cov(X_n^i,X_n^j))_{ij}^d$). 

Tomemos $\bar{X}_n:=\displaystyle\frac{1}{n} \sum^n_{k=1}X_k$, entonces
$$ \sqrt{n}(\bar{X}_n-\mu)\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }\mathcal{N}(0,\Gamma) \, .$$
\end{theorem}
Para demostrarlo usaremos el siguiente lema:
\begin{lemma}
\label{lemma:lema_tcl}
Sea $W$ v.a. real tal que $\mathbb{E}(W^2)<\infty$, entonces $$\varphi_W(t)=1+it\mathbb{E}(W)-\frac{t^2}{2}\mathbb{E}(W^2)+o(t^2) \, .$$
\end{lemma}
\begin{proof}
\gris Notemos usando Taylor que
$$ e^{ix} = \displaystyle 1+ix-\frac{x^2}{2} + R_2(ix) \, ,$$
donde $R_2(ix) = \displaystyle\int_0^{ix}e^{it}(ix-t)^2dt$. 
\\ Se puede demostrar (\ejercicio\gris) que $|R_2(ix)|\leq \frac{|x|^3}{6}$ y que $|R_2(ix)|\leq 2|x|^2$. Entonces tenemos
$$ \varphi_W(t)=\displaystyle\E(e^{itW})=1+it\E(W)-\frac{t^2}{2}\E(W^2)+\E(R_2(itW)) \, .$$
Basta ver que $\displaystyle\frac{1}{t^2}\E(R_2(itW)) \mbox{ }\overset{}{\substack{\longrightarrow \\t \to 0}}\mbox{ }0$. Para esto usamos lo siguiente:
$$\displaystyle \E(|R_2(itW)|) \leq \E(\min\{\frac{t^3|W|^3}{6},2t^2|W|^2\})<\infty \, ,$$ 
y como $\displaystyle\frac{1}{t^2}\E(R_2(itW))\leq \E(\min\{\frac{t|W|^3}{6},2|W|^2\}) $, podemos concluir usando T.C.D. . \findem
\end{proof}
% \vspace{2cm}

\begin{proof}[Demostración de TCL multivariado \ref{tcl}]
%\vspace{.5cm} \\
\gris Denotemos $Z_n = \sqrt{n}(\bar{X}_n-\mu)$ y sea $Z\sim\mathcal{N}(0,\Gamma)$. 
\begin{itemize}
    \item Por Teorema \ref{theorem:levy} (Lévy), basta probar que $\varphi_{Z_n}(\xi)\conv \varphi_Z(\xi), \espacio \forall\xi\in\R^d$.
    \item Primero notemos que podemos escribir $Z_n=\displaystyle \frac{\sum^n_{k=1}(\bar{X}_k-\mu)}{\sqrt{n}}$. Entonces %$\varphi_{Z_n}(\xi)$ como
    $$\displaystyle\varphi_{Z_n}(\xi) = \displaystyle \E(e^{\frac{i}{\sqrt{n}}\langle\xi,\sum^n_{k=1}(X_k-\mu)\rangle})=\displaystyle\E(\Pi^n_{k=1}e^{\frac{i}{\sqrt{n}}W_k}) \, ,$$
    donde $W_k:=\langle\xi,X_k-\mu\rangle$. Luego usando la independencia de las $W_k$ deducimos que 
    $$ \displaystyle\E(\Pi^n_{k=1}e^{\frac{i}{\sqrt{n}}W_k}) = (\varphi_W\bigg(\frac{1}{\sqrt{n}}\bigg))^n  \, .$$
    \item Ahora aplicaremos el Lema \ref{lemma:lema_tcl} a $W_k$. En efecto tomando $t=\frac{1}{\sqrt{n}}$, y usando que $\var(W)=\xi^T\Gamma\xi$, y que $\E(W)=0$, nos queda que
    \begin{alignat*}{2}
        \bigg(\varphi_W\bigg(\frac{1}{\sqrt{n}}\bigg)\bigg)^n & = \bigg(1+0-\frac{1}{2n}\xi^T\Gamma\xi+o\bigg(\frac{1}{n}\bigg)\bigg)^n \\
         & = \bigg(1+\frac{1}{n}\bigg(\frac{-\xi^T\Gamma\xi}{2}+\frac{1}{\frac{1}{n}}o\bigg(\frac{1}{n}\bigg)\bigg)\bigg)^n \\
         & \conv e^{-\frac{\xi^T\Gamma\xi}{2}}=\varphi_Z(\xi)  \, .
    \end{alignat*}
    La última convergencia se tiene ya que $\frac{-\xi^T\Gamma\xi}{2}+\frac{1}{\frac{1}{n}}o(\frac{1}{n}))$ está contenido en una bola de centro $\frac{-\xi^T\Gamma\xi}{2}$ y radio $\epsilon$. Luego para $N$ suficientemente grande, se está suficientemente cerca de $\frac{-\xi^T\Gamma\xi}{2}$.  % pendiente
\end{itemize}
\findem \negro
\end{proof}

\newpage
\section{Métodos de Monte Carlo}
Este capítulo está basado en los libros \textit{Monte -Carlo methods in financial engineering}, capítulo 1, P. Glasserman \cite{glass} y \textit{Processus de Markov et applications. Algorithmes, Réseaux, Génome et Finance}, capítulo 1, É. Pardoux\cite{pardoux}.
\subsubsection{Introducción}
La idea es:
\begin{itemize}
    \item calcular cantidades/integrales
    \item simular objetos matemáticos
\end{itemize}
Ambos deterministas, en base a números/objetos aleatorios.
\newp Origen: 1940-1950 (Fermi, Ulam, Metropolis, von Neumann) Relacionada a los avances en física nuclear: simulación computacional aleatoria de solucoones de la ecuación de fisión. \newline Aplicaciones en EDP, física, biología matemática, economía, ingeniería, finanzas, aprendizaje de máquinas, optimización, entre otras.

\subsection{Descripción de M.C.}
\newp Consideremos la siguiente integral:
$$ I:=\displaystyle\int_\Omega f(x)m(dx) \, ,$$
con $\Omega\subset\Rd,m\in\mathcal{P}(\Omega)$ y $f\in L^1(m)$.

Estas integrales son \textbf{esperanzas}: $I=\E(f(X))$ si $X\sim m$.
En particular si $m$ tiene densidad $g$,
$$ \E(f(X)) = \displaystyle \int_\Omega f(x)g(x)dx \, .$$
Luego \textit{simulando} $\vas$ \textit{replicas} i.i.d $\sim m$ podemos aproximar $I$, por \textbf{ley de grandes números}
$$ \displaystyle\frac{1}{n}\sum^n_{k=1}f(X_k)\convcs I \, ,$$
y también en $L^p$ si $f(X_1)\in L^p, p\in[1,\infty)$.
\newp \textbf{¿Como se puede llevar a cabo esta simulación?}

Los computadores pueden producir secuencias pseudoaleatorias de números $U_1,U_2,\dots,U_n,\dots$ que ``parecen'' i.i.d. $\sim \mathbb{U}([0,1])$. En realidad , son secuencias deterministas a valores en una grilla discreta muy fina, generadas con un sistema dinámico discreto con un ciclo larguísimo a partir de una \textit{semilla} (seed) generada ``aleatoriamente''.

A partir de $U_1,U_2,\dots,U_n,\dots$ podemos generar (en teoría y en la práctica) v.a., $\vas$ i.i.d. en $R^d$ de ley $m$ cualquiera.

%\newp \textbf{¿Cómo generamos $\vas$ i.i.d. $\sim m$ más generales que la uniforme?}
%
%Veremos algunos métodos a continuación
%
\newp \textbf{¿Qué tan buena es la aproximación (aleatoria)} $\displaystyle\frac{1}{n}\sum^n_{k=1}f(X_k)\approx I=\E(Y_1)$?

Sabemos que si $Y_k:=f(X_k)\in L^2$, tenemos la siguiente \textbf{cota para el error cuadrático medio}:
$$ \E[|\displaystyle\frac{1}{n}\sum^n_{k=1}f(Y_k)-\E(Y_k)|^2]\leq\frac{\var(Y_1)}{n} \, .$$
En efecto,
\begin{alignat*}{2}
   \E(|\mean{(Y_k-I)}|^2) &  =  \displaystyle\frac{1}{n^2}\sum_{k=1}^n\E((Y_k-Y)^2)+\frac{1}{n^2}\sum_{j\neq k}\frac{\E((Y_k-I)(Y_j-I))}{\E(Y_k-I)\E(Y_j-I)} \\
     & = \displaystyle\frac{1}{n^2}\sum_{k=1}^n\E((Y_k-Y)^2)=\frac{\var(Y_n)}{n}
\end{alignat*}
% $$\E(|\mean{(Y_k-I)}|^2)=\displaystyle\frac{1}{n^2}\sum_{k=1}^n\E((Y_k-Y)^2)+\frac{1}{n^2}\sum_{j\neq k}\frac{\E((Y_k-I)(Y_j-I))}{\E(Y_k-I)\E(Y_j-I)}=\displaystyle\frac{1}{n^2}\sum_{k=1}^n\E((Y_k-Y)^2)=\frac{\var(Y_n)}{n}$$

Además por T.C.L. (\ref{tcl}), obtenemos \textbf{intervalos de confianza asintóticos}: Sean $Y_1,Y_2,\dots \iid\in L^2 $ e $I=\mu=\E(Y_1)$:
$$ \sqrt{n}(\bar{Y_n}-\mu)\convley\normal(0,\sigma^2) \, ,$$
con $\sigma^2=\var(Y_1)$. Y entonces para $n$ grande,
$$ \P(|\bar{Y}_n-\mu|\leq\displaystyle\frac{\sigma}{\sqrt{n}}Z_{\frac{\alpha}{2}})\approx 1-\alpha \, ,$$
con $\alpha\in(0,1)$ y $Z_{\frac{\alpha}{2}}$ tal que $\P(\normal(0,1)>Z_{\frac{\alpha}{2}})=\displaystyle\frac{\alpha}{2}$, donde hemos usado la caracterización \textit{(vi)} del Teorema de Portmanteau (\ref{portmanteau}).
\newp Entonces \textbf{para lograr una precisión $\epsilon$ con probabilidad $\geq1-\alpha$}, i.e., $$\P(|\hat{Y}_n-\mu|\leq \epsilon)\geq1-\alpha \, ,$$ \textbf{tenemos que tomar n tal que}:
$$ \displaystyle\frac{\sigma Z_{\frac{\alpha}{2}}}{\sqrt{n}}\leq\epsilon \espacio \ssi \espacio n \geq \frac{\sigma^2 Z^2_{\frac{\alpha}{2}}}{\epsilon^2} \, .$$
\vspace{1cm}
\begin{remark}
\beforeitemize
\begin{itemize}
    \item El $n$ requerido para lograr una precisión $\epsilon$ dada no depende de la dimensión $d$, contrariamente a métodos deterministas, donde aproximar $\int_{[0,1}f(x)dx$ con precisión $\epsilon$ para $f$ Lipschitz requiere $\epsilon^{-d}$ evaluaciones, lo cual es impracticable para $d$ no pequeño ($>3$).
    \item El $n$ requerido para una precisión dada será mejor (más pequeño) si $\sigma^2$ es más pequeño. Esto es importante pues si disponemos de $Y_1,\dots,Y_n \mbox{ e } Y_1',\dots,Y_n' \iid \tq \E(Y_1)=\E(Y_1')=\mu$ y $\sigma^2=\var(Y_1)<\sigma^2'=\var(Y_1')$ entonces es preferible hacer M.C. con $Y_1,\dots,Y_n,\dots$ para aproximar $\mu$, siempre y cuando el costo de simular cada réplica de $Y_n$ y de $Y_n'$ sea similar.
    \item En general ¿cómo comparar entre dos M.M.C?\newline
    Consideremos:
    \begin{itemize}
        \item MC(1) v.a. $\iid Y_1,\dots,Y_n,\dots$ con costo $C$ por réplica
        \item MC(2) v.a. $\iid Y_1',\dots,Y_n',\dots$ con costo $C'$ por réplica
    \end{itemize}
    Para obtener una precisión $\epsilon>0$ con probabilidad $\geq 1-\alpha$ necesitamos alguno de los siguientes:
    $$ n\approx\nmc \mbox{ con MC(1) a costo total }Cn$$
    $$ n'\approx\displaystyle\frac{\sigma^2' Z^2_{\frac{\alpha}{2}}}{\epsilon^2}\mbox{ con MC(2) a costo total }C'n'$$
    Luego MC(1) es preferible a MC(2) ssi:
    $$ Cn<C'n'\ssi C\var(Y_1)<C'\var(Y'_1) \, .$$
    \item En general \textbf{no necesariamente conocemos} $\sigma^2=\var(Y_1)$, que es necesario para los análisis anteriores.
    \newline Sin embargo podemos estimarlo mediante una simulación \textit{piloto} (más pequeña y previa a calcular (I)) usando el estimador (insesgado):
    $$ \sigma^2\approx\displaystyle\frac{1}{q-1}\sum^q_{k=1}(\bar{Y}_q-Y_k)^2 \, ,$$
    con $q\approx10^2$ o $10^3$.
\end{itemize}
\end{remark}
\subsection{Simulación de variables aleatorias reales}  % clase 6 2 septiembre
Asumimos que podemos simular $(U_n)_{n\in\N} \iid \sim \unif$. Veremos como simular a partir de ellas realizaciones $(X_m)_{m\in \N}$ de v.a. de otras leyes. En teoría, basta una v.a. $\unif$ para simular variables aleatorias en cualquier espacio $(E,d)$ polaco. Más aún se tiene:
\begin{theorem}[de Representación de Skorokhod]
\label{sko}
Sean $X_n,n\in\N$, $X$ v.a. en $(E,d)$ polaco, tal que $X_n\convley X$. Entonces $\exists Y_n:[0,1]\to E,n\in\N$, $Y:[0,1]\to E$ medibles tal que si $U\sim\unif$,
$$ Y_n(U) \igualley X_n\mbox{,  } Y(U)\igualley X \mbox{ y }Y_n(U)\convcs Y(U) \, .$$
\end{theorem}
\begin{proof}
\gris En Billingsley \cite{billing} \negro
\end{proof}
\begin{remark}
Lamentablemente las funciones $Y_n,Y$ no son construibles.% (ver sección \ref{metgen}).
\\ En $E=\R$ si se pueden definir como $Y_n=F^{-}_{\mbox{ }X_n},Y=\Finvgen$, donde $F^-$ se será enunciado más adelante en definición \ref{def:invgen}.
\end{remark}
\vspace{2cm}\\
A continuación algunas \textbf{variables clásicas} que si se pueden simular con $\unif$.
\subsubsection{Bernoulli, Binomial y Geométrica}
\begin{definition}[Variable Bernoulli]
Sea $p\in(0,1)$ una v.a. Bernoulli $X$ puede realizarse como:
$$ X:=\mathbf{1}_{[0,p]}(U) \espacio \mbox{ con } \espacio U\sim\unif \, .$$
Esto se denota $ X\sim Ber(p)$.
\end{definition}
% Su \textbf{costo de simulación por réplica} está dado por:
% $$ C(Ber(p)) = C(U) + evaluación $$
\begin{definition}[Variable Binomial]
Sea $p\in(0,1),N\in\N$, una variable binomial $X$ está dada por:
$$ X:=\mathbf{1}_{[0,p]}(U_1)+\mathbf{1}_{[0,p]}(U_2)+\dots+\mathbf{1}_{[0,p]}(U_N)$$
Lo denotamos $X\sim Bin(p\,;N)$
\end{definition}
% Su \textbf{costo de simulación por réplica} está dado por:
% $$ C(Bin(p,N)) \approx N C(Ber(p)) \, .$$
\begin{definition}[Variable Geométrica]
Sea $p\in(0,1)$, decimos que $X$ es una variable geométrica si está dada por:
$$ X=\inf\{k\geq1:U_k\leq p\} \, .$$
Esto se denota $X\sim Geo(p)$
\end{definition}
% Su \textbf{costo de simulación por réplica} está dado por:
% $$ C(Geo(p)) \approx \displaystyle\frac{1}{p}C(Ber(p)) \, .$$
\begin{remark}[Costos de simulación por réplica]
Para las variables descritas anteriormente, los costos de simulación por réplica (denotado $C(\cdot)$) están dados por:
\begin{itemize}
    \item \textbf{Bernoulli}
    $$ C(Ber(p)) = C(U) + evaluación $$
    \item \textbf{Binomial}
    $$ C(Bin(p,N)) = N C(Ber(p))$$
    \item \textbf{Geométrica}
    % $$ C(Geo(p)) \approx \displaystyle\frac{1}{p}C(Ber(p)) $$
    $$ C(Geo(p)) = Geo(p)\cdot C(Ber(p))$$
\end{itemize}
El costo $C(Geo(p))$ es aleatorio, y lo aproximamos usando la esperanza:
% Agregar cosas % 43min clase 6
$$ Geo(p)\cdot C(Ber(p))\approx \E(Geo(p))C(Ber(p))=\displaystyle\frac{1}{p}C(Ber(p)) \, .$$
% \\ \textbf{¿Cómo estimar los costos de simulación por réplica?}
\\ \textbf{Además, para estimar el costo de simulación por réplica se puede proceder como sigue:}
\begin{enumerate}
    \item Simular $N(\approx100,1000)$ réplicas.
    \item Contar tiempo $T_N$ requerido.
    \item Costo/réplica $\approx \displaystyle\frac{T_N}{N}$.
\end{enumerate}
\end{remark}

\subsubsection{Variables reales generales}
Utilizaremos el \textbf{método de la inversa generalizada}.
\begin{definition}[Inversa generalizada]
\label{def:invgen}
Sea $X$ v.a. real, $F_X$ su función distribución. Definimos la inversa generalizada de $F_X:[0,1]\mapsto\R$,
$$ \Finvgen(t):=\inf\{x\in\R : F_X(x)>t\} \mbox{ con }t\in[0,1] \, .$$
\end{definition}
\begin{proposition}
\label{propinvgen}
Sea $X$ variable aleatoria real y $F^-_{\mbox{ }X}$ su inversa generalizada. Si $U\sim\unif$, entonces tenemos: $$\Finvgen(U) \sim Ley(X) \, .$$
\end{proposition}
Esta es la base del método de la inversa generalizada. Podemos entonces simular la ley de $X$ usando uniformes.
\begin{remark}
\beforeitemize
\begin{itemize}
    \item $\Finvgen$ es creciente continua a la derecha % \\ \ejercicio
    \item Si $F_X$ es creciente estricta $\Finvgen=\Finv$ \\ Es por esto que se llama inversa generalizada, pues cuando la inversa existe entonces coincide con ella. % \ejercicio 
    \newline Bajo la condición crecimiento estricto la Proposición \ref{propinvgen} es directa puesto que:
    $$ \P(\Finv(U)\leq x)= \P(U\leq F_X(x))=F_X(x) \, .$$
    \item En general, $\Finvgen$ es \textit{inversa por la derecha}: $F_X\circ \Finvgen=Id$ \\ % \ejercicio
\end{itemize}
\end{remark}
\demejercicio
\begin{proof}[Demostración de Proposición \ref{propinvgen}]
\gris Sea $X$ variable aleatoria real y $F^-_{\mbox{ }X}$ su inversa generalizada.
\begin{alignat*}{2}
    \Finvgen \geq x & \ssi \inf\{y\in\R : F_X(y)>U\}\geq x\\
     & \ssi \forall y \in \R, F_X(y)>U \Longrightarrow y\geq x \\
     & \ssi \forall y<x, F_X(y)\leq U \\
     & \ssi F_X(x^-)\leq U
\end{alignat*}
Luego tenemos
\begin{alignat*}{2}
    \P(\Finvgen(U) \geq x) & = \P( F_X(x^-) \geq U)\\
     & = \P ( F_X(x^-) < U) \\
     & = 1 - F_X(x^-) \\
     & = 1 - \P(X<x) \\
     & = \P(X\geq x)
\end{alignat*}
\findem
\end{proof}

En la figura \ref{fig:inv_gen} mostramos un ejemplo de inversa generalizada para una función con discontinuidades. En ella tenemos que $y=\Finvgen(s), \, x = \Finvgen(t)$ y $z=\Finvgen(r)=\Finvgen(r')$.
% \begin{figure}
%     \centering
%     \includegraphics[scale=0.17]{img/clase_06_pag_10.jpg}
%     \caption{Ejemplo de inversa generalizada}
%     \label{fig:inv_gen}
% \end{figure}
\begin{images}[\label{fig:inv_gen}]{Ejemplo de inversa generalizada}
	\addimage{clase_06_pag_10_01}{width=11cm}{Función $F_X$}
	\imagesnewline
    \addimage{clase_06_pag_10_02}{width=10cm}{Función $F^-_{\mbox{ }X}$}
\end{images}

\vspace{1.5cm}\\
Veamos como se aplica el método de la inversa generalizada a las variables aleatorias \textbf{exponencial} y \textbf{Poisson}.

\begin{definition}[Variable exponencial]
Dado $\lambda>0$ la v.a. $X$ se dice exponencial si su densidad está dada por:
$$ f_X(x) = \lambda e^{-\lambda x}, \forall x\geq 0 \, ,$$
lo cual se denota $X\sim exp(\lambda)$.
\end{definition}

Sea la v.a. $X\sim exp(\lambda)$ con $\lambda>0$. Notemos que su función distribución está dada por \\ $F_X(x)=1-e^{-\lambda x}$. Luego por Proposición \ref{propinvgen},
$$\Finvgen(z)=\displaystyle\frac{-\ln(1-z)}{\lambda} \, .$$
Además, como tenemos $1-U\igualley U$, se sigue el siguiente corolario:
\begin{corolary}[Simulación de una exponencial con inversa generalizada]
\label{simexp}  %%
Sea la v.a. $X\sim exp(\lambda)$ con $\lambda>0$, entonces
% Sea la v.a. $X\sim exp(\lambda)$ con $\lambda>0$. Notemos que su función distribución está dada por \\ $F_X(x)=1-e^{-\lambda x}$. Luego por Proposición \ref{propinvgen}, 
$$X=\displaystyle\frac{-\ln(1-U)}{\lambda}\sim exp(\lambda) \, .$$
\end{corolary}

\begin{definition}[Distribución Poisson]
Dado $\lambda>0$ decimos que la v.a. $N$ posee distribución de Poisson de parámetro $\lambda$ si su función de distribución está dada por:
$$ \P(N=n)=\displaystyle\frac{e^{-\lambda}\lambda^n}{n!} \, .$$
\end{definition}
\begin{remark}
Recordemos que la distribución de Poisson aproxima el número de éxitos cuando repetimos muchas veces un experimento con probabilidad baja de éxito cada vez. Equivale al máximo $n\in\N$ tal que la suma de $n$ exponenciales no sume más que $\lambda$.
\end{remark}
Usando esto y el Corolario \ref{simexp} tenemos lo siguiente:
\begin{corolary}[Simulación de una Poisson con inversa generalizada]
Sean $(U_i)_{i\in\N} \, \iid$ y sea $\lambda>0$, entonces
$$ N = \sup\{n\in\N : \displaystyle - \sum^n_{i=1}\ln(U_i)<\lambda\}$$
% Sea $N$ distribución de Poisson, luego podemos simular $N$ como
tiene ley de Poisson de parámetro $\lambda$.
\end{corolary}
\subsubsection{Gaussianas}
Consideremos dos distribuciones Gaussianas independientes (ver definición \ref{gauss}). Para simularlas usaremos la transformada de Box-Muller:

\begin{proposition}[Método de Box-Muller] 
Sean $U,V\sim\unif$, definimos:
$$R:=\displaystyle\sqrt{-2 \ln U}\, ,$$ y $$ \hat{\theta}:=(cos(2\pi V),sen(2\pi V)) \, .$$
Luego tenemos
$$ X=(X_1,X_2):=R\hat{\theta}\sim\normal(0,I_2) \, .$$
O sea, $X_1\indep X_2$ y $X_1,X_2\sim\normal(0,1)$.
\end{proposition}
\begin{proof}
\ejercicio. \gris Indicación: usar teorema de cambio de variables en $\R^2$. \negro
\end{proof}

\subsubsection{Variables aleatorias discretas cualquiera}
\label{disc}
Queremos simular $X$ v.a. discreta que toma valores $x_1<x_2<x_3<\dots<x_n<\dots$ $n\in\N$.
\begin{proposition}
Sea $X$ v.a. discreta y $p_n=\P(X=x_n)$ su función de probabilidad. Definimos una partición de $[0,1]:0=a_0<a_1<\dots<a_n\leq1$ mediante $a_n=\sum_{k\leq n}p_k$. Entonces podemos simular $X$ usando
$$ Y:=\displaystyle\sum_{n\in\N}x_n \mathbf{1}_{a_{n-1},a_n}(U)\igualley X \, .$$
\end{proposition}
\begin{proof}
Basta ver que $Y=\Finvgen(U)$. \ejercicio
\end{proof}

\subsubsection{Caso inversa no explícita}
Sea $X$ variable aleatoria real con densidad $f_X$ estrictamente positiva. Entonces la función de distribución $F_X$ es derivable y estrictamente creciente. Si \textbf{no conocemos} $F_X$ %la inversa 
o no la podemos invertir, usaremos el siguiente método:
\begin{definition}[Aproximación de Newton-Raphson]
Dado $U\in[0,1]$ queremos encontrar el único $\bar{x}$ tal que $F_X(\bar{x})=U$ (de este modo $\bar{x}=\Finvgen(U)$). Es decir, buscamos la única raíz de $G(x)=F_X(x)-U$.
\newline Luego el método de Newton-Raphson consiste en tomar:
$$ x_n:=x_{n-1}-\displaystyle\frac{G(x_{n-1})}{G'(x_{n-1})}=x_{n-1}-\frac{(F_X(x_{n-1})-U)}{f_X(x_{n-1})}\conv \bar{x} \, ,$$
e iterar hasta que ($F_X(x_k)-U$)$\leq\delta$ para $\delta$ pequeño dado.
\end{definition}

\subsection{Método general}
\subsubsection{Aceptación-rechazo}
\label{metgen}
Supongamos que queremos simular v.a. $X$ en $E$ espacio medible, con densidad conocida $f$ con respecto a una medida $\lambda(dx)$ en $E$, y que existe $Y$ v.a. en $E$ con densidad $g$ con respecto a $\lambda$ tal que:
\begin{itemize}
    \item sabemos simular fácilmente $Y$
    \item $\exists K>0 \tq f(x)\leq Kg(x)\mbox{ } \forall x \tq g(x)>0$
\end{itemize}
Veremos a continuación que podemos simular $X$ usando una sucesión $\iid (Y_n,U_n)_{n\geq 1}$ con $Y_n\igualley Y$, $U_n\sim\unif$ y $Y_n\indep U_n$. La segunda condición nos dice que no es necesario que $g$ ``domine'' $f$, sin embargo lo haría si amplificamos por una constante $K$.
\newp Vamos a necesitar la función siguiente:  % , $\forall x \in E$, definimos
% $$ \alpha(x):=\displaystyle\frac{f(x)}{Kg(x)}\in[0,1] \, .$$ 
$$ \alpha(x) := \begin{cases} 
    \displaystyle\frac{f(x)}{Kg(x)}  & \mbox{ si }g(x)>0\\
    0 & \mbox{ si }g(x)\leq0  \, .
\end{cases}$$
% Notemos que si $g(x)>0$,  $\alpha(x)\in[0,1]$. %$\frac{f(x)}{Kg(x)}\in[0,1]$.
Notemos que $\alpha(x)\in[0,1]$. Además $\displaystyle\int f \lambda(dx)\leq K\int g\lambda(dx)$, luego $K\geq 1$.
% \newp Luego se tiene la siguiente proposición:
\begin{proposition}[Método de aceptación-rechazo]
Sea $E$ un espacio medible cualquiera. Consideramos $X$ v.a. en $E$ con densidad $f$ e $Y$ v.a. en $E$ con densidad $g$ de modo que $\exists K>0 \tq $
$$f(x)\leq Kg(x) \espacio \forall x\in \{z\in E \,|\,g(z)>0 \}$$
\newline Sea $N=\inf\{n\geq 1:U_n\leq \alpha(Y_n)\}$, entonces tenemos:
\begin{itemize}
    \item[(i)] $N\sim Geom(\frac{1}{K})$
    \item[(ii)] $Y_N\sim Ley(X)$
    \item[(iii)] $Y_N\indep N$
\end{itemize}
\end{proposition}
\begin{proof}
\gris 
\begin{alignat*}{2}
    \P(Y_N\in A, N=m) & = \P(Y_m\in A, U_m \leq \alpha(Y_m),U_k>\alpha(Y_k),k=1,\dots,m-1) \\
     & = \P(Y_m\in A,U_m\leq \alpha(Y_m))(1-p)^{m-1} %\, .
     % \\ & = \displaystyle \int_A(\int^{\alpha(y)}_0 dt)g(y)\lambda(dy)(1-p)^{m-1}
     \\&  = \displaystyle \int_A \left(\int^{\alpha(y)}_0 dt\right)g(y)\lambda(dy)(1-p)^{m-1}
     \\ & = \frac{1}{K}\int_Af(y)\lambda(dy)(1-p)^{m-1} \, ,
\end{alignat*}
donde $p=\P(U_1\leq\alpha(Y_1))$. Luego tomando $A=E$ obtenemos
\begin{alignat*}{2}
    \P(N=m) & = \P(U_m \leq \alpha(Y_m))(1-p)^{m-1} \\
     & = p(1-p)^{m-1}
     \\ & = \frac{1}{K}(1-p)^{m-1} \, .
\end{alignat*}
Luego $p=\P(U_m\leq \alpha(Y_m))=\frac{1}{K}$, y $\therefore \espacio N\sim Geom(p)=\frac{1}{K}$.
\\ Así,
\begin{alignat*}{2}
    \P(Y_N\in A, N=m) & = \displaystyle \int_A f(y)\lambda(dy) p(1-p)^{m-1} \\
     & = \P(X\in A)p(1-p)^{m-1}
     \\ & =  \P(X\in A)\P(N=m) \, .
\end{alignat*} 
Sumando sobre $m\geq 1$ nos queda que
$$ \P(Y_N\in A) = \P(N=m)$$
Por ende $Y_n \igualley x$.
\\ Finalmente,
$$ \P(Y_N\in A, N=m) = \P(Y_N\in A)\P(N=m) $$
$\therefore \espacio Y_N\indep N$
\findem \negro
\end{proof}
% \vspace{.5cm}
\begin{remark}
\beforeitemize
\begin{itemize}
    \item Para obtener una realización de una v.a. de $Ley(X)$ se requiere simular una cantidad $N$ finita, pero aleatoria $\sim Geom(\frac{1}{K})$ (desconocida a priori) de pares $(Y_1,U_1),\dots,(Y_n,U_n),\dots$.
    \newline De ahí el nombre del método aceptación rechazo, pues si se cumple la condición aceptamos y si no rechazamos.
    \item $N\sim Geom(\frac{1}{k}) \implies \E(N)=K$. Luego, mientras más pequeño sea $K\geq 1$ tal que $f(x)\leq Kg(x)$, mejor, pues aceptamos el valor $Y_m$ más pronto.
    \item $K$ pequeño significa que g está más ajustada a $f$. Idealmente nos gustaría $K=1$, pero en ese caso tendríamos $0\leq g(x)-f(x)\implies 0\leq\int|g-f|\lambda(dx)=1-1=0\implies g\equiv f$ o sea, $X\igualley Y$. Si tuviésemos esto entonces ``ya sabríamos simular $X$'' ($Y$ es una v.a. que sabemos simular fácilmente).
\end{itemize}
\end{remark}

\subsubsection{Simulación condicional a subconjunto}
Como aplicación tenemos el siguiente corolario, donde \textbf{no necesitamos simular las uniformes}:
\begin{corolary}[Simulación de una v.a. $Y$  ``condicional a que $Y\in A$'']
\label{corclase7}
\newline Sean $(Y_n)_{n\geq 1}$ réplicas $\iid$ (simulables) de una v.a. $Y$ en $E$ y $A\subset E$. Sea $N:=\inf\{n\geq 1: Y_n\in A\}$, entonces
$$ \hat{Y}:=Y_N\sim Ley(Y|Y\in A) \espacio \mbox{ y }\espacio Y_N\indep N \, .$$
\end{corolary}
\begin{proof}
\gris
$X\sim Ley(Y|Y\in A)$ tiene densidad $f$ con respecto a $\lamnda=Ley(Y)$, dada por
$$ f(x) = K\mathbf{1}_A(x) = \P(Y\in A)^{-1}\mathbf{1}_A(x)\leq K g(x) \, ,$$
con $g\equiv 1$ la densidad $Y$ con respecto a $\lambda$.  Por el método aceptación-rechazo, $Y_N\sim Ley(Y|Y\in A)$, para $N=\inf\{n\geq1:U_n\leq\alpha(Y_n)\}$, donde $(U_n)_{n\in\N}\indep (Y_n)_{n\geq1}$ son $\iid$ y distribuyen $\unif$. Pero $U_n\leq\alpha(Y_n)=\mathbf{1}_A(Y_n)\Longleftrightarrow Y_n\in A$
\findem
\negro
\end{proof}

% \begin{proposition}[Caso particular de Corolario \ref{corclase7}]
\subsection{Variables condicionadas a estar en un intervalo}
\begin{proposition}
Consideramos $X\sim \R$ y $A=[a,b]$. Sea $V\sim\mathbb{U}([F_X(a),F_X(b)])$.
Notemos que 
$$V\igualley F_X(a)+U(F_X(b)-F_X(a))$$
con $U\sim \unif$. Entonces
$$ \Finvgen (V) \sim Ley(x \, |\, x\in[a,b]) \, .$$
\end{proposition}
\begin{proof}
\ejercicio
\newline \gris Indicación: demostrar y utilizar: $F_{\{x|x\in[a,b]\}}(x)=\displaystyle\frac{F_X(x)-F_X(a)}{F_X(b)-F_X(a)}$. \negro
\end{proof}
\vspace{1cm}
\subsection{Técnicas de reducción de varianza en M.M.C}
Para una presentación sucinta de estos temas ver el capítulo 1 del libro \textit{Processus de Markov et applications. Algorithmes, Réseaux, Génome et
Finance}, capítulo 1, É. Pardoux \cite{pardoux} y con el algo más de profundidad \textit{Monte -Carlo methods in financial engineering} de P. Glasserman, capítulo 1. Para más detalles ver Asmussen ``Stochastic Simulation'' \cite{asm}.
\newp \textbf{Idea}: si disponemos de $Y_1,\dots,Y_n,\dots \iid$ para aproximar $I=\E(Y)$, el error cometido será menor si $\sigma^2=\var(Y)$ es menor (para igual $\E(Y)$).
\begin{itemize}
    \item $\E(|\bar{Y}_n-I|^2)=\displaystyle\frac{\var(Y)}{n}$
    \item con un nivel de confianza $\alpha\in(0,1)$ dado, $\P(|\bar{Y}_n-I|<\epsilon)\gtrsim 1-\alpha$ si $n\geq\nmc$
\end{itemize}
Luego si podemos simular $Y'_1,\dots,Y'_n$ tal que $\E(Y')=I$ y $\var(Y')<\var(Y)$ (a costo comparable), necesitaremos menos ``$n$''s y seremos más eficientes.
\newp Los métodos de reducción de varianza que veremos son los siguientes:
\begin{enumerate}
    \item Variable de control (\ref{control})
    \item Variables antitéticas (\ref{antitéticas})
    \item Muestreo preferencial (\ref{preferencial})
    \item Muestreo estratificado (\ref{estratificado})
\end{enumerate}
Notar de todos modos que para comparar dos métodos de calcular $I=\E(Y)$, de todas maneras hay que considerar el costo ``por réplica''.

\subsubsection{Variable de Control}
\label{control}
Supongamos que podemos simular $Y=f(X)$ y que conocemos $\E(h(X))$ explícitamente para cierta función real $h$, que llamaremos  \textbf{variable de control}.
\newp Disponemos entonces de muchos ``estimadores'' de $I$ ``posibles'': para cada $c\in\R$,
$$ \sumfx+c\mbox{ }[\frac{1}{n}\sum_{k=1}h(X_k)-\E(h(X))] 
 =\frac{1}{n}\sum_{k=1}^n[f(X_k)+c(h(X_k)-\E(h(X))]
 =\frac{1}{n}\sum^n_{k=1}\varphi_c(X_k) \, ,$$
con $\E(\varphi_c(X_1))=I$.\hspace{.3cm} ¿Cual es su varianza?
\begin{alignat*}{2}
    \var(\varphi_c(X_1)) & = \var(f(X_1)+ch(X_1)) \\
     & = \var(f(X_1))+c^2\var(h(X_1))+2\mbox{ }c\mbox{ }Cov(h(X_1),f(X_1)) \, .
\end{alignat*}
% $$ \var(\varphi_c(X_1))=\var(f(X_1)+ch(X_1)) = \var(f(X_1))+c^2\var(h(X_1))+2\mbox{ }c\mbox{ }Cov(h(X_1),F(X_1))$$
De lo anterior se desprende que la varianza disminuye cuando se cumple:
$$ c^2 Var(h(X)) + 2c Cov(h(X),f(X)) < 0 $$

Además podemos optimizar en $c\in\R$. Queda:
$$ c^*=\displaystyle\frac{-Cov(f(X_1),h(X_1))}{\var(h(X_1))} \, .$$
%i.e., hacemos que los dos términos finales se cancelen. Así quedará sólo la varianza de $f(X_1)$. 
Substituyendo arriba:
$$ \var(\varphi_{c^*}(X_1))=\sigma^2-\displaystyle\frac{Cov(f(X_1,h(X_1)))^2}{\var(h(X_1))} \, .$$
Luego $\var(\varphi_{c^*}(X_1))<\sigma^2$ si $Cov(f(X_1,h(X_1))\neq 0$, i.e., reducimos la varianza del estimador.
\newp En general, quizás no conocemos $Cov(f(X_1,h(X_1))$ ni $\var(h(X_1))$. Pero los podemos estimar con $q$ (pequeño) simulaciones ``piloto''. Primero, usando la Ley de grandes números tenemos que,
$$ \displaystyle\frac{\sum_{k=1}^q(Y_k-\bar{Y}_q)(Z_k-\E(Z_1))}{q-1} \mbox{ }\substack{\longrightarrow \\ q\to\infty}\mbox{ } Cov(Y_1,Z_1) \, ,$$
entonces
$$ Cov(f(X_1,h(X_1)))\approx \displaystyle\frac{\sum_{k=1}^q(Y_k-\bar{Y}_q)(Z_k-\E(Z_1))}{q-1} \, ,$$
con $Y_k=f(X_k),Z_k=h(X_k)$ y 
$$\var(h(X_1))\approx \displaystyle\frac{\sum_{k=1}^q(Z_k-\E(Z_k))^2}{q-1} \, ,$$
luego,
$$ \hat{c}^*=\displaystyle\frac{\sum_{k=1}^q(Y_k-\bar{Y}_q)(Z_k-\E(Z_1))}{\sum_{k=1}^q(Z_k-\E(Z_k))^2} \, ,$$
y aproximaremos entonces $I$ mediante el siguiente promedio empírico:
$$ \displaystyle \frac{1}{n}\sum^q_{k=1}[f(X_k)-\hat{c}^*(h(X_k))-\E(h(X_1))]=\frac{1}{n}\sum^q_{k=1}\hat{\varphi}(X_k) \, .$$
\begin{example}[Ejemplo ``juguete'' de aplicación de variable de control]
Queremos calcular $I=\int^1_0e^xdx$ (ya sabemos que esto vale $e-1$).
\newp Método ``usual'': $I=\E(f(X))=\E(e^X),X\sim \unif$ donde $\var=\var(e^X)\approx0,242$
\newp Por otro lado, considerando $h(X_1)=\int^1_0(x+1)dx$ como variable de control:
$$ I=\int^1_0e^xdx - (\int^1_0(x+1)dx-\frac{3}{2})=\int^1_0(e^x-1-x)dx+\frac{3}{2} \, .$$
Tomando $c=-1,h(X)=x+1$. En este caso se puede verificar que:
$$ \var(e^X-1-x)\approx0.00437 \, .$$
Esto es 5 veces menos que $Var(e^X)$.
\end{example}

\subsubsection{Variables antitéticas (de a pares)}
\label{antitéticas}
Sea $I=\E(Y)$ con \hspace{.2cm} $Y=f(X)$ y \hspace{.2cm} $X$ v.a. en $\R$. Consideremos el M.M.C usual usando $(X_n)_{n\geq1}\iid \sim Ley(x)$, con $\var(f(x))=\sigma^2$. Para un número par $2n$ de réplicas $Y_k=f(X_k)$, $\bar{Y}_{2n}=\frac{1}{2n}(Y_1+\dots+Y_{2n})\conv I$ con $ECM_{2m}=\E(|\bar{Y}_{2n}-I|^2)=\displaystyle\frac{\sigma^2}{2n}$ (error cuadrático medio).
\newp Supongamos que simultáneamente podemos simular $(X_n')_{n\geq 1}\sim Ley(X)$ de modo que $X_n$ y $X_n'$ están \textbf{correlacionadas}, y la sucesión de pares $(X_n,X_n')_{n\geq1}$ son $\iid$ (independiente de los anteriores). Entonces también:
$$ \displaystyle\frac{1}{2}(\bar{Y}_n+\bar{Y}_n')=\frac{1}{2n}[(Y_1+Y_1')+\dots+(Y_n+Y_n')]\conv I\, .$$
\textbf{¿Con qué $ECM'_{2m}$?}
\newline $\displaystyle\frac{1}{2}(\bar{Y}_n+\bar{Y}_n')=\frac{1}{n}\sum^n_{k=1}\frac{(Y_k+Y_k')}{2}$ con $\frac{Y_k+Y_k'}{2}=\frac{f(X_k)+f(X_k')}{2},\mbox{ } k\geq1\mbox{ }\iid$ de media $\E(f(X))=I$.
\begin{alignat*}{2}
ECM_2n' & = \E\bigg(\bigg|\frac{1}{2}(\bar{Y}_n+\bar{Y}_n'-I)\bigg|^2\bigg) \\
& =\frac{1}{n}\var\bigg(\frac{Y_1+Y_1'}{2}\bigg) \\
& =\frac{1}{4n}[2\var(Y_1)+2\cov(Y_1,Y_1')] \\
& =\frac{1}{2n}(\sigma^2+\cov(f(X_1),f(X_1')))
\end{alignat*}
% $$ECM_2n'=\E(|\frac{1}{2}(\bar{Y}_n+\bar{Y}_n'-I)|^2)=\frac{1}{n}\var(\frac{Y_1+Y_1'}{2})=\frac{1}{4n}[2\var(Y_1)+2\cov(Y_1,Y_1')]=\frac{1}{2n}(\sigma^2+\cov(f(X_1),f(X_1')))$$
Luego si $\cov(f(X_1),f(X_1'))<0$, $$ECM'_{2n}<\frac{\sigma^2}{2n}=ECM_{2n} \, .$$
Aplicación usual: $X_n=U_n\sim\unif$, $X_n'=1-U_n\sim\unif$ (la realización $U_n$ es la misma para $X$ y $X'$), y las $2n$ variables aleatorias $U_1,\dots,U_n$, $1-U_1,\dots1-U_n$ requieren solo $n$ simulaciones.
\newp \textbf{¿Cuando se tiene $\cov(f(U_1),f(1-U_1))\leq 0$?}
Respondemos esto con el siguiente lema.
\begin{lemma}
Sea $f:[0,1]\to\R$ función monótona tal que $f\in L^2([0,1],dx)$. Entonces $\cov(f(U),f(1-U))\leq0$.
\end{lemma}
\begin{proof}  % 10:40
\gris
Sea $\tilde{U}\sim \unif$, $\tilde{U}\indep U$. Para $f$ creciente o decreciente siempre tenemos
$$ (f(U)-f(\tilde{U}))(f(1-U)-f(1-\tilde{U})) \leq 0 \, .$$
Tomando la esperanza se tiene que
$$ 2(\E(f(U)f(1-U))-\E(f(U))\E(f(1-\tilde{U})))\leq 0 $$
$$ \Longleftrightarrow \E(f(U)f(1-U))-\E(f(U))^2 \leq 0 \, .$$
\findem
\negro
\end{proof}

\textbf{Receta general}: Luego si tenemos $f:[0,1]\to\R$ monótona (por ejemplo $f=F^-_{\mbox{ }Z}$ una función de distribución inversa) usando $n$ réplicas $\unif$, el estimador: $$\displaystyle\frac{1}{n}\sum^n_{k=1}[\frac{f(U_k)+f(1-U_k)}{2}]$$ aproxima $I=\E(f(U))$ con varianza menor a $\sigma^2/2n$, es decir, menos de la mitad de la varianza $\sigma^2/n$ del estimador usual: $$\displaystyle\frac{1}{n}\sum^n_{k=1}f(U_k)$$ construido con las mismas $n$ réplicas.
%\textbf{Receta general}: cuando tengamos $f$ función monótona aplicada a una uniforme $U$, entonces en vez de usar
%$$\displaystyle\frac{1}{n}\sum^n_{k=1}f(U_k)$$
%Usamos $$\displaystyle\frac{1}{n}\sum^n_{k=1}[\frac{f(U_k)+f(1-U_k)}{2}] $$
%Y reduciremos la varianza a la mitad.

\subsubsection{Muestreo preferencial (\textit{importance sampling})}
\label{preferencial}
Queremos calcular $I=\E(f(X))=\int f(x)\mu(dx)$ con $X\sim \mu$ y supongamos que podemos simular $Z\sim\nu$ simulable tal que $\nu\approx\mu$ .  % con $Z\sim \nu$ simulable. 
Es decir $\exists L:\R^d\to\R_+$ densidad de Radon-Nikodym tal que $\nu(dx)=L(x)\mu(dx)$ con $L>0$ $\mu-c.s.$,
\newline Entonces 
$$I=\displaystyle\int\frac{f(x)}{L(x)}\nu(dx)\, ,$$ 
y podemos calcular $I$ también como 
$$I=\displaystyle\E\left(\frac{f(Z)}{L(Z)}\right) \, .$$
\textbf{¿Podemos encontrar $\nu$ así, de manera que además $\var(\frac{f(Z)}{L(Z)})<\var(f(X))$?}
\begin{remark}
\begin{alignat*}{2}
    \var\left(\displaystyle\left(\frac{f(Z)}{L(Z)}\right)\right) & = \E\left(\frac{f(Z)^2}{L(Z)^2}\right)-I^2\\
     & = \int \frac{f(z)^2}{L(z)^2}\mu(dz) - [\int f(x)\mu(dx)]^2 \\
     & = \int \frac{|f(z)|}{L(z)}\frac{|f(z)|}{L(z)}\mu(dz) - [\int f(x)\mu(dx)]^2 \\
     & = \int |f(z)|\frac{|f(z)|}{L(z)}\mu(dz) - [\int f(x)\mu(dx)]^2 \, .
\end{alignat*}
\end{remark}
Si $f\geq0$, tomando $L(z)=\displaystyle\frac{f(Z)}{I}$, o sea $\nu(dz)=\displaystyle\frac{f(Z)}{I}\mu(dz)$ y obtendríamos 
$$\var\left(\frac{f(Z)}{L(Z)}\right)=0 \, ,$$
pues $\frac{f(Z)}{L(Z)}=I$.
% \begin{remark}
\newp Lo anterior no se puede usar en la práctica porque \textbf{no conocíamos $I$}. Pero la fórmula $\nu(dz)=\frac{f(Z)}{I}\mu(dz)$ nos sugiere que sirve samplear de una ley parecida a $\mu$ (y equivalente) pero que da más peso a puntos $z$ tales que $f(z)$ es más grande, o sea que son \textbf{más importantes} en el cálculo de $\int f(x)\mu(dx)$.
% \end{remark}
\newp \textbf{Heurística}:
\begin{itemize}
    \item Buscar una medida $\tilde{\nu}$ ``parecida a $|f(x)|\mu(dx)$''
    \item Esta medida debe cumplir que $\nu(dx):=\displaystyle\frac{\tilde\nu(dx)}{\int\tilde{\nu}(dx)}$, sea ``\textbf{fácilmente simulable}'' donde $\int\tilde{\nu}(dx)$ es una constante de normalización calculable. 
\end{itemize}
\begin{example}[Aplicación ``artificial'' de muestreo preferencial]
Queremos aproximar $$I=\displaystyle\int^1_0\cos(\frac{\pi x}{2})dx=\frac{2}{\pi} \, ,$$ con $\mu(dx)=dx$ uniforme en $[0,1]$, $f(x)=\cos(\frac{\pi x}{2})$.\\
\newline Elegimos $\nu(dz)=\frac{z}{2}(1-z^2)dz=L(z)dz$ y aproximamos
$$ I\approx\displaystyle\frac{1}{n}\sum^n_{k=1}\cos(\frac{\pi z_k}{2})/L(z_k), \espacio Z_k \iid \sim \nu \, .$$
En la figura \ref{fig:pref} se grafican ambas funciones. Se puede mostrar que la varianza se reduce en factor mayor que $10$.
\begin{figure}
    \centering
    \includegraphics[scale=0.12]{img/clase_08_pag_11.jpg}
    \caption{Ejemplo de muestreo preferencial}
    \label{fig:pref}
\end{figure}
% \begin{images}[\label{fig:pref}]{Ejemplo de muestreo preferencial}
%     \addimage{clase_08_pag_11}{width=10cm}{}
% \end{images}
\end{example}

\subsubsection{Muestreo estratificado (\textit{stratified sampling})}
\label{estratificado}
Queremos calucular $I=\E(Y), Y$ v.a. $\in\R$ con $\sigma^2=\var(Y)$. 
\newline Supongamos que tenemos una partición $\mathcal{D}=\{D_1,\dots,D_k\}$ (``estratos'') de $\R$, conocemos \\ $p_i=\P(Y\in D_i)$, y sabemos simular $Y^j\sim Ley(Y | Y \in D_j)$ para cada $j=1,\dots,k$.
\begin{example}
En $\R$, $D_j$ de la forma $[a_j,b_j)$, vimos como simular $Y^j$ usando $F_Y^-$ y una v.a. $U[f_Y(a_j),f_X(b_j)]$.
\end{example}
Disponemos entonces de:
$$ n=n_1+\dots+n_k \mbox{ v.a. independientes }:\begin{cases}
Y^1_1,\dots,Y^1_{n_1} \iid \igualley Y^1 &\\
\dots & \\
Y^k_1,\dots,Y^k_{n_k} \iid \igualley Y^k &
\end{cases} $$
\textbf{¿Podemos estimar $I$ con $\var<\displaystyle\frac{\sigma^2}{n}=\var\left(\frac{1}{n}\sum^k_{j=1}Y_j\right)$ con $(Y_n)\igualley Y \iid$?}
Para responder a esto notemos que
\begin{alignat*}{2}
    \E(Y) & = \E(\E(Y|K)) \mbox{ con }K=j \mbox{ ssi }Y\in D_j\\
     & = \displaystyle\E(\sum^k_{j=1}\E(Y|K=j)\mathbf{1}_{k=j}) \\
     & = \displaystyle\sum^k_{j=1}p_j\E(Y^j) \, .
\end{alignat*}
Definimos
$$ \hat{Y}_n=\displaystyle\sum^k_{j=1}p_j\frac{1}{n_j}\sum^{n_j}_{i=1}Y^j_i \, .$$
\begin{remark}
\beforeitemize
\begin{itemize}
    \item $\E(\hat{Y}_n)=I$, es decir, $\hat{Y}_n$ que es un estimador insesgado de $I$.
    \item $\hat{Y}_n\to I$ c.s.  cuando  $n_1, \dots, n_k\to \infty $ con   $n=n_1+ \dots + n_k $, por Ley de Grandes N\'umeros. 
\end{itemize}
\end{remark}
% \begin{proof}
% \gris Probar que son insesgados y usar ley de grandes números en cada estrato. (\ejercicio)\negro
%\end{proof}
\textbf{¿Con qué ECM (varianza)?}
\newline Usando la independencia de las v.a.  $Y_{k}^j$,  tenemos: % En general:
$$ \var(\hat{Y}_n)=\displaystyle\frac{1}{n}\sum^k_{j=1}\frac{p_j^2}{n_j}\sigma^2_j \, .$$
Veamos que esto es menor que $\sigma^2/n$ \, . Usaremos la
\begin{proposition}[Fórmula de la ``varianza total'']
Sea $Y$ v.a. real y $K$ v.a. cualquiera, luego
$$ \var(Y)=\E(\var(Y|K))+\var(\E(Y|K)) \, .$$
Donde $\E(\var(Y|K))=\E((Y-\E(Y|K))^2|K)$.
\end{proposition}
\begin{proof}
\ejercicio
\end{proof}
Entonces aplicando lo anterior a  la v.a. $K$ definida como $K=j$ cuando $Y\in D_j$, obtenemos:
$$ \sigma^2\,=\,\displaystyle\sum^k_{j=1}p_j\sigma^2_j+\sum^k_{j=1}p_j(\E(Y^j)-\E(Y))^2\,\geq \,\sum^k_{j=1}p_j\sigma^2_j \, .$$
Aqu\'i, por un lado $\sum^k_{j=1}p_j\sigma^2_j$ corresponde a la esperanza de la varianza condicional, mientras que el término $\sum^k_{j=1}p_j(\E(Y^j)-\E(Y))^2$ es la varianza de la esperanza condicional (ver Figura \ref{fig:estrat}). 
\begin{figure}
    \centering
    \includegraphics[scale=0.17]{img/clase_09_pag_3.jpg}
    \caption{Ejemplo gráfico de muestreo estratificado}
    \label{fig:estrat}
\end{figure}

\newp Notemos tambi\'en que  $\sum^k_{j=1}p_j\sigma^2_j= \var(\hat{Y}_n)$ si elegimos  $n_j\approx p_jn$.  Vemos entonces que, en este caso,   si $\exists j\tq\E(Y^j)\neq\E(Y)$ tendremos que $\var(\bar{Y}_n)>\var(\hat{Y}_n)$, donde $\hat{Y}$ es el estimador usual.   Dicho de otro modo, basta con que haya algún estrato para el cual la esperanza no sea igual a la esperanza de $Y$, para que se reduzca la varianza con esta elecci\'on de $n_j$'s.


% \newp Se puede escoger $n_1,\dots,n_k$ tal que $n_1+\dots+n_k=n$ de manera aún mejor resolviendo:
\newp Más aún, podemos elegir los $n_j$'s de manera aproximadamente  \'optima,  resolviendo:
\begin{alignat*}{2}
    \displaystyle\min_{n_1,\dots,n_k\geq0}& & \sum^k_{j=1}\sigma^2_j\frac{p_j^2}{n_j}\\
    \mbox{s.a.  }& \mbox{    } & n_1+\dots+n_k=n 
\end{alignat*}
suponiendo los $n_j$ reales y aplicando KKT.  Obtenemos as\'i como soluci\'on entera:
$$ n_j\approx\displaystyle n\bigg(\frac{p_j\sigma_j}{\sum^k_{l=1}p_l\sigma_l}\bigg) \, .$$
Los $\sigma_j$ a su vez, se pueden estimar con simulaciones piloto.

% $$ \color{red} \E(Y^1) \espacio \color{blue} \E(Y^2) \espacio \color{negro} \E(Y)
% \espacio \color{green} \E(Y^3)\espacio  \color{magenta} \E(Y^4)$$
% $$ \color{red} Var(Y^1)=\sigma^2_1 \espacio \color{blue} Var(Y^2)=\sigma^2_2 \espacio \color{green} Var(Y^3)=\sigma^2_3 \espacio  \color{magenta} Var(Y^4)=\sigma^2_4$$
% \big
% $$ \color{red} Y^1_1,\dots,Y^1_{n_1} \espacio \color{blue} Y^2_1,\dots,Y^2_{n_2} $$
% $$ \color{red} \overset{ley}{=}\,Y^1\,\overset{ley}{=}\,Y|Y\in D_1 \espacio \color{blue} \overset{ley}{=}\,Y^2\,\overset{ley}{=}\,Y|Y\in D_2 $$
% $$\color{green} \overset{ley}{=}\,Y^3\,\overset{ley}{=}\,Y|Y\in D_3 \espacio  \color{magenta} \overset{ley}{=}\,Y^4\,\overset{ley}{=}\,Y|Y\in D_4 $$
% $$\color{green} Y^3_1,\dots,Y^3_{n_3} \espacio  \color{magenta} Y^4_1,\dots,Y^4_{n_4} $$

\newpage
\section{Cadenas de Markov (CM)}
\subsection{Recuerdo}
En esta sección recordaremos las bases de Cadenas de Markov. Para revisar el tema en profundidad, se puede consultar  ``Markov Chains'' de J. Norris \cite{norris} o ``Processus de Markov et applications'' de E. Pardoux \cite{pardoux}.
\subsubsection{Definición}
\begin{definition}[Cadena de Markov]
Sea $E$ un conjunto numerable, $P=(P_{xy})_{x,y\in E}$ matriz estocástica y $\lambda=(\lambda_x)_{x\in E}$ vector de probabilidad inicial.

La $(X_n)_{n\in N}$ sucesión de variables aleatorias con $X_n:\edp \to E$ se dice \textbf{Cadena de Markov} C.M.($\lambda,P$) (homog\'enea) si:
\begin{itemize}
    \item $\forall n \in N, \forall x_0,\dots,x_{n+1}\in E \, ,$
    $$ \P(X_{n+1}=x_{n+1}|X_n=x_n,\dots,X_0=x_0) = P_{x_n,x_n+1} \, .$$
    \item $\forall x_0\in E$, $\P(X_0=x_0)=\lambda_0 \, .$
\end{itemize}
\end{definition}

\begin{property}
Algunas consecuencias:
\begin{itemize}
    % \item $\P(X_{n+1}=x_{n+1}|X_n=x_n) = P_{x_nx_{n+1}}$ \, .
    \item $\P(X_{n+1}=y|X_n=x) = P_{xy}$ \, .
    \item $X$ es $CM(\lambda,P)$ si y solo si $$\P(X_0=x_0,\dots,X_n=x_n)=\lambda_{x_0}P_{x_0,x_1}\dots P_{x_{n-1}x_n}\, .$$
\end{itemize}
\end{property}

\subsubsection{Definiciones y propiedades importantes}
\begin{notation}
Denotaremos
\begin{itemize}
    \item $\P_\mu(\cdot)$ a la ley de $\xcm$ cuando $X_0\sim\lambda=\mu$ \, .
    \item $\P_x(\cdot)=\P_{\delta_x}(\cdot)$, con $\delta_x$ masa de Dirac en $x\in E$ \, .
\end{itemize}
\end{notation}

\begin{definition}[$x$ ``pasa'' a $y$]
Sea $X=\xcm$, decimos que $x$ ``pasa'' a $y$ si
$$\P_x(\exists n\geq 0 \tq X_n=y)>0 \, ,$$
y se denota $x\longrightarrow y$.
\end{definition}
\begin{remark}
$x\longleftrightarrow y$ es relación de equivalencia (donde $x\longleftrightarrow y$ si $x\longrightarrow y$ y $y\longrightarrow x$).
\end{remark}
\begin{definition}[Cadena irreducible]
Una cadena $X=\cm$ se dice irreducible si $E$ es la única clase de equivalencia para $\longleftrightarrow$ \, .
\end{definition}
\begin{property}[de Semigrupo]
Sea $X\sim\cm$, entonces $$ \P_X(X_n=y)=(P^n)_{xy} \, .$$
\end{property}
\begin{definition}[Filtración]
Sea $\edp$ un espacio de probabilidad. Sea $(\mathcal{F}_i)_{i\in\N}$ una familia de sub-$\sigma$-álgebras de $\mathcal{F}$. \\ Si $\forall i,j\in\N, i<j$ tenemos $$\mathcal{F}_i\subseteq\mathcal{F}_j \, ,$$
entonces $(\mathcal{F}_i)_{i\in\N}$ se dice una filtración.
\end{definition}
\begin{remark}
Sea $(X_n)_{n\in\N}$ C.M. familia $(\mathcal{F}_i)_{i\in\N}$, dada por $\mathcal{F}_n=\sigma(X_0,\dots,X_n)\, \forall n\in\N$, es una filtración.
\end{remark}
\begin{theorem}[Propiedad de Markov]
Sea $X\sim\cm$ entonces $\forall F:E^\N \to \R$ medible y acotada se tiene
% $$ \E(F(X_{n+1},X_{n+2},\dots)|X_0,\dots,X_n)=\E_{X_n}(F(X_1,X_2,\dots)) \, .$$
$$ \E(F(X_{n+1},X_{n+2},\dots)|\mathcal{F}_0,\dots,\mathcal{F}_n)=\E_{X_n}(F(X_1,X_2,\dots)) \espacio c.s.\, ,$$
donde $\mathcal{F}_n=\sigma(X_0,\dots,X_n)$\,.
\end{theorem}
% \begin{remark}
% La familia $\mathcal{F}_n=\sigma(X_0,\dots,X_n)$ es una filtración (i.e., $\mathcal{F}_n\subset\mathcal{F}_{n+1}\subset\dots\subset\mathcal{F}$)
% \end{remark}
\begin{definition}[Tiempo de parada]
Sea $(\mathcal{F}_i)_{i\in\N}$ una filtración y sea $\tau:\Omega\mapsto\N\cup\{\infty\}$ v.a.. $\tau$ se dice tiempo de parada si
$$\forall m\in\N,\espacio \{\tau\leq m\}\in\mathcal{F}_m \, .$$
\newline A la $\sigma$-álgebra $\mathcal{F}_\tau:=\{A\in\mathcal{F}:A\cap\{\tau\leq m\}\forall m \in \N\}$ se le dice su tribu asociada.
\end{definition}
\begin{theorem}[Propiedad de Markov Fuerte]
Sea $X\sim\cm$, entonces
$$ \E(F(X_{\tau+1},X_{\tau+2},\dots)|\mathcal{F}_\tau)=\E_{X_\tau}(F(X_1,\dots)) \mbox{ c.s. en }\{\tau<\infty\} \, .$$
\end{theorem}
\begin{remark}
El tiempo de retorno a $x\in E$ está dado por $\tau_X=\inf\{n\geq1:X_n=x\}$, y es un tiempo de parada.
\end{remark}
\begin{definition}[Recurrencia y Transiencia]
$x\in E$ se dice recurrente si:
$$ \P_x(\tau_x<\infty)=1 \, .$$
En caso contrario se dice transiente.
\end{definition}
\begin{remark}
Recurrencia es una propiedad de clase.
\end{remark}
\begin{proposition}
\beforeitemize
\begin{itemize}
    \item Una $CM(\lambda,P)$ irreducible se dice recurrente/transiente si $x\in E$ lo es.
    \item Toda $\cm$ irreducible en $E$ finito es recurrente.
\end{itemize}
\end{proposition}
\\ \vspace{3cm}
\begin{theorem}
\label{theorem:visitas}
Sea $X\sim\cm$ y denotemos
$$ N_x:=\displaystyle\sum_{n\in\N}\mathbf{1}_{\{X_n=x\}} \, ,$$
que corresponde al \textbf{número de visitas} de la cadena al estado $x\in E$.
\newline Entonces son equivalentes:
\begin{itemize}
    \item $x\in E$ recurrente.
    \item $N_x=\infty \mbox{ }\P_X-{ c.s.}$.
    \item $\displaystyle\sum_{n\in \N}(P^n)_{xx}=\E_x(N_x)=\infty$.
\end{itemize}
\end{theorem}
\begin{definition}[Medida y Probabilidad Invariante]
Sea $X\sim\cm$, $\gamma=(\gamma_x)_{x\in E}$ con $\gamma_x\geq0$ se dice \textbf{medida invariante} (con respecto a $P$) si
$$ \gamma P = \gamma \, ,$$
esto es:
$$ \sum_{x\in E}\gamma_xP_{xy}=\gamma_{y}\mbox{ }\forall y\in E \mbox{ y }\gamma\neq 0 \, .$$
Una \textbf{probabilidad invariante} $\pi$ es una medida invariante finita tal que $\sum_{x\in E}\pi_x = 1$ \,.
\end{definition}
\begin{proposition}
Sea $\pi$ probabilidad invariante, si $X\sim\cm$:
$$ (X_{n+m})_{n\in \N}\sim CM(\pi,P) \mbox{ }\forall n\in \N \, .$$
En particular $Ley(X_n)=\pi \mbox{ }\forall n \in\N$.
\end{proposition}
\begin{theorem}
Sea $X$ $CM$ irreducible y recurrente, entonces existe una única $\gamma$ medida invariante  (salvo constante multiplicativa) tal que $\gamma_y>0\espacio \forall y \in E$.
\newline Más aún para cada $x$, 
$$\gamma_y^{(x)}:=\E_x\left(\displaystyle\sum^{\tau_x}_{n=1}\mathbf{1}_{\{X_n=y\}}\right),\espacio y\in E \, ,$$
es la única medida invariante tal que $\gamma_x^{(x)}=1$.
\end{theorem}
\begin{remark}
Notemos que $\gamma_y^{(x)}$ es el número esperado de visitas a $y$ entre dos visitas consecutivas a $x$\,.
\end{remark}
\begin{definition}[Recurrencia Positiva o Nula]
Denotamos $m_x=\E_x(\tau_x)$ como el tiempo de retorno esperado a $x$\,.
\newline Un estado $x$ se dice recurrente positivo si $m_x<\infty$ y recurrente nulo si $m_x=\infty$.
\end{definition}
\begin{remark}
Recurrencia positiva y nula son propiedades de clase y 
$$\E_x(\tau_x)<\infty\ssi\E_x(\tau_y)<\infty \mbox{ }\forall y\longleftrightarrow x \, .$$
\end{remark}
\begin{theorem}
Sea $X \mbox{ una } CM$ irreducible, las siguientes son equivalentes:
\begin{itemize}
    \item $x\in E$ es recurrente positivo.
    \item Todo $x\in E$ es recurrente positivo.
    \item $\exists ! \pi$ medida invariante para $P$ estrictamente positiva, y está dada por:
    $$ \pi = (\pi_x=m_x^{-1})_{x\in E}>0 \, .$$
\end{itemize}
\end{theorem}
\begin{remark}
$X\sim\cm$ irreducible en $E$ finito $\implies$ $X$ recurrente positiva.
% \newline Por ejemplo si simulamos una fila, tenemos una cantidad no acotada de clientes posibles, por ende nuestro espacio de estados está indexado por los naturales. Sin embargo podemos considerar ciertas cotas. Por ende al simular acotaremos el espacio de estados de modo que $E$ será finito. La irreducibilidad entonces nos garantizará la existencia de una distribución invariante.
\newline En la práctica, para efectos de simulaciones siempre nos restringiremos a espacios finitos.  En los casos en que la cadena es recurrente positiva, esta suposición es suficientemente representativa.
\end{remark}
\begin{notation}
$\P_\mu(X_n=y)=\mu P^n$
\end{notation}
\begin{definition}[Aperiodicidad]
$X$ se dice aperiódica si $\forall x\in E,\exists n_0\in\N\tq$
$$(P^n)_{xx}>0\espacio \forall n\geq n_0 \, .$$
\end{definition}
\begin{theorem}
Sea $X=(X_n)_{n\in \N}$ $CM$ irreducible, entonces:
\begin{itemize}
    \item[(a)] Si $X$ es transiente o recurrente nula, $\forall\mu,\forall y\in E$,     $$ \P_\mu(X_n=y)\conv 0 \, .$$
    \item[(b)] Si $X$ es recurrente positivo, $\forall\mu,\forall y\in E$,
    $$ \displaystyle\frac{1}{n}\sum^n_{k=0}\P_\mu(X_k=y)\conv \pi_y>0 \, ,$$
    con $\pi$ distribución invariante.
    \item[(c)] Si $X$ es recurrente positiva y aperiódica, $\forall\mu,\forall y\in E$,
    $$ \P_\mu(X_n=y)\conv \pi_y \, .$$
\end{itemize}
\end{theorem}

% \\ \vspace{3cm}
\subsection{Simulación de cadenas de Markov}
El siguiente resultado es útil para entender la construcción de cadenas de Markov. En particular será importante para su simulación.
\begin{proposition}
\label{propsimmarkov}
Sean $X_0$ v.a. en $E$, $(Z_n)_{n\in\N}\mbox{ v.a. }\iid$ a valores en un espacio medible $(F,\Sigma)$ independientes de $X_0$. Sea $\Phi:E\times F\to E$ medible, entonces si definimos recursivamente:
$$X_{n+1}:=\Phi(X_n,Z_{n+1}) \espacio \forall n\in\N \, ,$$
luego $\xcm$ es una cadena de Markov $CM(\nu,Q)$ con $\nu=Ley(X_0)$, y $Q_{xy}=\P(\Phi(x,Z_1)=y)$.
\end{proposition}
\begin{proof} \newline
\gris
Por construcción tenemos que $\nu=Ley(X_0)$. \newline Veamos que $(X_n)_{n\in\N}$ es cadena de Markov. Notemos que reemplazando, la probabilidad del cilindro $\P(X_{n+1}=x_{n+1},X_n=x_n,\dots,X_0=x_0)$ puede escribirse como
$$ \P(\Phi(x_n,Z_{n+1})=x_{n+1},\Phi(x_{n-1},Z_n)=x_n,\dots,\Phi(x_0,Z_1)=x_1,X_0=x_0) \, .$$
Pero usando la independencia de los $Z_n$ esto igual a 
$$ \P(\Phi(x_n,Z_{n+1})=x_{n+1})\P(X_n=x_n,\dots,X_0=x_0) \, ,$$
que a su vez por definición corresponde a
$$ Q_{x_nx_{n+1}}\P(X_n=x_n,\dots,X_0=x_0)\, . $$
Por otro lado, sumando sobre todos los estados posibles $x_0,\dots,x_n$ queda que 
$$ \P(X_{n+1}=x_{n+1},X_n=x_n) = \P(\Phi(X_n,Z_{n+1})=x_{n+1})\P(X_n=x_n)=Q_{x_n,x_{n+1}}\P(X_n=x_n) \, .$$
Despejando $Q_{x_nx_{n+1}}$ en ambas ecuaciones se obtiene la propiedad buscada. \findem
\negro
\end{proof}
\vspace{.5cm}\\
A partir de lo anterior deducimos como simular cadenas de markov fácilmente en un computador. En efecto tenemos el siguiente corolario:
\begin{corolary}[Simulación de cadenas de Markov]
Sean $(U_n)_{n\in\N} \iid \sim\unif$, $\lambda=(\lambda_x)_{x\in E}$ vector de probabilidad y $P=(P_{xy})_{x,y\in E}$ matriz estocástica dados, con $E=\{y_0,y_1,\dots,y_n,\dots\}$. % \newline 
Sean 
% $\Phi_0:[0,1]\to E$, $y_n=\Phi_0(u)$ % $u\longmapsto y_n$ 
% si $u\in[\displaystyle\sum^{n-1}_{k=0}\lambda_{y_k},\sum^n_{k=0}\lambda_{y_k}]$  % (que tiene largo $\lambda_{y_n}$)
$$ \Phi_0:[0,1]\to E \, ,\, y_n=\Phi_0(u)\espacio\mbox{si}\espacio u\in[\displaystyle\sum^{n-1}_{k=0}\lambda_{y_k},\sum^n_{k=0}\lambda_{y_k}]$$
y
% \newline $\Phi:E\times[0,1]\to E$, $y_n=\Phi(x,u)$  % $(x,u)\longmapsto y_n$ 
% si $u\in[\displaystyle\sum^{n-1}_{k=0}P_{xy_k},\sum^n_{k=0}P_{xy_k}]$ % (que tiene largo $\P_{xy_n}$).
$$ \Phi:E\times[0,1]\to E\,,\,y_n=\Phi(x,u)\espacio\mbox{si}\espaciou\in[\displaystyle\sum^{n-1}_{k=0}P_{xy_k},\sum^n_{k=0}P_{xy_k}] \, .$$
Entonces el proceso $X_0$ definido por
$$ X_0:=\Phi_0(U_0), \mbox{ }X_{n+1}:=\Phi(X_n,U_{n+1}), n\in\N\, ,$$
es una cadena de Markov de parámetros $\lambda$, $P$.
\end{corolary}
\begin{proof}
\gris
En la Proposición \ref{propsimmarkov} tomar $Z_n=U_n$, $X_0=\Phi_0(U_0)$, y notar que $\P(X_0)=\lambda_y$ y $\P(\Phi(x,Z_1)=y)=P_{xy}.$ \findem
\negro
\end{proof}
\vspace{.5cm} \\
\begin{remark}
Siempre podemos asumir que $E=\{y_0,\dots,y_n,\dots\} = \{0,1,\dots,n,\dots\}=\N$.
\newline Luego, para simular $X_0\sim\lambda$ y cada transición $Ley(X_{n+1}|X_n=x)=P_{x_0}$, podemos usar simulaciones de variables discretas en $\N$ como las vistas en sección \ref{disc}.
\end{remark}
\vspace{1.5cm}\\
\subsection{Ley de grandes números para cadenas de Markov}
\begin{theorem}[Ley de grandes números para CM o Teorema ergódico]
\label{lgn_cm}
Sea $X=\xcm$ cadena de markov recurrente positiva e irreducible y $\pi$ su distribución invariante. Entonces $\forall\mu$ distribución inicial, $\forall f:E\to\R$ acotada,
$$ \displaystyle\frac{1}{n}\sum^n_{k=0}f(X_k)\conv\langle\pi,f\rangle=\sum_{y\in E}\pi_y f(y), \mbox{ }\P_\mu-c.s. \, .$$
En particular, $\forall y\in E$:
$$ \displaystyle\frac{1}{n}\sum^n_{k=0}\mathbf{1}_{\{X_k=y\}}\conv\pi_y=\P_\pi(X_0=y) \, \mbox{ }\P_\mu-c.s. \,  .$$
Como consecuencia \textbf{podemos aproximar integrales con respecto a $\pi$ usando una sola trayectoria de $X$.} 
\end{theorem}
\begin{proof}
\gris Seguiremos la demostración de ``Processus de Markov et applications'' % capítulo 3 
de E. Pardoux \cite{pardoux}.

Estudiaremos primero el límite $\displaystyle\frac{N_x(n)}{n}$ cuando $n\to\infty$, donde
$$ N_x(n):=\displaystyle\sum_{1\leq k\leq n}\mathbf{1}_{\{X_k=x\}}$$
es el número esperado de visitas al estado $x$ antes de $n$.

Denotamos $S^0_x,S^1_x,\dots,S^k_x,\dots$ los largos de las sucesivas excursiones $\mathcal{E}_0,\mathcal{E}_1,\dots,\mathcal{E}_k,\dots$ de la cadena recurrente $X$  partiendo y terminando en $x$. Tenemos que
$$ S^0_x+S^1_x+\dots+S^{N_x(n)-1}_x \leq n < S^0_x+S^1_x+\dots+S^{N_x(n)-1}_x+S^{N_x(n)}_x \, ,$$
con lo cual tenemos que
$$ \displaystyle \frac{S^0_x+S^1_x+\dots+S^{N_x(n)-1}_x}{N_x(n)} \leq \frac{n}{N_x(n)} < \frac{S^0_x+S^1_x+\dots+S^{N_x(n)-1}_x+S^{N_x(n)}_x}{N_x(n)} \,. $$
Usando la propiedad de Markov fuerte se demuestra que las excursiones $(\mathcal{E}_k)_{k\in\N}$ son $\iid$ y por ende también lo son las v.a.  $(S_x^k)_{k\in\N}$. Adem\'as, cada $S_x^k$ tiene la misa ley que $T_x$ bajo $\P_x$, donde $T_x = \inf\{n > 0: X_n = x\}$. Deducimos que 
$$ \frac{S^0_x+S^1_x+\dots+S^{N_x(n)-1}_x+S^{N_x(n)}_x}{N_x(n)} \conv \E_x(T_x)=m_x \espacio \P_x-c.s. \, .$$
Por Teorema \ref{theorem:visitas}, $N_x(n)\to + \infty \espacio \P_x-c.s. \,$ cuando $n\to \infty$, y entonces obtenemos
$$ \displaystyle \frac{N_x(n)}{n}\conv \frac{1}{m_x} \espacio \P_x-c.s. \,.$$
Sea ahora $F\subset E$ y denotemos $\bar f = \displaystyle \sum_{x\in E}\pi_x f(x)$. Luego, con $c>0$ una cota para $|f|$, se tiene
\begin{alignat*}{2}
\bigg| \frac{1}{n}\sum^n_{k=1}f(X_k)-\bar f\bigg| & = \bigg|\sum_{x\in E}\bigg( \frac{N_x(n)}{n}-\pi_x \bigg)f(x)\bigg|\\
& \leq c\sum_{x\in F}\bigg|\frac{N_x(n)}{n}-\pi_x\bigg|+c\sum_{x\notin F}\bigg(\frac{N_x(n)}{n}+\pi_x\bigg) \\
& = c\sum_{x\in F}\bigg|\frac{N_x(n)}{n}-\pi_x\bigg|+c\sum_{x\in F}\bigg(\pi_x-\frac{N_x(n)}{n}\bigg)+2c\sum_{x\notin F}\pi_x \\
& \leq 2c\sum_{x\in F}\bigg|\frac{N_x(n)}{n}-\pi_x \bigg|+2c\sum_{x\notin F}\pi_x \, .
\end{alignat*}
En la 3era l\'inea cambiamos $\sum_{x\notin F} \frac{N_x(n)}{n} = 1 - \sum_{x\in F}  \frac{N_x(n)}{n} = \sum_{x\in E}  \pi_x - \sum_{x\in F}  \frac{N_x(n)}{n}  $ . Elegimos ahora $F$ tal que $\displaystyle \sum_{x\notin F}\pi_x\leq\frac{\epsilon}{4c}$ y $N(\omega)$ tal que $\forall n\geq N(\omega)$ 
$$ \sum_{x\in F}\bigg|\frac{N_x(n)}{n}-\pi_x\bigg|\leq\frac{\epsilon}{4c}. $$
Entonces, $\forall n\geq N(\omega)$ tenemos $$ \bigg| \frac{1}{n}\sum^n_{k=1}f(X_k)-\bar f\bigg|\leq\epsilon \, $$
concluyendo  la convergencia. \findem
\negro
% Théorème 2.5.7 Pardoux
\end{proof}
\subsubsection{Estimación de matriz de transición}
\begin{corolary}[Estimación de probabilidades de transición]
Sea $\xcm \cm$ recurrente positiva. $\forall \mu$, $\forall x,y\in E$ tal que $P_{xy}>0$,
$$ \displaystyle\frac{1}{n}\sum^n_{k=0}\mathbf{1}_{\{X_{k+1}=y,X_k=x\}} \mbox{ }\overset{\P_\mu-c.s.}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }\pi_xP_{xy}$$
y además,
$$ \displaystyle \frac{\sum^n_{k=0}\mathbf{1}_{\{X_{k+1}=y,X_k=x\}}}{\sum^n_{k=0}\mathbf{1}_{\{X_{k}=x\}}}\mbox{ }\overset{\P_\mu-c.s.}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }P_{xy}\,.$$
\end{corolary}
\begin{proof}
\ejercicio \gris: \newline $(\hat{X}_n)_{n\in\N}:=(X_n,X_{n+1})_{n\in\N}$ es $CM(\hat{\mu},\hat{P})$ en $\hat{E}:=\{(x,y)\in E\times E:P_{xy}>0\}$ con distribución inicial $\hat{\mu}_{(x,y)}:=\mu_xP_{xy}$ y $\hat{P}_{(z,x|(\omega,y))}:=\begin{cases}
P_{xy}
& \mbox{ si }x=\omega \mbox{ ,}\\
0 & \mbox{ si no.}
\end{cases}$ \newline Además es irreducible y con distribución invariante $\hat{\pi}_{(x,y)}=\pi_xP_{xy}$.
\newline Por la parte anterior, aplicada a $\hat{X}\sim CM(\hat{\mu},\hat{P})$,
$$ \displaystyle\frac{1}{n}\sum^n_{k=0}\mathbf{1}_{\{X_{k+1}=y,X_k=x\}} \mbox{ }\overset{\P_\mu-c.s.}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }\pi_xP_{xy} \, ,$$ y tomar cociente (dividir por la aproximación de $\pi_x$). \negro \findem
\end{proof}

\subsection{Distancia de Variación total y coupling }
Nos interesa cuantificar la convergencia de cadenas de Markov y en particular encontrar condiciones para convergencia ``rápida'' (geométrica) al equilibrio. Para ello  estudiaremos una noci\'on apropiada de distancia entre probabilidades en $E$.
\begin{definition}[Distancia de variación total]
\label{def:var_tot}
Dadas $\mu$, $\nu\in\mathcal{E}$, con $E$ numerable, su distancia de variación total es $$\|\mu-\nu\|_1=\sum_{x\in E}|\mu_x-\nu_x| \, .$$
o sea, la distancia en $l^1(E)=L^1(E,\#)$ con $\#$ la medida de conteo.
\newline En general, en un espacio medible $(E,\Sigma)$ cualquiera, la distancia en variación total está dada por:
$$ \|\mu-\nu\|_1=\displaystyle\int_E|\frac{d\mu}{d\lambda}(x)-\frac{d\nu}{d\lambda}|\lambda(dx) \, ,$$
donde $\lambda$ es cualquier medida $\sigma$-finita tal que $\mu,\nu \ll \lambda$ (ejemplo: $\lambda=\mu+\nu$), y se puede verificar que en el caso de $E$ discreto esta noci\'on corresponde con la antes dada. 
\end{definition}
\begin{definition}[Coupling]
Dadas $\mu$, $\nu\in\mathcal{P}(E)$, un coupling (o acoplamiento) entre $\mu$ y $\nu$ es un par de variables aleatorias $X$ e $Y:\edp\to(E,\beta)^2$ definido en algún espacio $\edp$ común tal que $X\sim\mu$, $Y\sim\nu$.
\end{definition}
\begin{remark}
Como el espacio de probabilidad es común, podemos samplear $X$ e $Y$ simultáneamente.
\end{remark}
\begin{lemma}[Desigualdad de Coupling]
\label{lemma:des_coup}
$\forall\mu,\nu\in\mathcal{P}(E)$,
$$ \|\mu-\nu\|_1 = \displaystyle 2\inf_{(X,Y) \mbox{ coupling de }\mu\mbox{ y }\nu}\P(X\neq Y)\, .$$
Además, este ínfimo se alcanza. En particular $\forall (X,Y)$ coupling de $\mu$ y $\nu$ se tiene:
$$ \|\mu-\nu\|\leq 2\P(X\neq Y) \, .$$
A esta última se le llama desigualdad de coupling. 
\end{lemma}
\begin{remark}
\beforeitemize
\begin{itemize}
    \item Elegir $X$ e $Y$ de manera inteligente nos permite tener una buena cota para la distancia de variación total.
    \item Siempre existen coupling. Por ejemplo tomar $X\indep Y$, sin embargo esta no es la mejor cota que podemos encontrar.
\end{itemize}
\end{remark}

\begin{proof}[Demostración del Lema \ref{lemma:des_coup}]  % clase 10 pag 12 %16:16
\gris Sea $(X,Y)$ coupling de $\mu,\nu$, entonces
\begin{alignat*}{2}
    \P(X=Y) & = \displaystyle\sum_{x\in E}\P(X=Y=x)  \\
     & \leq \displaystyle\sum_{x\in E}\min\{\mu_x,\nu_x\} \, .
\end{alignat*}
\begin{alignat*}{2}
    \therefore \P(X\neq Y) & = \geq 1-\displaystyle \sum_{x\in E}\min\{\mu_x,\nu_x\}  \\
     &  = \displaystyle \sum_{x\in E}(\mu_x-\min\{\mu_x,\nu_x\}) \, \\
     & = \displaystyle\sum_{x\in E}(\mu_x-\nu_x)_+.
\end{alignat*}
% $\therefore \P(X\neq Y)\geq 1-\displaystyle \sum_{x\in E}\min\{\mu_x,\nu_x\} = $
Luego, como $|x| = |x|_++|-x|_+$\,,
$$ \|\mu_x-\nu_x\|_1 = \displaystyle\sum_{x\in E}|\mu_x-\nu_x|\leq 2\P(X\neq Y) \, .$$
Ahora construiremos explícitamente un coupling ``óptimo'' donde el ínfimo se alcanza. \\ Sea $\alpha = \displaystyle\sum_{x\in E}\min\{\mu_x,\nu_x\}\leq 1$ y $\xi\in\mathcal{P}(E)$ dada por
$$ \xi_x = \alpha^{-1}\min\{\mu_x,\nu_x\}\, .$$
Sean $\xi$, $U$, $V$, $W$ v.a. independientes con leyes $\xi\sim Ber(\alpha)$, $U\sim\xi$, $V\sim\bigg(\bar{\mu}_x\displaystyle:=\frac{(\mu_x-\nu_x)_+}{1-\alpha}\bigg)_{x\in E}$, $W\sim\bigg(\bar{\nu}_x\displaystyle:=\frac{(\nu_x-\mu_x)_+}{1-\alpha}\bigg)_{x\in E}$. Entonces definimos
$$ X:=\begin{cases}
U & \mbox{ si }\xi=1\\
V & \mbox{ si }\xi=0
\end{cases}, \espacio Y:=\begin{cases}
U & \mbox{ si }\xi=1\\
W & \mbox{ si }\xi=0
\end{cases}$$
Veamos que $(X,Y)$ es un coupling de $\mu$ y $\nu$. En efecto
\begin{alignat*}{2}
    \P(X=x) & = \P(X=x|\xi=0)\P(\xi=1)+\P(X=x|\xi=0)\P(\xi=0) \\
     & = \alpha\P(U=x)+(1-\alpha)\P(V=x) \\
     & = \displaystyle \alpha\bigg(\frac{\min\{\mu_x,\nu_x\}}{\alpha}\bigg)+(1-\alpha)\frac{(\mu_x-\nu_x)_+}{1-\alpha} \\
     & = \min\{\mu_x,\nu_x\}+(\mu_x-\nu_x)_+ = \mu_x \, .
\end{alignat*}
Por ende tenemos que $X\sim\mu$. De manera completamente análoga podemos concluir que $Y\sim\nu$.
Veamos ahora que el coupling es óptimo. Notemos que $\P(X\neq Y)=1-\alpha$. En efecto,
\begin{alignat*}{2}
    \P(X\neq Y) & = \P(X\neq Y|\xi=0)\P(\xi=0)+\P(X\neq Y|\xi=1)\P(\xi=1) \\
     & = (1-\P(V=W))(1-\alpha) \\
     & = \bigg(1-\displaystyle\sum_{x\in E}\frac{(\mu_x-\nu_x)_+(\nu_x-\mu_x)_+}{(1-\alpha)^2}\bigg)(1-\alpha) \\
     & = 1-\alpha \, ,
\end{alignat*}
pues $(\mu_x-\nu_x)_+(\nu_x-\mu_x)_+ = 0$. Usando esto tenemos que
\begin{alignat*}{2}
    \|\mu-\nu\|_1 & = \displaystyle \sum_{x\in E}|\mu_x-\nu_x| \\
     & = \displaystyle \sum_{x\in E}[\mu_x+\nu_x-2\min\{\mu_x,\nu_x\}] \\
     & = \displaystyle 2-2\sum_{x\in E}\min\{\mu_x,\nu_x\} \\
     & = 2-2\alpha = 2\P(X\neq Y) \, .
\end{alignat*}
$\therefore$ este coupling alcanza el ínfimo. \findem
\negro 
\end{proof}

\vspace{.5cm} \\
\begin{remark}
La distancia en variación total se puede definir en $(E,\Sigma)$ espacio medible cualquiera como
$$ \|\mu-\nu\|_1 = \displaystyle \int_E|\frac{d\mu}{d\lambda}(x)-\frac{d\nu}{d\lambda}(x)|\lambda(dx)\, ,$$
donde $\lambda$ es cualquier medida $\sigma$-finita tal que $\mu,\nu<<\lambda$ (por ejemplo $\lambda=\mu+\nu$).
\\ Esta definición coincide con la del curso de Teoría de la Medida:
$$ \|\mu-\nu\|=|\mu-\nu|(E) \, $$
donde $|\espacio|$ es la medida de variación total; y también con la definición \ref{def:var_tot} que hicimos para el caso $E$ numerable, excepto que esta vale para $E$ espacio medible cualquiera.
\end{remark}
\begin{proposition}%pag 4 c
Sea $E$ numerable, se tiene:
$$ \mu^n\convdebil\mu \espacio \Longleftrightarrow \espacio \|\mu^n-\mu\|_1\conv0\,.$$
\end{proposition}
\begin{proof}
\ejercicio \espacio \gris Indicación: Notar que $E$ es un espacio métrico con $d(x,y)=\mathbf{1}_{x\neq y}$, y toda función es Lipschitz.  La distancia en variación total es un caso particular de una familia de m\'etricas generales definidas sobre medidas de probabilidad, conocidas como distancia de transporte: 
\negro
\end{proof}
\begin{proposition}[Distancia Wasserstein]  %pag 4 d
Sea $(E,d)$ espacio métrico cualquiera, $d$ induce una distancia $W_d$ en $\mathcal{P}(E)$ mediante:
$$ W_d(\mu,\nu) = \displaystyle \inf_{(X,Y)\text{ coupling de }\mu,\nu}\E(d(X,Y)) \, .$$
Se le llama \textbf{distancia de Wasserstein o de transporte}.
\end{proposition}

\subsection{Convergencia Geométrica}
\begin{definition}[Condición de Doeblin]
Decimos que se cumple la condición de Doeblin, denotada por $(D)$, si $\exists n_0\geq1$, $\exists\beta>0$, $\exists m\in\mathcal{P}(E)$ tal que
$$(P^{n_0})_{xy}\geq\beta m_y \mbox{ }\forall x,y\in E \, .$$
\end{definition}
\vspace{.5cm} \\
\begin{remark}
\beforeitemize
\begin{itemize}
    \item  Si vemos $(P^{n_0})_{xy},y\in E$ como medida de probabilidad, $(D)$ dice que esta está minorada por una medida de probabilidad $m_y$ que no depende de $x$.
    \item Cuando se simula una cadena de Markov, si queremos simular una transición entre $x$ e $y$, la condición nos dice que a veces podremos hacerlo sampleando una variable aleatoria con ley $m_y$, olvidándonos de que $x$ partimos. Esta capacidad de ``olvidar el pasado'' nos va a dar la convergencia al equilibrio.
\end{itemize}
\end{remark}

\begin{property}
Consideremos la condición de Doeblin $(D)$, entonces:
\begin{alignat*}{2}
    (D) & \Longleftrightarrow (\exists y\in E)(\exists n_0 \geq 1) \mbox{ tal que }\displaystyle \inf_{x\in E}(P^{n_0})_{xy}>0 \\
     & \Longleftrightarrow (\exists n_0 \geq 1) \mbox{ tal que } \displaystyle \sum_{y\in E}\inf_{x\in E}(P^{n_0})_{xy}>0 \, .
\end{alignat*}
\end{property}
\begin{proof}
\ejercicio
\end{proof}
\begin{remark}
\beforeitemize
\begin{itemize}
    \item $\beta\in[0,1]$ \, (sumando con respecto a $y$ en $(D)$) y $\beta<1$ a menos que $(P^{n_0})_{xy}=m_y$ $\forall x,y\in E$ .
    \item $(D)$ es raramente satisfecha cuando $|E|=+\infty$ \, .
    Típicamente sucederá que $\forall n\in \N$, $\forall y\in E$ $\inf_{x\in E}(P^n)_{xy}=0$ \, .
    \item $P$ irreducible y aperiódica y $|E|<\infty$ implica que $(D)$ se cumple.
    \newline     \ejercicio
\end{itemize}
\end{remark}
\begin{theorem}
Supongamos $P$ irreducible y que cumple $(D)$. Entonces $P$ es aperiódica y recurrente positiva, y si $\pi$ denota su distribución invariante, se tiene
$$ \|\mu^{P^n}-\pi\|_1\leq 2(1-\beta)^{\lfloor\frac{n}{n_0}\rfloor}, \mbox{ } \forall \mu\in \mathcal{P}(E),\forall n\in\N\,,$$
donde $\beta\in(0,1)$ y $n_0\geq 1$ vienen de $(D)$. Es decir, la convergencia  al equilibrio tiene lugar a tasa geométrica.
\end{theorem}
\begin{proof} % Clase 11 10:50 pag 6-10
\gris
\beforeitemize
\begin{enumerate}
    \item Probemos que $\forall\mu,\nu\in\mathcal{P}(E),\,\forall n\in\N$
    $$ \|\mu P^n-\pi\|_1\leq 2(1-\beta)^{\lfloor\frac{n}{n_0}\rfloor} \, .$$
    Basta construir para cada $n$ un coupling $(X_n,Y_n)$ de $\mu P^n$ y $\nu P^n$ tal que
    $$ \P(X_n\neq Y_n)\leq(1-\beta)^{\lfloor\frac{n}{n_0}\rfloor} \, $$
    y tomar despu\'es $\nu=\pi$, de modo que $\nu P^n=\pi$ para todo $n$.  
    Escribimos: $n=k n_0+j$, con $j<n_0$.
    \begin{itemize}
        \item Notar que $Q_{xy}:=\displaystyle\frac{(P^{n_0})_{xy}-\beta_{m_y}}{1-\beta}$ es matriz estocástica por $(D)$.
        \\ Sea $f:E\times[0,1]\mapsto E$ función de transición asociada:
        $$ \P(f(x,U)=y)=Q_{xy}\text{ si }U\sim\unif\,.$$
        \item Sean $X_0\sim\mu$, $Y_0\sim \nu$, $\xi_l\sim Ber(\beta)$, $U_l\sim\unif$, $W_l\sim m, \, l=1,\dots,k$ independientes. Definimos recursivamente para $l=0,\dots,k-1$
        $$X_{(l+1)n_0} = \begin{cases}
                W_{l+1} & \mbox{ si }\xi_{l+1}=1\\
                f(X_{l_{n_0}},U_{l+1}) & \mbox{ si no}
            \end{cases}$$
        $$Y_{(l+1)n_0} = \begin{cases}
                W_{l+1} & \mbox{ si }\xi_{l+1}=1\\
                f(Y_{l_{n_0}},U_{l+1}) & \mbox{ si no} \, .
            \end{cases}$$
        Se puede probar que
        $$ (P^{n_0})_{xy} = \begin{cases}
                \P(X_{(l+1)_{n_0}}=y|X_{l_{n_0}}=x) \\
                \P(Y_{(l+1)_{n_0}}=y|Y_{l_{n_0}}=x)
            \end{cases} $$
        y $$ (X_{l_{n_0}})^k_{l=0}\sim C.M.(\mu,P^{n_0})$$
        $$ (Y_{l_{n_0}})^k_{l=0}\sim C.M.(\nu,P^{n_0})\,$$
        pues ambas son construcciones recursivas con ``innovaciones'' independientes.
        \item Sea ahora $\hat{f}:E\times[0,1]\mapsto E$ función de transición asociada a $P^j$ y $\hat{U}\sim\unif$, independiente a todo lo anterior. Entonces $(X_n,Y_n)$ con
        $$ X_n := \hat{f}(X_{k_{n_0}},\hat{U})$$
        $$ Y_n := \hat{f}(Y_{k_{n_0}},\hat{U})$$
        es un coupling de $\mu P^n$ y $\nu P^n$, 
        y se tiene: $\{X_n\neq Y_n\}\subseteq \displaystyle \bigcap^k_{l=0}\{X_{l_{n_0}}\neq Y_{l_{n_0}}\}\subseteq \bigcap^k_{l=0}\{\xi_l=0\}$,
        $$ \therefore \P(X_n\neq Y_n)\leq (1-\beta)^k=(1-\beta)^{\lfloor\frac{n}{n_0}\rfloor}$$
        $$ \text{ y } \|\mu P^n-\nu P^n\|_1\leq 2(1-\beta)^{\lfloor\frac{n}{n_0}\rfloor} \, ,$$
        gracias al Lema \ref{lemma:des_coup},
        que es lo que queríamos.
    \end{itemize}
    \item Veamos que $\mu P^n$ converge a algo en % $\mathcal{P}(E)$
    $l^1(E)$ (variación total)
    \\ Tomando $\nu = \mu P^m$, $m\in\N$ fijo, obtenemos
    % $$ \longrightarrow \|\mu P^n-\mu P^{n+m}\|_1\leq 2(1-\beta)^{\frac{n}{n_0}+1}\, \forall m\in\N$$
    \begin{alignat*}{2}
        &  \|\mu P^n-\mu P^{n+m}\|_1\leq 2(1-\beta)^{\frac{n}{n_0}+1}\, \forall m\in\N \\
        & \Longrightarrow \{\mu P^n\}_{n\in\N}\text{ suc. de Cauchy en }l^1(E) \\
        & \Longrightarrow \exists \pi\in\mathcal{P}(E) \text{ tal que }\mu P^n\conv\pi\, .
    \end{alignat*}
    \item Notar que $\|\mu P-\nu P\|_1\leq \|\mu-\nu\|_1$, o sea $\nu\mapsto\nu P$ es Lipschitz con respecto a $\|\,\|_1$, por lo que tomando $\nu=\mu P$ tenemos
    $$ \|\mu P^n-\mu P^{n+1}\|_1=\|\mu P^n-(\mu P^n)P\|_1\conv \|\pi-\pi P\|_1 \, .$$
    Por otro lado, como $\mu P^n$ es sucesión de Cauchy, $\|\mu P^n-(\mu P^n)P\|_1\conv=0$, con lo cual $\|\pi-\pi P\|_1=0$. Sigue que $\pi$ es medida de probabilidad invariante y $P$ es recurrente positiva.
    \\ Además, $(P^n)_{xx}\conv\pi_x>0$, luego $P$ es aperiódica. Finalmente tomando $\nu=\pi$
    $$ \Longrightarrow \|\mu P^n-\pi\|_1\leq 2(1-\beta)^{\lfloor\frac{n}{n_0}\rfloor}\, \forall n\in\N\, .$$
    $\findem$
\end{enumerate}
\negro
\end{proof}
\subsection{Teorema central del límite para CM}
\begin{definition}[Uniforme ergodicidad]
Una % cadena de Markov 
$\cm$ (o su matriz de transición) se dice uniformemente ergódica si es irreducible, recurrente positiva y $\exists m>0$, $\rho\in(0,1)$ tal que $\displaystyle\sum_{y\in\E}|(P^n)_{xy}-\pi_y|\leq M\rho^n$, $\forall n\in\N$, $\forall x\in E$\,.
\end{definition}
\begin{theorem}[T.C.L para cadenas de Markov]
\label{tcl_markov}
Sea $(X_n)_{n\in \N}$ irreducible, aperiódica y uniformemente ergódica. Sea $\pi$ su ley invariante y $f\in L^2(E,\pi)$, es decir,  $\displaystyle\sum_{x\in E}f^2(x)\pi_x<\infty$. Entonces
$$ \displaystyle \sqrt{n}\bigg(\frac{\sum^n_{k=0}f(X_k)-\langle\pi,f\rangle)}{n}\bigg)\convley\normal(0,\sigma^2_f)\,,$$
con $\sigma^2_f=\langle\pi,(Qf)^2\rangle-\langle\pi,(PQf)^2\rangle$ donde $(Qf)_x=\displaystyle\sum^\infty_{n=0}\E_x(f(X_n))$, $x\in E$.
\newline Notar que $\displaystyle\sum^\infty_{n=0}$ converge gracias a la uniforme ergodicidad.
\end{theorem}
\begin{proof}
\gris En Pardoux \cite{pardoux}.
\negro
\end{proof}
\begin{corolary}
Sea $\xcm$ una cadena de Markov irreducible que satisface $(D)$, en particular si $E$ es finito, satisface el TCL (\ref{tcl_markov}).
\end{corolary}
\subsection{Simulación exacta de una ley invariante}
Consideremos $(X_n)_{n\in \N}\sim \cm$ irreducible, recurrente positiva y aperiódica. Sabemos que:
$$ X_n\convley X_\infty\sim\pi$$
Sin embargo esto sigue siendo una aproximación. Sin embargo nos gustaría simular una $X_\infty$ que distribuya $\pi$ de manera \textbf{exacta}, no aproximada. Se darán dos condiciones y algoritmos para aquello.
\subsubsection{Algoritmo simulación perfecta}
Supongamos $(D)$ con $n_0=1$ y sea $\beta=\displaystyle\sum_{y\in E}\inf_{x\in E}P_{xy}\in(0,1)$ y consideremos la siguiente elección de $m$:
\begin{itemize}
    \item $m\in\mathcal{P}(E)$, con $m_y:=\displaystyle\frac{\inf_{x\in E}P_{xy}}{\beta}$, $y\in E$\,.
    \item $\bigg(Q_{xy}=\displaystyle\frac{P_{xy}-\beta m_y}{1-\beta}\bigg)_{x,y\in E}$ \, matriz estocástica.
\end{itemize}
Notemos que si simulamos $\xi\sim Ber(\beta)$, $Z\sim m$, $Y\sim Q_{X_0}$ independientes, entonces $X$ dada por 
$$ X:=\begin{cases}
Z
& \mbox{ si }\xi=1\\
Y & \mbox{ si }\xi=0
\end{cases}\espacio $$
 tiene ley $P_{x_0}$. En efecto:
\begin{alignat*}{2}
    \P(X=z) & = \P(X=z|\xi=1)\P(\xi=1)+\P(X=z|\xi=0)\P(\xi=0) \\
     & = \P(Z=z)\beta + \P(Y=z)(1-\beta)\\
     & = \beta m_Z+(1-\beta)Q_{xz} = P_{xz}\,,
\end{alignat*}
i.e., obtenemos $P_{xz}$ por construcción.
\newp Entonces construimos una función de transición $f:E\times [0,1]\to E$ tal que si $U\sim\unif$,
$$ \P(f(x,U)=y|U\leq \beta)=m_y$$
$$ \P(f(x,U)=y|U>\beta)=Q_{xy}\,.$$
Basta tomar:
\begin{itemize}
    \item $f_1:E\times[0,1]\to E$ función de transición asociada a $Q:\P(f_1(x,U)=y)=Q_{xy}$\,.
    \item $f_0:[0,1]\to E$ función para simular $m:\P(f_0(U)=y)=m_y$\,.
\end{itemize}
Y definimos: 
$$f(x,u)=f_0\bigg(\frac{u}{\beta}\bigg)\mathbf{1}_{u\leq\beta}+f_1\bigg(x,\frac{u-\beta}{1-\beta}\bigg)\mathbf{1}_{u>\beta}\, .$$
Esta función cumple lo requerido (\ejercicio) y además 
$$\forall x\in E,\espacio \P(f(x,U)=y)=P_{xy}\,.$$
\vspace{1.5cm}\\
\begin{theorem}[Algoritmo de simulación perfecta]
\beforeitemize
\begin{enumerate}
    \item[(i)] Sean $U_0,U_{-1},U_{-2},\dots,\iid\sim\unif$ y $\tau=\max\{k\leq 0:U_k<\beta\}$\,.
    \item[(ii)] Sea $X_\tau:=f(\bar{x},U_\tau)$ con $\bar{x}$ fijo cualquiera.
    \item[(iii)] Para $k=\tau+1,\tau+2,\dots,-1,0$ sean $X_k=f(X_{k-1},U_k)$ donde las $U_k$ son las mismas uniformes de antes.  % , de modo que $X_k$ tiene ley $Q$\,.
\end{enumerate}
Entonces $-(\tau-1)\sim Geo(\beta)$, $X_\tau\sim m$, $X_\tau\indep-(\tau-1)$ y  % (donde se sampleo es independiente de cuando se hizo), y
$$ X_0\sim\pi\,,$$
donde $\pi$ es invariante para $P$\,.
\end{theorem}
\begin{proof} % clase 11 pag 16
\gris
Sean $k\in\N$, $x_0,x_1,\dots,x_k\in E$,
\begin{alignat*}{2}
    & \P(\tau = -k,X_{-k}=x_k,X_{-(k-1)}=x_{k-1},\dots,X_{-1}=x_{-1},X_0=x_0) \\ & = \P[f(\bar{x},U_{-k})=x_k,U_{-k}<\beta,f(x_k,U_{-(k-1)})=x_{k-1},U_{-(k-1)}\geq\beta,\dots,f(x_{1},U_0)=x_0,U_0\geq\beta] \\
    & = m_{x_k}\beta\cdot Q_{x_k x_{k-1}}(1-\beta)\dots Q_{x_1 x_0}(1-\beta) \, ,
\end{alignat*}
donde usamos que las uniformes son independientes.
Por otro lado, sumando, para $x_0,\dots,x_k$ se verifica que
$$ \P(-(\tau+1)=j,X\tau=x)=m_x\beta(1-\beta)^{j-1} \, ,$$
que es la ley geométrica buscada. Veamos ahora que $\P(X_0=x)$ es invariante.
% $$ \P(\tau=-k,X_{-k}=x_k) = m_{x_k}\beta(1-\beta)^k \, ,$$
% lo cual equivale a que
% $$ \P(\tau+1=-(k-1),X_\tau=x_k) = m_{x_k}\beta(1-\beta)^k \,$$
% con lo cual, colocado en términos de $-(\tau-1)$ y $j$ queda
% $$ \P(-(\tau-1)=j,X_\tau=x)=m_x\beta(1-\beta)^{j-1} \, .$$
\\ Denotando $\mu_x=\P(X_0=x)$, tenemos entonces que 
\begin{alignat*}{2}
    \mu_x & = \displaystyle \sum^\infty_{k=0}\sum_{x_1,\dots,x_k\in E}\P(\tau=-k,X_{-k}=x_k,\dots,X_{-1}=x_1,X_0=x) \\
    & = \sum_{k=0}^\infty(mQ^k)_x\beta(1-\beta)^k \\
    & = \beta\sum^\infty_{k=0}(mQ^{k+1})_x(1-\beta)^{k+1}+\beta m_x \\
    & = \sum_{y\in E}\beta\sum^\infty_{k=0}(mQ^{k})_y(1-\beta)^{k}[(1-\beta)Q_{yx}+\beta m_x] \\
    & = \sum_{y\in E}\P(x_0=y)P_{yx} = \sum_{y\in E}\mu_y P_{yx}\,,
\end{alignat*}
gracias a que $P_{xy}=(1-\beta)Q_{yx}+\beta m_x$\, y que $\P(x_0=y)=\beta\sum^\infty_{k=0}(mQ^{k})_y(1-\beta)^{k}$. Como
$$ \mu_x = \displaystyle\sum \mu_yP_{yx}$$
tenemos que $\mu_x$ es medida invariante. Como la medida invariante es única concluimos que $\mu_x=\pi\, . \findem$
\negro
\end{proof}

\subsubsection{Coupling from the past}
Sea $P$ finita irreducible (y entonces recurrente positiva) en un espacio $E=\{1,\dots,N\}$. \\ Sea $f:E\times[0,1]\mapsto E$ la función de transición asociada. Sean $(U_n:n\in\mathbb{Z},y\in E)\sim\unif\iid$ que simulamos una sola vez.
Entonces, tenemos funciones aleatorias independientes
\begin{alignat*}{2}
\Phi_n^y:E & \longmapsto E \\
x & \longmapsto \Phi_n(x) = f(x,U_n)\, ,
\end{alignat*}
con $n\in\mathbb{Z}, y\in E$. Las usaremos para construir, para cada $n\in\N$, $N$ cadenas de Markov $(X_{n,m}^y)_{m\geq n}$, $y=1,\dots,N$, partiendo en tiempo $n$ de los $N$ estados distintos, pero \textbf{coalescentes}. Esto es:
\begin{itemize}
    \item Para cada $y=1,\dots,N$, $X^y_{n,m}$ parte de $y$ y evoluciona usando el flujo $\Phi_m\circ\dots\circ\Phi_n:E\longmapsto E,$ es decir $X^y_{n,m}=\Phi_m\circ\dots\Phi_n(y)$.
    % Sean $(X^1_{n,n},X^2_{n,n},\dots,X^N_{n,n})=(1,2,\dots,N)$ y para $m>n$:
    % donde $x_y=\min\{x\in\{1,\dots,y-1\}:X^y_{n,n-1}=X^x_{n,m-1}\}$.
    \item Si $\exists m>n$ tal que $X^y_{n,m}=X^z_{n,m}=x$, en ese momento coalescen, pues ambas evolucionan desde ahí como $\Phi_{m+k}\circ\dots\circ\Phi_m(x)$, i.e., siguen igual desde aquel punto.
    \item Para cada $n\in-\N$, sea $S_n:=\inf\{m>n:X^1_{n,m}=\dots X^N_{n,m}\}$ el primer instante en que coalescen todas las cadenas iniciadas a tiempo $n$. Notar que $S_n$ puede ser infinito, pues puede ser que en ningún momento se junten todas. En la figura \ref{fig:past} visualizamos como van coalesciendo las cadenas.
    \item Sea $\tau_0:=\sup\{n\leq 0:S_n\leq 0\}$ el último $n\leq 0$ tal que las cadenas iniciadas en $n$ coalescen antes de $m=0$. $\tau_0$ representa aquel tiempo ``menos en el pasado'' tal que al lanzar las cadenas en ese momento hayan coalescido en $0$.
    \item En general, se necesitan condiciones para asegurar que $\tau_0>-\infty$.  % , de existir un orden en $E$, que se refleja en la funciones de transición de alguna forma, entonces si se puede.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[scale=0.16]{img/clase_12_pag_9.jpg}
    \caption{Ejemplo de $(X^y_{n,m})_{m\geq n}$ que coalescen.}
    \label{fig:past}
\end{figure}

% \Huge
% $$ 3\, n\,-(n+1)\,2n\,-3\,-2\,-1\,-\N\,N$$
% $$ \color{orange} 1\,=\,X_{n,n}^1\,\Phi(X_{n,n}^1,U_n)=X^1_{n,n-1}$$
% $$ \color{blue} 2\,=\,X_{n,n}^2$$
% $$ \color{red} 3\,=\,X_{n,n}^3\,\Phi(X_{n,n}^3,U_n)=X^3_{n,n-1}$$
% $$ \color{green} N = X_{n,n}^N\,\Phi(X_{n,n}^N,U_n)=X^N_{n,n-1}$$


\iffalse  %%%%%%%%%%%%%%%%%
\begin{alignat*}{2}
    X^1_{n,m} & = \Phi^1_m(X^1_{n,m-1}) \\
    X^2_{n,m} & = \begin{cases}
        \Phi^2_m(X^2_{n,m-1}) & \mbox{ si }X^2_{n,m-1}\neq X^1_{n,m-1}\\
        \Phi^1_m(X^2_{n,m-1}) & \mbox{ si }X^2_{n,m-1}= X^1_{n,m-1}
    \end{cases} \\
    \vdots & \\
    X^y_{n,m} & = \begin{cases}
        \Phi^y_m(X^y_{n,m-1}) & \mbox{ si }X^y_{n,m-1}\notin\{ X^1_{n,m-1},\dots,X^{y-1}_{n,m-1}\}\\
        \Phi^{x_y}_m(X^y_{n,m-1}) & \mbox{ si }X^y_{n,m-1}\in\{X^1_{n,m-1},\dots,X^{y-1}_{n,m-1}\}
    \end{cases} \\
    \vdots & \\
    X^N_{n,m} & = \begin{cases}
        \Phi^N_m(X^N_{n,m-1}) & \mbox{ si }X^N_{n,m-1}\notin\{ X^1_{n,m-1},\dots,X^{N-1}_{n,m-1}\}\\
        \Phi^{x_N}_m(X^N_{n,m-1}) & \mbox{ si }X^N_{n,m-1}\in\{X^1_{n,m-1},\dots,X^{N-1}_{n,m-1}\}
    \end{cases}\, ,
\end{alignat*}
\fi  %%%%%%%%%%%%%%%%%%%%5
\vspace{.5cm}\\
\begin{theorem}
Si $\tau_o>-\infty$ c.s., se tiene 
$$X^y_{\tau_0,0}\sim\pi\espacio\forall y\in E \,.$$
\end{theorem}
\vspace{.5cm}\\
\begin{proof}  % min 38
\gris
$\forall k\in -\N$, $x,y\in E$
$$ \P(X^y_{\tau_0,0}=x,\tau_0>k) = \P(X^y_{k,0}=x,\tau_0>k) \,.$$
Luego $\P(X^y_{\tau_0,0}=x)=\displaystyle\lim_{k\to-\infty}\P(X^y_{k,0}=x)$. Por otro lado
\begin{alignat*}{2}
    |\P(X^y_{k,0}=x)-\pi_x| & = |\P(X_0=x|X_k=y)-\displaystyle\sum_z\pi_z\P(X_0=x|X_n=z)| \\
     & \leq \sum_{z}\pi_z|\P(X_0=x|X_k=y)-\P(X_0=x|X_k=z)|\,.
\end{alignat*}
En general, si $(V,W)$ es un coupling de dos leyes $\mu$ y $\nu$,
\begin{alignat*}{2}
\mu_y = \P(V=y) & = \P(V=y,W=y)+\P(V=y,W\neq y) \\
 & \leq \P(W=y)+\P(V\neq W) \\
 & = \nu_y + \P(V\neq W)\,. \\
 \therefore |\mu_y-\nu_y| & \leq \P(V\neq W) \\
 \therefore |\P(X^y_{k,0}=x)-\pi_x| & \leq \displaystyle\sum_z\pi_z\P(X^y_{k,0} \neq X^z_{k,0}) \\
 & \leq \P(\tau_0<k) \mbox{ }\substack{\longrightarrow \\ k\to-\infty}\mbox{ }0\, .
\end{alignat*}
Entonces,
$$ \P(X^y_{\tau,0}=x)=\mbox{ }\substack{\longrightarrow \\ k\to-\infty}\mbox{ }\P(X^y_{k,0}=x)=\pi_x \, .$$
\findem
\negro
\end{proof}
\vspace{.5cm}\\
\subsubsection{Criterio Foster-Lyapunov para convergencia geométrica}
\begin{theorem}[de Harris]
Sea $(X_n)_{n\in\N}$ cadena de Markov en $E$ con matriz $P$ irreducible tal que
\begin{itemize}
    \item $\exists K\subseteq E$, $\exists\beta>0$, $m\in\mathcal{P}(E)$, $n_0\in\N$ tal que
    $$ (P^{n_0})_{xy}\geq\beta_{my}\espacio\forall x\in K,\,\forall y\in E \, .$$
    Esto es, una condición de tipo Doeblin (D) en $K$.
    \item $\exists V:E\longmapsto [1,\infty)$, $\rho\in(0,1)$, $c>0$ tal que
    $$ PV(x)\leq\rho V(x)+c\mathbf{1}_K(x) \espacio\forall x\in E \,.$$
    (Esta condición de ``tipo Lyapunov'' fuera de $K$ nos dice que $V$ tiende a decrecer en promedio.)
\end{itemize}
Entonces, $(X_n)_{n\in\N}$ es recurrente positiva, y $\exists\theta\in(0,1)$, $M>0$ tal que $\forall x\in E$
$$ \|P^n_{x_0}-\pi\|_1\leq M\theta^n \, ,$$
i.e., es uniformemente ergódica.
\end{theorem}
\vspace{.5cm}\\
Probaremos sólo un resultado intermedio:
\begin{lemma}
Sea $\tau_K=\inf\{n\geq1:X_n\in K\}$ tiempo de parada. Entonces $\forall x\notin K$,
$$ \E(\rho^{-\tau_K})<\infty \, .$$
\end{lemma}
\begin{remark}
Luego $\tau_K$ tiene un momento exponencial ($\rho^{-1}>1$), y $\forall n$
$$ \P_x(\tau_K>n)=\E_X(\mathbf{1}_{\tau_k>n})=\E_X(\mathbf{1}_{\rho^{-\tau_X}>\rho^{-n}})\leq\rho^n\E_X(\rho^{-\tau_k}) \, ,$$
i.e., $\tau_k$ tiene ``cola geométrica'' (y entonces toma valores grandes con baja probabilidad).
\end{remark}
\begin{proof}
\gris
Probaremos que $Y_n:=\rho^{-\min\{n,\tau_K\}}V(X_{\min\{n,\tau_K\}})$ es una sobre-martingala en la filtración $\mathcal{F}_n:=\sigma(X_0,\dots,X_n)$, esto es. $\E_x(Y_{n+1}|\mathcal{F}_n)\leq Y_n$. Como las esperanzas decrecen, $\E(Y_n)\leq E_x(Y_0)$, y dado que  $V(\cdot)\geq 1$, se obtendr\'a entonces que
$$ \E_x(\rho^{\min\{n,\tau_K\}})\leq\E_X(\rho^{\min\{n,\tau_K\}V(X_{\min\{n,\tau_K\}})})\leq V(x)<\infty \, . $$
Luego, tomando $n\to\infty$, por T.C.M. se concluye que  $\E_x(\rho^{-\tau_K})<\infty$. Estudiemos entonces  
\begin{alignat*}{2}
\E_x(Y_{n+1}|\mathcal{F}_n) & = \E_x(\rho^{-(n+1)}V(X_{n+1})\mathbf{1}_{\tau_K>n}|\mathcal{F}_n)+\E_x(\rho^{-\tau_K}V(X_{\tau_K})\mathbf{1}_{\tau_K\leq n}|\mathcal{F}_n) \, .
\end{alignat*}
El primer término queda
\begin{alignat*}{2}
\E_x(\rho^{-(n+1)}V(X_{n+1})\mathbf{1}_{\tau_K>n}|\mathcal{F}_n) & = \rho^{-(n+1)}\E(V(X_{n+1})|\mathcal{F}_n)\mathbf{1}_{\tau_K>n} \\
& = \rho^{-(n+1)}PV(X_n)\mathbf{1}_{\tau_K>n} \\
& \leq \rho^{-n}V(X_n)\mathbf{1}_{\tau_K>n}, 
\end{alignat*}
donde usamos la propiedad de Markov y que $X_n\notin K$ en $\{\tau_K>n\}$ (entonces  $c\mathbf{1}_{K}(x)=0$). Por otro lado en el segundo término podemos sacar la $\E(\cdot|\mathcal{F}_n)$ pues $\mathcal{F}(X_{\tau_K})\mathbf{1}_{\tau_K\leq n}$ es $\mathcal{F}_n$-medible. Luego
\begin{alignat*}{2}
\E_x(Y_{n+1}|\mathcal{F}_n) & \leq \rho^{-n}V(X_n)\mathbf{1}_{\tau_K>n}+\rho^{-\tau_K}V(X_{\tau_K})\mathbf{1}_{\tau_K\leq n} \\
& = \rho^{\min\{n,\tau_K\}}V(X_{\min\{n,\tau_K\}})=Y_n \, .
\end{alignat*}
\findem
\negro
\end{proof}

\newpage
\section{Algoritmos estocásticos basados en CM}
Referencias en Pardoux \cite{pardoux} y Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., Teller, E.  \cite{metro}.
\subsection{Cadenas de Markov reversibles} % clase 12 lunes 5 oct
\begin{proposition}
\label{prop:4_1_1}
Sea $\xcm\sim\cm$, $n\in\N$ entonces $(\hat{X}_n)_{n=0}^N:=(X_{N-n})_{n=0}^N$ es cadena de Markov \textbf{no homogénea} con
$$ \P(\hat{X}_{n+1}=y|\hat{X}_n=x)=\displaystyle\frac{(\mu P^{N-n-1})_y}{(\mu P^N)_x}P_{yx} \, .$$
En particular si $\mu=\pi$ con $\pi$ distribución invariante de $P$, donde $P$ es irreducible, entonces $(\hat{X}_n)_{n=0}^N$ es $CM(\pi,\hat{P})$ homogénea con matriz de transición:
$$ \hat{P}_{xy}=\displaystyle\frac{\pi_y}{\pi_x}P_{yx} \, .$$
\end{proposition}
\begin{proof}
\gris
\begin{alignat*}{2}
    \P(\xhat_{n+1}=y|\xhat_{n}=X_n,\dots,\xhat_0=x_0) & = \displaystyle \frac{\P_\mu(X_N=x_0,\dots,X_{N-n}=x_n,X_{N-n-1}=y)}{\P_\mu(X_N=x_0,\dots,X_{N-n}=x_n)} \\
     & = \frac{(\mu P^{N-n-1})_yP_{y,x_n}P_{x_n,x_{n-1}},\dots,P_{x_0,x_0}}{(\mu P^{N-n})_{x_n}P_{y,x_n}P_{x_n,x_{n-1}},\dots,P_{x_1,x_0}} \\
     & =  \frac{(\mu P^{N-n-1})_yP_{y,x_n}}{(\mu P^{N-n})_{x_n}} = \frac{\P_\mu(X_{N-n-1}=y,X_{N-n}=x_n)}{\P_\mu(X_{N-n}=x_n)}\\
     & = \P(\hat{X}_{n+1}=y|\hat{X}_n=x_n) \, .
\end{alignat*}
\findem
\negro 
\end{proof}
\begin{definition}[Reversibilidad]
Sea $\xcm$ cadena de Markov irreducible recurrente positiva en equilibrio. Decimos que es reversible si $\forall n\in\N$
$$ Ley((X_n)_{n=0}^N)=Ley((X_{N-n})_{n=0}^N)\espacio(=Ley((\hat{X}_{n})_{n=0}^N)) \, .$$
\end{definition}
\begin{proposition}[Condición de balance detallado]
Sea $\xcm$ cadena de Markov irreducible recurrente positiva en equilibrio. $X$ es \textbf{reversible} si y sólo si $(\pi,P)$ cumplen la condición de balance detallado:
$$ \pi_xP_{xy}=\pi_yP_{yx}\forall x,y\in E\, .$$
\end{proposition}
\begin{proof}
\gris
\mbox{ }\newline ($\Rightarrow$) Reversible $\implies \P_\pi(X_0=x,X_1=y)(=\pi_xP_{xy})=P_\pi(X_1=x,X_0=y)(=\pi_yP_{yx})$
\newline ($\Leftarrow$) \mbox{Balance detallado } $\implies \hat{P}_{xy}:=\displaystyle\frac{\pi_y}{\pi_x}P_{yx}=P_{xy}$
\newline $\therefore (\hat{X}_n)_{n=0}^N\sim CM(\pi,P)$ gracias a la proposición \ref{prop:4_1_1}. \findem
\negro
\end{proof}
\vspace{.5cm}\\
\begin{remark}
\beforeitemize
\begin{itemize}
    \item Notación: si $(\pi,P)$ están en balance detallado, decimos también que $\pi$ es reversible con respecto a $P$ y vice-versa.
    \item Si tenemos $P$ matriz estocástica irreducible y $\pi\in\mathcal{P}(E)$ es reversible (i.e., en balance detallado) con respecto a $P$,  entonces $\pi$ es invariante para $P$. En efecto: 
    $$ \pi_xP_{xy}=\pi_yP_{yx}\implies (\pi P)_y=\pi_y (\mbox{ sumando para }x\in E)\, .$$
    La recíproca no es cierta en general.
    \item $\pi$ es reversible con respecto a $P$ si y solo si  $$\P_\pi(X_{n+1}=y,X_n=x)=\P_\pi(X_{n+1}=x,X_n=y)\espacio \forall x, y \in E \,.$$
    \item $\pi$ es invariante con respecto a $P$ si y solo si % $\ssi$  
    $$\forall y \in E,\espacio \P_\pi(X_{n+1}=y,X_n\neq y)=\P_\pi(X_{n+1}\neq y,X_n\neq y)\,.$$
\end{itemize}
\end{remark}
\begin{example}[Grafo no-orientado finito]
Sea $G$ grafo no-orientado finito. Sea $(X_n)_{n\in\N}$ un paseo aleatorio simple, es decir: 
$$ P_{xy}=\P(X_{n+1}|X_n=x):=\begin{cases}
\frac{1}{deg_x}   & \mbox{ si }y\sim x\\
0   & \mbox{ si no,}
\end{cases}$$
con $deg_x=|\{y|y\sim x\}|$ (i.e., el grado de cada vértice). Entonces
$$ deg_x\cdot P_{xy}=1=deg_y \cdot P_{yx}, \forall x,y$$
$$ \therefore \pi=(\pi_x)_{x\in E}=\displaystyle\bigg(\frac{deg_x}{\sum_{y\in E}deg_y}\bigg)_{x\in E} \mbox{ está en balance detallado con }P \, .$$
$$ \therefore \pi \mbox{ es invariante.}$$
\end{example}
\subsection{Markov Chain Monte Carlo}
\subsubsection{Idea general}
Sea $\pi\in \mathcal{P}(E),\pi>0$
\newline \textbf{Pregunta: ¿existe $P$ matriz estocástica (irreducible) tal que $\pi P=\pi$?} ($\pi$ invariante con respecto a $P$).
\newp La utilidad de esto sería que si queremos \textbf{simular} aproximadamente una variable aleatoria $x_\infty\sim\pi$, basta encontrar $P$ tal que $\pi P=\pi$ y simular $(X_n)_{n\in\N}\sim \cm$ por tiempo suficiente.
\newline \textbf{Es más fácil buscar $P$ matriz estocástica tal que $\pi_xP_{xy}=\pi_yP_{yx} \mbox{ (reversible) }, \forall x.y\in E$}\,.

\newp \textbf{Objetivo}: dado $\pi\in\mathcal{P}(E),\pi>0$, queremos construir $P$ irreducible tal que $(\pi,P)$ estén en balance detallado y tal que $(X_n)_{n\in\N}\sim CM(\mu,P)$ es fácilmente simulable.
\subsubsection{Los métodos MCMC}
Partimos con $R=(R_{xy})_{xy\in E}$ matriz de transición irreducible ``cualquiera'' tal que $\forall x,y$,\\ $R_{xy}>0\implies R_{yx}>0$ y además, cuyas transiciones (de $CM(\pi,R)$) sean fáciles de simular.
\newline Luego definimos
$$ P_{xy}=\begin{cases}
\min(R_{xy},(\frac{\pi_y}{\pi_x})R_{yx})  & \mbox{ si }x\neq y\\
1-\displaystyle\sum_{z\neq x}P_{xz}  & \mbox{ si }x=y
\end{cases}$$
\begin{proposition}
$P=(P_{xy})$ es matriz estocástica irreducible y $(\pi,P)$ están en balance detallado.
\end{proposition}
\begin{proof}
\gris
\beforeitemize
\begin{itemize}
    \item Veamos que es matriz estocástica: $\sum_{y\neq x}p_{xy}\leq \sum_{y\neq x}R_{xy}\leq 1 \implies P_{xx}\in[0,1]$ y $\sum_z P_{xz}=1$.
    \item Para la irreducibilidad notemos que $\forall x,y\in E$ existen $n,x_1,\dots,x_n\in E$ tal que 
    $$R_{xx_1},R_{x_2,x_3},\dots,R_{x_ny}>0\,,$$
    luego $R_{yx_n},R_{x_n,x_{n-1}},\dots,R_{x_1y}>0$, con lo cual $P_{xx_1},\dots,P_{x_ny}>0$.
    \item El caso $x=x$ es directo. Para $x\neq y$, $\pi_xP_{xy}=\pi_xR_{xy}\land \pi_yR_{yx}=\pi_yP_{yx}$. \\ Entonces se tiene la condición de balance detallado.
\end{itemize}
\findem
\negro
\end{proof}

\newp \textbf{¿Cómo escoger $R$?}
\newline Elegimos un grafo no orientado $G$ con conjunto de vértices $E$ y  $R$ tal que $\forall x,y$, $R_{xy}>0$ si y sólo si $x \sim y$ (vecino) en $G$.
\newline Dos elecciones ``clásicas'' son:
\begin{itemize}
    \item \textbf{Gibbs sampler} (muestreo de Gibbs)
    $$ R_{xy}=\begin{cases}
    \pi_{y}(\displaystyle\sum_{z\sim x}\pi_z)^{-1}  & \mbox{ si }x\sim y\\
    0  & \mbox{ si no}
    \end{cases}$$
    \item \textbf{Algoritmo metrópolis} (paseo aleatorio simple en $G$)
    $$ R_{xy}=\begin{cases}
    \displaystyle\frac{1}{deg_x} & \mbox{ si }x\sim y\mbox{ con }deg_x=|\{y:y\sim x\}|\\
    0  & \mbox{ si no}
    \end{cases}$$
\end{itemize}
\subsubsubsection{Metropolis-Hasting}
\label{m-h}
\textbf{¿Cómo simular $(X_n)_{n\in\N}\sim CM(\mu,P)$?}
\newline Sean $(V_n)_{n\in\N}\sim\iid \unif$ y $f:[0,1]\times E\to E$ función de transición asociada a $R$.
\newline Sean $(U_n)_{n\geq 1}\sim \iid\unif$ independientes de las $(V_n)_{n\in\N}$. Simulamos $X_0=Y_0\sim\mu$ usando $V_0$.
\newline Luego, recursivamente definimos:
\begin{itemize}
    \item Dado $X_n=x$, simulamos
    $$ Y_{n+1}:=f(V_{n+1},x)=y \mbox{, es decir, una transición según }R\,.$$
    \item Definimos
    $$ X_{n+1}=\begin{cases}
    Y_{n+1}  & \mbox{ si } \espacio U_{n+1}\leq \displaystyle\frac{\pi_y R_{yx}}{\pi_x R_{xy}}  \\
    X_n  & \mbox{ si } \espacio U_{n+1}> \displaystyle\frac{\pi_y R_{yx}}{\pi_x R_{xy}}
    \end{cases}$$
\end{itemize}
\begin{proposition}
$$ (X_n)_{n\in\N}\sim CM(\mu,P)$$
\end{proposition}
\begin{proof}
\gris
$\forall x\neq y$ tenemos:
\begin{alignat*}{2}
    \P(\xhat_{n+1}=y,\xhat_{n}=y) & = \P\bigg(f(V_{n+1},x)=y,X_n=x,U_{n+1}\leq \displaystyle\frac{\pi_y R_{yx}}{\pi_x R_{xy}}\bigg) \\
     & = R_{xy}\P\bigg(U_{n+1}\leq \frac{\pi_y R_{yx}}{\pi_x R_{xy}}\bigg)\P(X_n=x)\\
     & = R_{xy}\min\bigg(\frac{\pi_y R_{yx}}{\pi_x R_{xy}},1\bigg)\P(X_n=x) \,,
\end{alignat*}
$\implies \P(X_{n+1}=y| X_n=x)=\displaystyle\min\{\frac{\pi_y}{\pi_x}R_{yx},R_{xy}\}=P_{xy}$\,,
\newline y $\P(\xhat_{n+1}=y|\xhat_{n}=x)=1-\displaystyle\sum_{y\neq x}\P(X_{n+1}=y|X_n=x)=1-\sum_{y\neq x}P_{xy}=P_{xx}$\,. \findem
\negro
\end{proof}
\begin{remark}
La construcción sólo requiere conocer $\lambda = \alpha \pi$ con $\alpha>0$ una constante (depende sólo de $\displaystyle \frac{\pi_x}{\pi_y},x$ e $y$). % \newline 
Esto es muy importante en la práctica pues muchas veces se conoce sólo la medida $\lambda$ en $E$, y calcular la constante de normalización puede ser inviable numéricamente si $E$ es grande.
\end{remark}
\vspace{.5cm} \\ %%%%%%%%%%
\begin{remark}
El grafo $G$ debe escogerse idealmente de forma que
\begin{itemize}
    \item No haya estados ``muy aislados'', de modo que una $CM(\mu,R)$ lo ``recorre bien'' y $(X_n)_{n\in\N}\sim CM(\mu,P)$ ``alcanza rápido'' el equilibrio.
    \item Las transiciones desde cada $x$ sean fáciles de simular (lo que es m\'as difícil si $x$ tiene demasiados vecinos).
\end{itemize}
Notar que estas dos propiedades apuntan  en sentidos contrarios.
\end{remark}
\vspace{.5cm} \\ %%%%%%%%%%
\begin{remark}  % clase 13 7 oct
En el caso Gibbs,  para calcular $(R_{xy})_{xy\in E}$, hay que calcular sumas $\sum_{z\sim x}\pi_z$, $x\in E$\,.
    \newline \underline{Atención}: si $x$ tiene muchos vecinos, calcular estas sumas puede ser impracticable. Entonces es mejor usar Metropolis.
\end{remark}
\subsection{Aplicación de MCMC: simulated annealing}
%\subsubsection{Simulated Annealing}
Simmulated annealing (``recocido simulado'') tiene como objetivo \textbf{minimizar} una funci\'on o  ``energía'' $U:E\to\R$ (donde $E$ es grande) con un algoritmo estocástico.
\newp Consideramos un par\'ametro  $\beta>0$ que denominamos ``temperatura inversa'' (i.e., $T=\beta^{-1}$ representa la temperatura).
\vspace{.2cm} \\ %%%%%%%%%%
\begin{definition}[Medida de Gibbs]
Definimos $\pi^\beta\in\mathcal{P}(E)$ mediante:
$$ \pi^\beta_x = \displaystyle \frac{\exp^{-\beta U(x)}}{Z_\beta}\,,$$
con
$$ Z_\beta = \displaystyle\sum_{y\in E}\exp^{-\beta U(y)} \espacio\mbox{constate de normalización}\,.$$
A $\pi^\beta$ se le llama medida de Gibbs.
\vspace{.5cm} \\ %%%%%%%%%%
\end{definition}
\begin{remark}
\beforeitemize
\begin{itemize}
    \item $\pi^\beta$ da más probabilidad a los $x$ con menor $U(x)$\,.
    \item Cuando $\beta \searrow 0$ ($T\nearrow\infty$): $\pi^\beta\mbox{ }\substack{\longrightarrow \\ \beta\to\infty}\mbox{ }Unif(E)$\,,
    i.e., es indiferente de la energia $U$\,.
    \item ¿Que pasa cuando $\beta\nearrow \infty$ ($T\searrow 0$)?
    \newline Sean $U_*=\min U$, $A_U=\arg\min U$,
    \begin{alignat*}{2}
        \pi^\beta_x & = \displaystyle\frac{e^{-\beta U(x)}}{\sum_{y\inA_U}e^{-\beta U(y)}+\sum_{y\in A^C_U}e^{-\beta U(y)}} \\
         & = \displaystyle\frac{e^{-\beta U(x)}}{\# A_U e^{-\beta U_*}+\sum_{y\in A^C_U}e^{-\beta U(y)}} \\
         & = \displaystyle\frac{e^{-\beta (U(x)-U_*)}}{\# A_U +\sum_{y\in A^C_U}e^{-\beta (U(y)-U_*)}}) \mbox{ }\substack{\longrightarrow \\ \beta\to\infty}\mbox{ }\begin{cases}
    \frac{1}{\# A_U}  & \mbox{ si } x\in A_U  \\
    0  & \mbox{ si } x \notin A_U
    \end{cases}
    \end{alignat*}
    Entonces si $\beta\nearrow \infty$,\espacio $\pi^\beta\mbox{ }\substack{\longrightarrow \\ \beta\to\infty}\mbox{ }\pi^\infty=Unif(A_U)$\,.
    \item Para cada $\beta>0$ sabemos simular $(X^\beta_n)_{n\in\N}$ (reversible) que converge en ley a $\pi^\beta\propto e^{-\beta U}$ cuando $n\to\infty$\,.
\end{itemize}
\end{remark}
La idea del método \textbf{Simmulated annealing} consiste en tomar $\beta_n\searrow \infty$ y simular una cadena de markov no homogénea $\xcm$``$=$''$(X_n^{\beta_n})_{n\in\N}$ (usando Metropolis-Hastings (\ref{m-h})), es decir, en cada tiempo $n$, simular transición tal que
$$ \P(X_{n+1}=y|X_n=x)=\P(X_{n+1}^{\beta_{n+1}}=y|X_n^{\beta_{n+1}}=x)\,.$$
\vspace{.5cm}\\
Antes de estudiar como hacerlo, veamos qu\'e hace $(X^\beta_n)_{n\in\N}$ cadena simulada con Metropolis-Hastings con $\beta>0$ fijo.
\begin{itemize}
    \item Contamos con $G$ grafo \underline{regular} no orientado con conjunto de v\'ertices $E$\,.
    \item $\pi^\beta \propto e^{-\beta U(x)}, \forall x \in E$\,.
    \item En el tiempo $n$, dado que $X_n=x$, simulamos
    $ y=Y_{n+1}\sim Unif\{z:z\sim x\}$, transición desde $x$ para un paseo aleatorio simple ($R_{xy}=(deg(G))^{-1}, \forall x\sim y$)\,.
    \item Sampleamos $U_{n+1}\sim\unif \indep$ de todo y 
    $$ X_{n+1}^\beta =\begin{cases}
    Y_{n+1}=y  & \mbox{ si } U_{n+1}\leq\displaystyle\min\bigg\{\displaystyle\frac{\pi_y^\beta}{\pi_x^\beta},1\bigg\} , \\
    X_n^\beta  & \mbox{ si } U_{n+1}>\min\bigg\{\displaystyle\frac{\pi_y^\beta}{\pi_x^\beta},1\bigg\}\espacio.
    \end{cases}$$
    Notemos que  $\min\bigg\{\displaystyle\frac{\pi_y^\beta}{\pi_x^\beta},1\bigg\} = \min \{ e^{-\beta(U(y)-U(x))},1\}$. Luego: 
    \begin{itemize}
        \item Si $U(y)<U(x)$ la transición siempre se realiza pues  el m\'inimo es $1$ y $U_{n+1}\leq 1$ siempre. Esto quiere decir que si la energía del estado propuesto $y$ es menor que la del estado actual $x$, la transición se realiza de todos modos.
        \item Si $U(y)\geq U(x)$  el m\'inimo es menor que $1$, luego se realiza la transici\'on si  $U_{n+1}\leq e^{-\beta(U(y)-U(x))}$, y en caso contrario ($>$) no. As\'i, si la propuesta es transitar a un estado de mayor energía, esto puede llevarse acabo a veces,  pues puede permitir salir de un mínimo local, distinto del mínimo  global buscado.
    \end{itemize}
\end{itemize}
Entonces,  si $\beta\gg 1 $ ($0<T\ll  1 $) el valor de la energ\'ia es menos propenso a ``subir'' en cada paso, y \textbf{tiende a ir hacia un m\'inimo}  (que puede no ser global). Por otro lado si $0<\beta\ll  1$ ($T\gg 1$),  la energ\'ia fluct\'ua de manera más aleatoria, pudiendo  subir o bajar en cada paso, lo que permite \textbf{escapar de mínimos locales}.
\newp Así, para $n$ pequeño ($\beta_n$ pequeño y $T_n$ alta), $X_n$ tiende a pasearse por todo $E$ sin tomar muy en cuenta el ''paisaje de energ\'ia'' dado por $U$ (desorden), mientras que, para $n$ grande ($\beta_n$ grande y $T_n$ chico) $X_n$ se mueve muy poco, sólo alrededor de algún mínimo (local) hasta que se ``congela'' ahí. 

El  siguiente resultado indica una elecci\'on  te\'orica para $\beta_n$ que asegura convergencia a m\'inimos globales: 


\begin{theorem}
Sea $$\triangle >Osc(U):=\displaystyle\max_{x\in E}U(x)-\min_{x\in E}U(x)\,.$$
Entonces, si $\beta_n:=\displaystyle\frac{1}{\triangle}\log(1-n)$
$$ \implies Ley(X_n)\conv\pi^\infty=Unif(A_U) \mbox{ minimizante}\,.$$
Más aún, se prueba que $X_n\convp A_U$ (mínimo global).
\end{theorem}
\begin{proof}
\gris
Ver Pardoux cap 10 \cite{pardoux} para una versión para C.M. en tiempo continuo. 
\negro
\end{proof}
% \vspace{1cm} \\ %%%%%%%%%%
\begin{remark}
\beforeitemize
\begin{itemize}
    \item En la práctica $\ln(1+n)$ es demasiado lento para poder observar una evoluci\'on descendiente de la energ\'ia.
    \item Usualmente se suele escoger:
    \begin{itemize}
        \item $\beta_n$ polinomial en $n$ (por ejemplo $n^2$)
        \item $\beta_n$ exponencial en $n$, 
    \end{itemize}
    pero no hay garantías de convergencia a $A_U$ en esos casos.
    \item En cada problema hay que jugar con distintas sucesiones $(\beta_n)_{n\in\N}$ tal que $\beta_n\nearrow \infty$\, y puntos de inicio $X_0$, y escoger finalmente el mejor m\'inimo encontrado. 
    \item El algoritmo da una heurística para encontrar buenos mínimos locales más eficiente que optimización discreta.
\end{itemize}
\end{remark}
\textbf{Idea física}:
\newline ``Annealing'' = recocido, viene de la metalurgia y se refiere a un  procedimiento para hacer más dúctil (menos duro) una aleación, calentando(a) por sobre el nivel de cristalización y dejándola enfriar lentamente para alcanzar un estado de energía potencial basal material homogéneo, pocas ``dislocaciones''.
\newline Si el enfriamiento es muy rápido queda un material homogéneo pero duro:
\newline ``Quenching''=templado.
\begin{figure}
    \centering
    \includegraphics[scale=0.16]{img/clase_14_pag_12.jpg}
    \caption{Idea del efecto de las sucesiones $\beta_n$ en la minimización de la energía $U$.}
    \label{fig:betas}
\end{figure}

\newpage
\subsection{MCMC y estadística Bayesiana}
\subsubsection{Recuerdo de estadística Bayesiana}
\textbf{Idea}: modelamos simultaneamente y probabilisticamente:
\begin{itemize}
    \item Observaciones $x$ de un fenómeno o dato aleatorio, cuya ley ``$p(x|\theta)$'' depende de un parámetro $\theta\in\Theta$.
    \item La incertidumbre, desconocimiento o conocimiento parcial de $\theta$, lo que representamos asumiendo que $\theta$ es aleatorio y sólo conocemos su distribución ``$p(\theta)$'' ``a priori''.
    \item En lo anterior, la observación $x$ está en un conjunto $\mathfrak{X}$ que puede ser subconjunto de $\R^d$, un conjunto finito o numerable, etc. 
    \newline En general $x\mapsto p(x|\theta)$ (ley de $x$ dado $\theta$) es
    \begin{itemize}
        \item o bien una función de masa discreta: 
        $$\displaystyle\sum_{x\in \mathfrak{X}}p(x|\theta)=1$$
        \item o bien una densidad de probabilidad: 
        $$\displaystyle\int_{\mathfrak{X}}p(x|\theta)dx=1\,.$$
    \end{itemize}
    \item $x$ puede ser una ``muestra'' $x=(x_1,\dots,x_n)$.
    \item $\Theta$ puede ser subconjunto de $\R^k$, conjunto finito o numerable y $p(\theta)$ denota, seg\'un corresponda:
    \begin{itemize}
        \item una función de masa discreta
        \item o bien una densidad de probabilidad,
    \end{itemize}
    con respecto a $\theta$,  y se le llama \textbf{ley a priori}.
    \item $p(x,\theta):=p(x|\theta)p(\theta)$ es ley conjunta en $\mathfrak{X}\times \Theta$ del parámetro y una observación.
    \newline Notar que
    $$ \displaystyle\int_\Theta\int_\mathfrak{X} p(x,\theta)dxd\theta=\int_\Theta\int_\mathfrak{X} p(x|\theta)dx p(\theta)d\theta=1\,.$$
    \item Dado $\theta\in\Theta$, $x\mapsto p(x|\theta)$ se llama ley de $x$ dado $\theta$, o bien la densidad de $x$ condicional a $\theta$.
    \item Dado $X\in\mathfrak{X}$ observación, $\theta\mapsto L(\theta)=L_X(\theta)=p(X|\theta)$ se llama \textbf{función de verosimilitud} (likelihood). Notar que $\int L(\theta)d\theta\neq1$ en general.
\end{itemize}
\textbf{Objetivo de la inferencia Bayesiana}: ``Re-estimar'' el parámetro $\theta$ (modificar o actualizar la ley que describe lo que sabemos de $\theta$) usando la información que nos da el observar $x$.
\begin{theorem}[Bayes]
La ley de $\theta$ dado $x$, también llamada ley posterior de $\theta$, está dada por:
\begin{alignat*}{2}
    p(\theta|x) & := \displaystyle \frac{p(x,\theta)}{p(x)}\\
     & = \displaystyle\frac{p(x|\theta)p(\theta)}{p(x)} \,,
\end{alignat*}
con $p(x)=\displaystyle\int_\Theta p(x,\theta)d\theta$ ley marginal (no condicional).


\end{theorem}
Luego $p(\theta|x)\propto p(x|\theta)p(\theta)$. De esta forma,   $p(x)$ aparece  sólo como una constante de normalización, dependiente de la observaci\'on $x$, cuyo c\'alculo requiere en general sumar o integrar sobre todo el espacio. Por ello,    \textbf{calcular la constante de normalizaci\'on  $p(x)$ puede ser muy costoso,   y hay  evitar tener que hacerlo}.  Es por este motivo que los m\'etodos MCMC son muy \'utiles en este contexto, como veremos un poco m\'as adelante. 
\subsubsection{Aplicaciones de estadística Bayesiana}
\begin{itemize}
    \item Observamos $x_!,\dots,x^n \iid \sim p(x|\theta)dx$ con $\theta$ fijo.
    \newline Sean $\mathcal{D}=\{x_1,\dots,x_n\}$ ``datos'' y su densidad conjunta dada por:
    $$ p(\mathcal{D}|\theta):=p(x_1,\dots,x_n|\theta)=\displaystyle\prod^n_{i=1}p(x_i|\theta)\, . $$
    Además,  consideramos su función de verosimilitud:
    $$ L_\mathcal{D}(\theta)=L_{x_1,\dots,x_n}(\theta)=\displaystyle\prod^n_{i=1}L_{x_i}(\theta)$$
    \item Luego la ley a posteriori de $\theta$ dados $\mathcal{D}$ es la ley en $\Theta$: $$\mathcal{D}=p(\theta|x_1,\dots,x_n)=\displaystyle\frac{\prod^n_{i=1}p(x_i|\theta)p(\theta)}{p(\mathcal{D})}\propto \prod^n_{i=1}p(x_i|\theta)p(\theta)\,$$
    donde la última expresión es evaluable.  $p(\mathcal{D})$ es la densidad marginal de los datos y requiere integrar:
    $$ \displaystyle \int_\Theta p(\mathcal{D}|\theta)p(\theta)d\theta=\int_\Theta\prod^n_{i=1}p(x_i|\theta)p(\theta)d\theta\,.$$
    \item En base a $p(\theta|\mathcal{D})=p(\theta|x_1,\dots,x_n)$, finalmente se construye un ``\textbf{posterior predictivo}'', i.e., un valor $\hat{\theta}_n\in\Theta$ (``estimador de $\theta$'') que mejor explica los datos $x_1,\dots,x_n$.
\end{itemize}
\vspace{1cm}\\
\subsubsubsection{Ejemplos de estimadores Bayesianos}
Los siguiente son estimadores Bayesianos clásicos (hay muchos otros):
\begin{itemize}
    \item \textbf{Media a posteriori}
    \newline Considera  como estimador predictivo la media con respecto a la ley a posteriori de $\theta$.
    $$ \hat{\theta}_n = \displaystyle\int_\Theta p(\theta|\mathcal{D})d\theta\,.$$
    \item \textbf{Máximo a posteriori}
    \newline Es un par\'ametro cuyo  valor  maximiza la probabilidad a posteriori de observar la data $\mathcal{D}$
    \begin{alignat*}{2}
        \hat{\theta}_n & := \displaystyle\arg\max_{\theta} p(\theta|\mathcal{D})\\
         & = \arg\max_{\theta}[\sum^n_{i=1}\log p(x_i|\theta)+\log p(\theta)]\,.
    \end{alignat*}
    En la práctica, la maximización se lleva a cabo como en la última expresión.
\end{itemize}
\begin{remark}
\beforeitemize
\begin{itemize}
    \item También es posible construir regiones o \textbf{regiones o intervalos de confianza} para $\theta$, es decir (para $\theta$ real), un intervalo $I$ tal que $\P(\theta\in I | X_1,\dots,X_n)\approx 95\%$ (por ejemplo).
    \item Se requiere ``conocer'' $p(\theta|\mathcal{D})$ para optimizar en $\theta$ o bien para integrar $p(\theta|\mathcal{D})$ con respecto a $\theta$. En algunos (pocos) casos, $p(\theta|\mathcal{D})$ tiene forma analítica explícita.
\end{itemize}
\end{remark}
%\begin{example}
%\beforeitemize
%\begin{itemize}
%    \item Una moneda... %clase 15 min 48 pag8-9
%\end{itemize}
%\end{example}
\subsubsection{Uso de MCMC}
En general $p(\theta|x_1,\dots,x_n)=\displaystyle \prod^n_{i=1}p(x_i|\theta)p(\theta)$ no tiene forma cerrada.
\newline Además para tener su valor numérico, se requiere integrar $\int\prod^n_{i=1}p(x_i|\theta)p(\theta)d\theta=p(x_1,\dots,x_n)$, lo cual puede ser muy costoso.
\newp \textbf{Idea}: samplear de
\begin{alignat*}{2}
    \Pi(\theta) & := p(\theta|x_1,\dots,x_n)\\
     & = \displaystyle \frac{\prod^n_{i=1}p(x_i|\theta)p(\theta)}{p(\mathcal{D})}\,
\end{alignat*}
usando MCMC en $\Theta$, pues con este m\'etodo \textbf{no se requiere conocer ni calcular $p(\mathcal{D})$, y 
basta poder evaluar rápidamente $\prod^n_{i=1}p(x_i|\theta)p(\theta)$}. La cadena de Markov construida con MCMC vive en $\Theta$ y tiene distribución invariante $$\Pi(\theta)\propto\displaystyle\prod^n_{i=1}p(x_i|\theta)p(\theta)\,.$$  
Cabe notar que: 
\begin{itemize}
    \item el m\'etodo puede aplicarse tanto para $\Theta$ discreto como $\Theta=\R^k$ (usando ``paseo aleatorio en $\R^k$ como cadena Markov de base''). %algo más pag 11
    \item la simulación es costosa en general, sobretodo si el espacio de parámetros tiene dimensión muy grande (se requiere correr muchas veces la cadena de MCMC, por mucho tiempo).
\end{itemize}

\newpage
\section{Algoritmos estocásticos en aprendizaje de máquinas}
\subsection{Introducción}
Observamos  $\samples\in\R^d\times\R$ muchos datos (potencialmente infinitos) de dimensión (posiblemente) grande.
\begin{example}[Clasificación de imágenes]
Tenemos un conjunto de imágenes, cada una con una etiqueta. Por ejemplo, la foto de un gato, como se grafica en figura \ref{fig:gato}\footnote{Fuente: \url{https://dongminlee.tistory.com/18}}, puede representarse como un vector al ``aplanar'' la matriz correspondiente a la imagen. Las distintas $k$ clases posibles pueden enumerarse de modo que al concepto \textit{gato} se le asigna un número $y\in\{1,\dots,k\}\subset\N\subset\R$.
\begin{figure}
    \centering
    \includegraphics[scale=0.32]{img/figura_gato.png}
    \caption{Imagen con etiqueta ``gato''.}
    \label{fig:gato}
\end{figure}
De este modo, dado un elemento $i$ de nuestros $n$ datos, $x_i\in\R^d$ es un vector que representa la imagen, mientras que $y\in\{1,\dots,k\}$ corresponde a una etiqueta.
\end{example}
\newp \textbf{Objetivo}: aprender de los datos, i.e., aprender a predecir, al ver un $x$ nuevo, la etiqueta $y$ correspondiente.
\begin{example}[Redes Neuronales]
\label{ejemplo:red_neuronal}
Se busca predecir $y$ mediante
% $$ \hat{y}(x,\theta) = \displaystyle \frac{1}{N}\sum^N_{j=1}\sigma_*(x_j,b_j) \,.$$
$$ \hat{y}(x,\theta) = \displaystyle\sum^N_{j=1}\sigma_*(x_j,\theta_j) \,.$$
donde:
\begin{itemize}
    % \item $\sigma_*:\R^d\times\R^D\to\R$ se llama función de activación.
    \item $N$ es el número de neuronas.
    \item $\theta=(\theta_1,\dots,\theta_N)\in(\R^D)^N$ son los parámetros (``pesos'').
    \newline En general, para $j=1,\dots,N$ tomamos $\theta_j=(a_j,b_j,\omega_j)\in \R\times\R\times\R^D $ y $\sigma_*$ de la forma
    $$ \sigma_*(x,\theta_j)=a_i\sigma(\langle x,\omega_i\rangle+b_i) \,.$$
\end{itemize}
\textbf{¿Como aprende?}
\begin{itemize}
    \item Consideramos $l:\R\times\R\to\R_+$ función de pérdida.
    \newline Ej: $l(y,\hat{y})=(y-\hat{y})^2$
    \item Idealmente, buscamos
    $$ \hat{\theta}=\displaystyle\arg\min_\theta \sum^M_{i=1}l(y_i,\hat{y}(x_i,\theta))\,,$$
    con $M$ grande.
    \item Primera idea: aplicar un algoritmo de optimización para ``entrenar la red'' con los datos conocidos. Es decir, encontrar $\hat{\theta}$ óptimo (o casi).
\end{itemize}
\textbf{Problemas}:
\begin{itemize}
    \item Función objetivo costosa de evaluar, debido a una o varias de las razones siguientes:
    \begin{itemize}
        \item $M$ es grande
        \item $d$ es grande, donde $d$ es la dimensión de $x_i$.
        \item $N$ es grande
    \end{itemize}
    \item Requiere tiempo y memoria computacionales (se requiere usar toda la información en todos los pasos).
    \item Sobreajuste (overfitting): Si entrenamos ``perfectamente'' usando todos los $\samples$ (mínimo global) sobreajustamos y perdemos la ``capacidad de generalización''.
\end{itemize}
\textbf{Segunda idea}: Supongamos $(x_i,y_i)\sim\iid$ de ley $\mu$ no conocida.
\newline En vez de buscar
\begin{alignat*}{2}
        \hat{\theta} & = \displaystyle\arg\min_\theta\sum^M_{i=1}l(y_i,\hat{y}(x_i,\theta)) \\
         & = \arg\min_\theta \frac{1}{M}\sum^M_{i=1}l(y_i,\hat{y}(x_i,\theta), 
    \end{alignat*}
buscamos
$$ \hat{\theta}=\displaystyle\arg\min_\theta \E(l(y,\hat{y}(X,\theta)))$$
con $(X,Y)\sim\mu$.
\newp \textbf{¿Tiene sentido?, ¿de qué sirve si no conocemos $\mu$? ¿qué cambia?}
\end{example}
\vspace{1cm}\\
\subsection{Algoritmo de gradiente estocástico}
El descenso de gradiente estocástico (o \textit{S.G.D.} por sus siglas en inglés) es uno de los pilares del desarrollo reciente del aprendizaje de máquinas y de la inteligencia artificial.
\newp Bibliografía: \textit{Stochastic Approximation} (Robbins \& Monro) \cite{robbins}...... Referencia m\'as actual: \textit{Online Learning and Stochastic Approximations} \cite{bottou}.

\subsubsection{El algoritmo}
Consideramos el problema
$$ \min_\theta \E(f(\theta,X)) \, ,$$
con:
\begin{itemize}
\item $f:\R^d\times \R^k\to\R$
    \item $X\sim\mu$
    \item $(\gamma_t)_{t\in\N}$ pasos o tasa de aprendizaje (\textit{learning rate})
\end{itemize}
 Asumiremos en lo que sigue que $f$ es tal que $F(\theta):= \E(f(\theta,X))$ est\'a bien definida, y que se cumple la relaci\'on $\nabla F(\theta)=\E(\nabla_\theta f(\theta,X)) $ para todo $\theta$.
 
En vez de usar un algoritmo gradiente usual $\theta^{t+1}=\theta^t-\gamma_{t+1}\nabla F(\theta^t)=\theta^t-\gamma_{t+1}\E(\nabla_\theta f(\theta^t,X))$, con $(\gamma_t)_{t\in\N}$ sucesión en $\R_+$ pasos, la idea ser\'a \textbf{considerar observaciones} $x_1,x_2,\dots,x_t \iid \sim\mu$ y el algoritmo definido por las iteraciones: 
$$ \theta^{t+1}:=\theta^t-\gamma_{t+1}\nabla_\theta f(\theta^t,x_{t+1})\,.$$
\begin{remark}
\beforeitemize
\begin{itemize}
    \item El término $\nabla_\theta f(\theta^t,x_{t+1})$ se puede ver como un gradiente exacto perturbado por cierto ruido aleatorio, m\'as precisamente, 
    $$ \nabla_\theta f(\theta^t,x_{t+1})=\E(\nabla_\theta f(\theta^t,X))+\Delta_{t+1}=\nabla F(\theta^t)+\Delta_{t+1},$$
    con $\Delta_{t+1}=\nabla_\theta f(\theta^t,x_{t+1})-\E(\nabla_\theta f(\theta^t,X))$  v.a. centrada. 
    \item En cada paso necesitamos evaluar una sola vez $\nabla_\theta f$ (en un solo dato nuevo).
    \item Podemos usar los datos a medida que llegan.
    \item \textbf{Caso particular importante (literatura de optimización)}
    \newline Se quiere optimizar $\tilde{F}(\theta):=\displaystyle\sum^M_{i=1}\tilde{f}_i(\theta)$, con $\tilde{f}_i:\R^d\to\R$ ($M$ funciones), lo cual es equivalente a
    % \newline $\ssi$ 
    optimizar $F(\theta)=\displaystyle\frac{1}{M}\sum^n_{i=1}\tilde{f}_i(\theta)=\E(f(I,\theta))$ con $I=X\sim \frac{1}{M}\sum^M_{i=1}\delta_i$ (esto es,  $I\in\{1,\dots,M\}$ es un índice  aleatorio  elegido de manera uniforme), y $f(i,\theta)=\tilde{f}_i(\theta)$.  Entonces, 
    $$ \theta^{t+1}:=\theta^t-\gamma_t \nabla_\theta f(\theta^t,I_{t+1})=\theta^t-\gamma_t \nabla_\theta\tilde{f}_{I_{t+1}}(\theta), $$
    con $I_1,I_2,\dots \iid\sim\mathbb{U}(\{1,\dots,M\})$.
\end{itemize}
\end{remark}
El siguiente es un enunciado cl\'asico sobre este tipo de algoritmo, que damos inicialmente sin todos los detalles: 
\begin{theorem}[Sigmund, Robbins, (1951)] 
Bajo hipótesis razonables sobre $f$ (regularidad en $\theta$, integrabilidad de $\nabla_\theta f_+$, cotas), si $\gamma_t\,\substack{\searrow \\ \tiny{t\to\infty}}\, 0$ suficientemente lento la sucesión:
$$ \theta^t\mbox{ }\overset{c.s.}{\substack{\longrightarrow \\t \to \infty}}\mbox{ }\mbox{ Conjunto de puntos críticos de } F(\theta)=\E(f(\theta,X)).$$
Más aún, si $f$ es convexa,  estrictamente en $\theta$ (más algunas hipótesis adicionales), entonces
$$ \theta^t\mbox{ }\overset{c.s.}{\substack{\longrightarrow \\t \to \infty}}\mbox{ }\arg\min_\theta \E(f(\theta,X))\,.$$
\end{theorem}
\begin{remark}

Si bien este tipo de resultados fueron obtenidos por primera vez hace cerca de 70 años, la utilizaci\'on del descenso de gradiente estocástico (o \textit{S.G.D.}  ha sido fuertemente reimpulsada con el desarrollo reciente  del aprendizaje de máquinas y de la inteligencia artificial pues, entre otros motivos, los siguientes: 
\begin{itemize}
    \item Permite ``entrenar'' (estimar) parámetros con datos,  a medida que estos  van llegando.
    \item Permite ``actualizar'' la estimación en tiempo real (a medida que llegan nuevos datos).
    \item Es ``escalable'': el costo es temporal (uso de memoria constante) y proporcional al tamaño del conjunto de datos.
    \item Si bien no está garantizada la convergencia a un óptimo global, muchas veces permite encontrar un mínimo local $\hat{\theta}=\theta^t$ que ``generaliza bien'' (como estimador) en el sentido siguiente:  si $t$ es suficientemente grande,  $X_1,\dots,X_t$ es el conjunto de entrenamiento,  y  $X_{t+i}, i=0,\dots, N$ son datos nuevos,  entonces
    $$ \displaystyle\frac{1}{N}\sum^N_{i=1}f(\theta^t,X_{t+i})\mbox{ es cercano a }\E(f(\theta^t,X)) \mbox{ y } \E(f(\theta^t,X)) \mbox{ es cercano a } \min_\theta \E(f(\theta,X)). $$
\end{itemize}
\end{remark}
   
\begin{remark} Desde el punto de vista pr\'actico: 
\begin{itemize}
\item Típicamente, se buscan mínimos locales ``buenos'', de valores cercanos al global, pero no necesariamente un m\'inimo global.
    \item Se sugiere correr el algoritmo desde muchos $\theta_0$ iniciales y guardar el mejor valor obtenido.
    \item Se sugiere probar adem\'as distintas elecciones de paso $\gamma_t$ y guardar la que de mejores resultados. En la práctica se suele usar $\gamma_t=$constante o constante por tramos ($t$ no tiende a infinito en la vida real)
    \item Hay muchas variantes para acelerar ``convergencia'' o bien para explorar mejor el espacio. Un ejemplo de eso \'ultimo es agregar ``ruido'' o aleatoriedad adicional,  para evitar quedar atrapado en  mínimos locales malos.
    \end{itemize}
\end{remark}
\begin{notation}
En lo que sigue, denotamos $\partial f(\theta,x)=\nabla_\theta\,.
\end{notation}
\subsubsection{Convergencia en el caso convexo}

Para probar la convergencia en el caso convexo necesitaremos herramientas de cálculo estocástico.
\begin{definition}[Martingala]
Sea $(\Omega,\mathcal{F},\P)$ un espacio de probabilidad dotado de una filtración $(\mathcal{F}_t)_{t\in\N}$ (i.e., $\sigma$-álgebras tal que $\forall t\in\N$, $\mathcal{F}_t\subset\mathcal{F}_{t+1}\subset\mathcal{F}$).  Una familia de variables aleatorias se dice martingala si
$$ X_t\in L^1(\mathcal{F}_t) \forall t\in\N$$ y $$\forall t\in\N\espacio \E(X_{t+1}|\mathcal{F}_t)=X_t  \mbox{ c.s.}$$
\begin{notation}
Si en la última ecuación tenemos $\geq$ en vez de igualdad entonces la familia se dice \textbf{sub-martingala}. Si tenemos $\leq$ entonces la llamamos \textbf{sobre-martingala}.
\end{notation}
\end{definition}
En particular usaremos el siguiente teorema del curso de cálculo estocástico, que asumiremos sin demostración.
\begin{theorem}[Convergencia sobre-martingalas]
\label{theorem:sobre-m}
Sea $(X_t)_{t\in\N}$ una sobre-martingala con respecto a $(\mathcal{F}_t)_{t\in\N}$ tal que $\displaystyle\sup_{t\in\N}\E(X_t^-)<\infty$. Entonces $\exists X_\infty\in L^1$ tal que $$ X_t\convcst X_\infty \,.$$
\end{theorem}
\begin{theorem}[Descenso de gradiente estocástico caso convexo]
\label{teo:sgd}
Supongamos que los datos ....  son v.a. i.i.d. de ley $\mu$ definas en un espacio de probabilidad $(\Omega,\mathcal{F},\P)$.  Adem\ás, supongamos que:  
\begin{itemize}
    \item[i)] $\forall \theta\, , f(\theta,\cdot)\in L^1(\mu). \mbox{ Adem\'as }  $f(\cdot,x)\in\mathcal{C}^1 \mu(dx)-c.s.$,  \mbox{ y } $\partial_\theta f(\theta,\cdot)\in L^1(\mu)$
    \item[ii)] $F$ tiene un único mínimo $\theta^*$
    \item[iii)] $\forall \epsilon >0$, $\displaystyle \inf_{\theta:\|\theta-\theta^*\|^2>\epsilon}(\theta-\theta^*)\nabla F(\theta)>0$
    \newline i.e., lejos de $\theta^*$, $F$ decrece estrictamente, uniformemente hacia $F(\theta^* )$
    \item[iv)] $\exists A,B\geq 0$ tal que $\E(\|\partial f(\theta,X)\|^2)\leq A+B\|\theta-\theta^*\|^2, \forall \theta$
    \item[v)] $\sum_{t\in\N}\gamma_t=\infty$, $\sum_{t\in\N}\gamma_t^2<\infty$ \espacio (por ejemplo: $\gamma_t=\frac{1}{t}$).
\end{itemize}
Entonces
$$ \theta_t\mbox{ }\overset{c.s.}{\substack{\longrightarrow \\t \to \infty}}\mbox{ }\theta^*\,.$$

\end{theorem}
\begin{remark}
\beforeitemize
\begin{itemize}
\item (i) $\implies F\in\mathcal{C}^1(\R^d)$ con  $\nabla F=\E(\partial_\theta f(\theta,x))$. 
     \item Si bien no pedimos explícitamente que la función $F$ sea  convexa, haciendo un Taylor en $\theta$ se puede verificar que las hipótesis ii) y iii) se cumplen, si  $F$ es  de clase ${\cal C}^2$ y estrictamente convexa. 
     \item (iii)  impide que el gradiente se ``aplane'' lejos del m\'inimo global. 
    \item (iv) se cumple si por ejemplo $f(\cdot,x)$ es  de clase ${\cal C}^2$ con  $\E(\|Hess_\theta f(\theta,X)\|)<\infty$ % . TERMINAR LA REMARK
\end{itemize}
\end{remark}
\begin{proof}[Demostración de Teorema \ref{teo:sgd} descenso de gradiente estocástico, caso convexo]
\gris \\ Tomemos la función  siguiente como “funci\'on de de Lyapunov" :
$$ h(\theta) = \|\theta-\theta^*\|^2\, , $$
y denotemos $h_t:=h(\theta_t)$. Queremos probar que $h_t$ converge a $0$ c.s., con lo cual tendremos que $\theta_t\mbox{ }\overset{c.s.}{\substack{\longrightarrow \\t \to \infty}}\mbox{ }\theta^*$. En efecto,
$$ \theta_{t+1}-\theta^* = \theta_t-\theta^*-\gamma_t\partial f(\theta_t,x_{t+1}) \,.$$
Luego aplicando $\|\espacio\|^2$,
$$ \|\theta_{t+1}-\theta^*\|^2 = \|\theta_t-\theta^*\|^2-2\gamma_t(\theta_t-\theta^*)\partial f(\theta_{t},x_{t+1})+\gamma^2_t\|\partial f(\theta_t,x_{t+1})\|^2 \,,$$
entonces
$$ h_{t+1}-h_t = -2\gamma_t(\theta_t-\theta^*)\partial f(\theta_t,x_{t+1})+\gamma_t^2\|\partial f(\theta_t,x_{t+1})\|^2 \, .$$
Ahora haremos aparecer una sobre-martingala. Tomemos como filtración la tribu generada por las observaciones, i.e.,$ (\mathcal{F}_t=\sigma(x_0,\dots,x_t))_{t\in\N}$, asumiendo adem\'as que la condici\'on inicial $\theta_0$ es determinista. 

Por inducci\'on se ve f\'acilmente  que  $\theta_t\in L^2(\Omega,\mathcal{F},\P)\,\forall t\in\N$, gracias a la condici\'on iv). Tomando esperanza condicional respecto a $\mathcal{F}_t$:
$$ \E(h_{t+1}-h_t|\mathcal{F}_t)  = -2\gamma_t(\theta_t-\theta^*)\E(\partial f(\theta,X))\big\vert_{\theta = \theta_t}  +\gamma_t^2\E(\|\partial f(\theta,X)\|^2)\big\vert_{\theta = \theta_t} \,,$$
donde usamos que $x_{t+1}\indep \mathcal{F}_t$, con lo cual $\E( G(\theta_t,x_{t+1})|\mathcal{F}_t)) = \E(G(\theta,X))\vert_{\theta = \theta_t}$ para toda $G$ apropiada. Además, por (iv),  $\exists A,B\geq 0$ tal que $\E(\|\partial f(\theta,X)\|^2)\big\vert_{\theta = \theta_t} \leq A+B\|\theta_t-\theta^*\|^2$, por ende
$$ \E(h_{t+1}-h_t|\mathcal{F}_t) \leq -2\gamma_t(\theta-\theta^*)\E(\partial f(\theta,X))\big\vert_{\theta=\theta_t} +A\gamma_t^2+B\gamma_t^2 h_t \,.$$
Como $h_t$ es $\mathcal{F}_t$-medible, puede entrar en la esperanza condicional del lado izquierdo, con lo cual 
\begin{alignat*}{2}
\E(h_{t+1}-(1+\gamma_t^2 B)h_t|\mathcal{F}_t) & \, \leq \, -2\gamma_t(\theta_t-\theta^*)\nabla F(\theta_t)+\gamma_t^2 A .
\end{alignat*}
 Definimos $\mu_t:=\displaystyle\prod^{t-1}_{k=1}\frac{1}{1+\gamma_k^2B}$ y $h'_t:=\mu_th_t$. Multiplicando por $\mu_{t+1}$ queda que
$$ \E(h'_{t+1}-h_t'|\mathcal{F}_t)\leq -2\gamma_t(\theta_t-\theta^*)\nabla F(\theta_t) + \gamma_t^2A\mu_{t+1}   \, \leq \gamma_t^2A\mu_{t} \,,   $$
donde usamos (iii) y el hecho que $\mu_{t+1}\leq \mu_t $. Notar que $\mu_t \convt \mu_\infty\in(0,\infty)$, como puede verse tomando logaritmo y usando que $\sum\gamma_t^2<\infty$. De la desigualdad anterior deducimos que
$$ X_t:=h_t'-\displaystyle\sum^{t-1}_{k=0}A\gamma_t^2\mu_t$$
es una sobre-martingala con respecto a $(\mathcal{F}_t)_{t\in\N}$, es decir $\E(X_{t+1}|\mathcal{F}_t)\leq X_t\,\forall t\in\N$. Para usar el Teorema \ref{theorem:sobre-m} de convergencia de  sobre-martingalas, veamos que es acotada por debajo. En efecto, como $h_t'\geq 0$, tenemos que 
$$ X_t \geq 0 - \displaystyle A\sum^\infty_{k=0}\gamma_t^2\cdot(\sup_{j\in\N}\mu_j)>-\infty.  $$
Así, por el Teorema de convergencia de  sobre-martingalas, 
 $\exists X_\infty\in L^1$ tal que $X_t\convcst X_\infty$. Dado que 
 $$A\sum^N_{t=1}\gamma_t^2\mu_t \mbox{ }\substack{\longrightarrow \\ N\to\infty}\mbox{ } A\sum^\infty_{t=1}\gamma_t^2\mu_t <\infty, $$ se sigue que
$ h'_t \mbox{ }\overset{c.s.}{\substack{\longrightarrow \\t \to \infty}}\mbox{ } h'_\infty \,,$
para cierta v.a. $ h'_\infty\in L^1$, 
y puesto que $\mu_t \convt \mu_0\in(0,\infty)$, deducimos que $$h_t\mbox{ }\overset{c.s.}{\substack{\longrightarrow \\t \to \infty}}\mbox{ }h_\infty$$ para cierta $h_\infty\in L^1$. 
Para concluir el teorema,  veamos que $h_\infty=0\,c.s.$.  Volviendo a la desigualada satisfecha por $\E(h'_{t+1}-h_t'|\mathcal{F}_t)$, vemos que 
$$ 0\leq 2\mu_t \gamma_t(\theta_t-\theta^*)\nabla F(\theta_t)\leq \gamma_t^2A\mu_t + \E(h'_t-h'_{t+1}|\mathcal{F}_t) \,,$$
de donde
\begin{alignat*}{2}
 2\E(\displaystyle\sum^N_{t=0}\gamma_t\mu_t(\theta_t-\theta^*)\nabla F(\theta_t)) & \leq \displaystyle\sum^N_{t=1}\gamma_t^2 A\mu_t+\E(h_0)-\E(h_{N}) \\
& \leq A\sum^{\infty}_{t=1}\gamma_t^2\mu_t+\E(h_0)<+\infty
\end{alignat*}
$$ \therefore \espacio \displaystyle\sum^\infty_{t=0}\gamma_t\mu_t(\theta_t-\theta^*)\nabla F(\theta_t)<\infty \espacio c.s.\,.$$
Puesto que $\mu_t\convt\mu_\infty\in(0,\infty)$, $\mu_t$ est\'a acotada por debajo lejos de $0$, por lo que 
$$  \displaystyle\sum_t\gamma_t(\theta_t-\theta^*)\nabla F(\theta_t)<\infty \, c.s.\,.$$
Usando (v),\espacio $\displaystyle\sum_t\gamma_t=\infty$, con lo cual necesariamente se cumple que 
$$ \displaystyle\liminf_{t\to\infty}(\theta_t-\theta^*)\nabla F(\theta_t) = 0 \,c.s.\,.$$
Para concluir, consideremos $\Omega'_\epsilon=\{h_\infty>\sqrt{\epsilon}\}$ y supongamos que $\P(\Omega'_\epsilon)>0$. Usando (iii) tenemos que $\P(\displaystyle\liminf_{t\to\infty}(\theta_t-\theta^*)\nabla F(\theta_t)>0)\geq \P(\Omega'_{\epsilon/2})  >0$, que es una contradicción. 
$$ \therefore\espacio \P(\Omega'_\epsilon)=0\espacio\forall\epsilon>0,\espacio \Longrightarrow\espacio h_\infty=0\espacio c.s.\,. $$ \findem
\negro
\end{proof}
\vspace{1cm}\\
\subsubsection{Convergencia en caso no-convexo}
\begin{theorem}[Extensión de Descenso de gradiente estocástico (caso no-convexo)]
\label{theorem:sgd_no_conv}
Supongamos:
\begin{itemize}
    \item[i)] $f(\theta,\cdot)\in L^1(\mu)$, $f(\cdot,x)\in\mathcal{C}^3,  \mu(dx)-c.s.$, $\partial^k f(\theta,\cdot)\in L^1(\mu), k=1,2,3$ (luego $F\in\mathcal{C}^3$)
    \item[ii)] $F$ es acotada por debajo
    \item[iii)] $\exists A_k,B_k\geq 0$ tal que $\E(\|\partial f(\theta,X\|^k)\leq A_k+B_k\|\theta\|^2,\espacio k=1,2,3$
    \item[iv)] $\forall M >0$, $\displaystyle \inf_{\theta:\|\theta\|^2>M}\theta\nabla F(\theta)>0$
    \item[v)] $\sum_{t\in\N}\gamma_t=\infty$, $\sum_{t\in\N}\gamma_t^2<\infty$
\end{itemize}
Entonces tenemos lo siguiente (casi seguramente):
\begin{itemize}
    \item[a)] $(\theta_t)_{t\in\N}\subset \mbox{ un compacto }\R^d$
    \item[b)] $\exists F_\infty \in L^1 \mbox{ tal que }F(\theta_t)\convcst0$
    \item[c)] $\nabla F(\theta_t)\convcst 0$
\end{itemize}
\end{theorem}

\begin{remark}
\beforeitemize
\begin{itemize}
    \item El resultado no implica convergencia c.s. de $(\theta_t)_{t\in\N}$\,.
    \item (a)$\implies$ toda subsucesión tiene una subsucesión convergente c.s. $\theta_{t_k}\mbox{ }\substack{\longrightarrow \\ k\to\infty}\mbox{ } \tilde{\theta}$
    \newline (b), (c) $\implies$ por continuidad de $F$ y $\nabla F$,
    $$F(\theta_{t_k})\mbox{ }\substack{\longrightarrow \\ k\to\infty}\mbox{ }F(\tilde{G})\espacio\land\espacio \nabla F(\theta_{t_k})\mbox{ }\overset{c.s.}{\substack{\longrightarrow \\k \to \infty}}\mbox{ }0=\nabla F(\tilde{\theta})\,.$$
    \newline $\therefore\mbox{ si }Crit:\{\theta:\nabla F(\theta)=0\}$, $\forall \epsilon>0$, $\P(dist(\theta_t,Crit)>\epsilon)\mbox{ }\substack{\longrightarrow \\ t\to\infty}\mbox{ }0$\espacio, i.e., la probabilidad de estar a distancia positiva de puntos críticos tiende a $0$.
\end{itemize}
\end{remark}
\vspace{.75cm}\\
\begin{proof}[Esquema de Demostración de Teorema \ref{theorem:sgd_no_conv}, gradiente estocástico, caso no-convexo]
\gris 
% Veremos un esquema de demostración. 
Para más detalles ver Bottou et al. \cite{bottou}.
\newp Usaremos nuevamente el argumento con sobre-martingalas usando una función de Lyapunov ($h(\theta)$). Veremos un confinamiento, o sea que el algoritmo nos llevará a estar en un compacto. Tomamos
$$ h(\theta) = (\|\theta\|^2-M)^2_+=\phi(\|\theta\|^2)\,,$$
donde $\phi(r)=(r-M)_+^2$, y tomamos $h_t:=h(\theta_t)\,\forall t\in\N$. Usando la hipótesis (iii) y un Taylor de orden 1 se demuestra que
$$ \E(h_{t+1}-h_t|\mathcal{F}_t) \leq -2\gamma_t\theta_t\nabla F(\theta_t)\phi'(\|\theta\|^2)+\gamma_t^2(A+Bh_t) \,, $$
con $\phi'(r)=\begin{cases}
0 & r\leq M \\ 2(r-M) & r>M \, .
\end{cases}$ 
\newline Luego por Teorema \ref{theorem:sobre-m}, $h_t\convt h_\infty\in L^1$. Entonces
$$ \displaystyle\sum_{t=0}^\infty\gamma_t\theta_t\nabla F(\theta_t)\phi'(\|\theta_t\|^2)<\infty\espacio c.s. \, , $$
y como por (v) tenemos $\sum\gamma_t=\infty$, necesariamente $\displaystyle\liminf_{t\to\infty} \theta_t\nabla F(\theta_t)\phi'(\|\theta_t\|^2) = 0$. Además, por (iv) $\displaystyle\liminf_{t\to\infty}\phi'(\|\theta_t\|^2)=0\espacio \implies \espacio h_\infty=0\,c.s.\espacio$, $\therefore \|\theta_t\|\leq 2M \espacio c.s.$ para todo $t$ suficientemente grande (aleatorio).
\newp Para demostrar la convergencia de $F(\theta_t)$ usamos un Taylor de orden 1 para obtener 
$$ F(\theta_{t+1})-F(\theta_t)\leq -\gamma_T\partial f(\theta_t,X_{t+1})\nabla F(\theta_t)+\frac{1}{2}\gamma^2_t\|\partial f(\theta_t,X_{t+1})\|^2\partial^2F(\tilde{\theta}_t) \,,$$
luego obtenemos una sobre-martingala al tomar esperanza condicional con respecto a $\mathcal{F}_t$. Luego definiendo $h_t:=F(\theta_t)\espacio(\geq cte)$ se demuestra que
$$ \E(h_{t+1}-h_t|\mathcal{F}_t) \leq -\gamma_t\|\nabla F(\theta_t)\|^2+\gamma_t^2C \,,$$
y nuevamente tenemos que existe $F_\infty\in L^1$ tal que $F(\theta_t)=h_t\convt F_\infty$.

\newp Para la convergencia de $\nabla F(\theta_t)$ a $0$ notemos que
$$ \E(h_{t+1}-h_t|\mathcal{F}_t) \leq -\gamma_t\|\nabla F(\theta_t)\|^2+\gamma_t^2C \espacio \implies \espacio \displaystyle\sum^\infty_{t=1}\gamma_t\|\nabla F(\theta_t)\|^2<\infty \,,$$
con lo cual $\displaystyle\liminf_{t\to\infty}\|\nabla F(\theta_t)\|^2=0$. Para concluir debemos mostrar que $h_t:=\|\nabla F(\theta_t)\|^2$ es convergente. En efecto haciendo un taylor de orden 2 para $\|\nabla F\|^2$ tenemos
$$ \|\nabla F(\theta_{t+1})\|^2-\|\nabla F(\theta_t)\|^2 \leq -2\gamma_t\partial f(\theta,X_{t+1})Hess(F(\theta_t))\nabla F(\theta_t)+\gamma_t^2\|\partial^2 f(\theta,X_{t+1})\| M''' \,,$$
luego
$$ \E(h_{t+1}-h_t|\mathcal{F}_t) \leq 2\gamma_t\|\nabla F(\theta_t)\|^2 +\gamma_t^2M''' \,.$$
Basta notar que tanto $2\gamma_t\|\nabla F(\theta_t)\|^2$ como $\gamma_t^2M'''$ son convergentes, por ende podemos usar nuevamente el Teorema \ref{theorem:sobre-m} para encontrar un $h_\infty$, que necesariamente debe ser $0$ por lo anterior, i.e.,
$$ h_t = \|\nabla F(\theta_t)\|^2\convt 0\,c.s.$$
\findem
\negro
\end{proof}
\begin{remark}[\textbf{Posibilidades a tener en mente}]
% \textbf{Posibilidades a tener en mente}
Al correr el algoritmo podemos encontrarnos en los siguientes casos, graficado en la figura \ref{fig:sgd}.
\begin{itemize}
    \item Mínimo local: Ok, eligiendo el mejor entre muchas corridas.
    \item Máximo local (u otra cosa): Malo, pero fácil de descartar.
    \item Mínimo global: No necesariamente deseable pues se corre el riesgo de sobreajustar. Además es ``inalcanzable''.
    \item Plateau asintótico: Muy malo. No deberia ocurrir bajo la hipótesis (iv).
\end{itemize}
\end{remark}
\begin{figure}
    \centering
    \includegraphics[scale=0.16]{img/Clase_18_pag_09.jpg}
    \caption{Distintos resultados al aplicar S.G.D.}
    \label{fig:sgd}
\end{figure}
% \vspace{1cm}\\
\subsubsection{Tasa de convergencia}
Sobre la tasa de convergencia tenemos las siguientes comparaciones entre descenso de gradiente y descenso de gradiente estocástico:
\newp \textbf{Descenso Gradiente (algoritmo clásico determinista)}
\begin{itemize}
    \item $F$ convexa: $F(\theta_t)-F(\theta^*)=O(\frac{1}{\sqrt{t}})$
    \item Convexa con $\nabla F$ Lipschitz: $F(\theta_t)-\F(\theta^*=O(\frac{1}{t})$
    \item Fuertemente convexa + $\nabla F$ Lipschitz: $F(\theta_t)-F(\theta^*)=O(e^t),\espacio e\in(0,1)$
\end{itemize}
\newp \textbf{Descenso de gradiente estocástico con pasos $\gamma_t\sim \frac{1}{t}$}
\begin{itemize}
    \item $F$ convexa: $F(\theta_t)-F(\theta^*)=O(\frac{1}{\sqrt{t}})$
    \item Convexa con $\nabla F$ Lipschitz: $F(\theta_t)-\F(\theta^*=O(\frac{1}{\sqrt{t}})$
    \item Fuertemente convexa + $\nabla F$ Lipschitz: $F(\theta_t)-F(\theta^*)=O(\frac{1}{t})$
\end{itemize}
\subsection{Variantes de Gradiente Estocástico}
\subsubsection{Gradiente estocástico con Mini-Batch}  % 47:50 clase 18
Es una variante muy utilizada en la práctica. Viene del anglicismo Batch que significa ``lote''.
\newp \textbf{Idea}: en general tenemos
$$ \theta_{t+1}=\theta_t - \gamma_t \partial_\theta f(\theta_t,X_{t+1})=
\theta_t - \gamma_t (\nabla F(\theta_t)+\partial_\theta f(\theta_t,X_{t+1})-\nabla F(\theta_t)\,,$$
donde $\Delta_t=\partial_\theta f(\theta_t,X_{t+1})-\nabla F(\theta_t)$ es aleatorio y lo interpretamos como ruido, con $\E(\Delta_t)=0$. Dicho de otro modo, tenemos un ``gradiente perturbado''.
\newline Notemos que $\partial_\theta f(\theta_t,X_{t+1})$ es un \textbf{estimador insesgado} de $\nabla F(\theta)$. Si tomamos esperanza nos da
$$ \E(\partial_\theta f(\theta_t,X_{t+1}))=\nabla F(\theta_t)\,.$$
Sin embargo también lo es la siguiente expresión, para $m$ fijo:
$$ \displaystyle\frac{1}{m}\sum^m_{i=1}\partial_\theta f(\theta_t,X_{t+i})\,.$$
Más aún, con lo anterior estaremos reduciendo varianza.

\newp \textbf{Descenso de gradiente con mini-batch}:
\newline Sea $m$ fijo (tamaño del mini-batch), y $x_1,\dots,x_t,\dots \iid \sim \mu$, entonces tomamos:
$$ \theta_{t+1}=\theta_t - \gamma_t\cdot \displaystyle\frac{1}{m}\sum^m_{k=1}\partial_\theta f(\theta_t,x_{t+i})$$
con
$$ Var(\displaystyle \frac{1}{m}\sum^m_{k=1}\partial_\theta f(\theta,x_{t+i}))=\frac{Var(\partial_\theta f(\theta,x))}{m}=\frac{\sigma^2}{m}$$
Posee entonces las siguientes ventajas:
\begin{itemize}
    \item El estimador del gradiente posee mucha menos varianza.
    \item Lo anterior implica que hay fluctuaciones más pequeñas.
    \item Aprovecha ``vectorización en computadores'' (GPU, paralelización, ...).
\end{itemize}
\begin{remark}
\beforeitemize
\begin{itemize}
    \item En el caso conexo fuerte con $\gamma_t=\gamma<\frac{2}{\mu}$ se puede probar que
    $$ \E(\|\theta_t-\theta^*\|^2)\leq (1-2 \alpha \mu)^t\|\theta_0-\theta^*\|^2+\frac{\gamma \sigma^2}{m 2\mu}\,,$$
    donde 
    $$ F(\theta)-F(\eta)\geq \nabla F(\theta)(\theta-\eta)+\frac{\mu}{2}\|\theta-\eta\|^2\,.$$
    \item $m$ y $\eta_t$ se deben ajustar conjuntamente.
    \item ``mini-batch'': entre el lote completo de datos (Descenso de gradiente usual) y S.G.D. ``en línea''.
    \item En la práctica, S.G.D. se usa casi siempre con mini-batch, de tamaño fijo $m$ para aprovechar capacidad de cálculo ``paralelo''. El costo de un paso con mini-batch es menor a $m$ por el costo de un paso con un dato.
    \item Tener varianza pequeña no siempre es deseable al comienxo: ayuda a no quedar bloqueado en mínimos locales, por ende tomamos $\gamma_t\searrow$.
    \item Tampoco es necesario ``esperar'' a que las fluctuaciones ``lleguen a $0$'' (con $\gamma_t\to 0$).
    \newline Se puede parar fijando un criterio $|F(\theta_{t_k})-F(\theta_{t_{k+1}})|<\epsilon$\,.
\end{itemize}
\end{remark}

\subsubsection{Más variantes de Gradiente Estocástico}
Existen variantes de descenso de gradiente estocástico que explotan la dinámica determinista subyacente para una convergencia más rápida. Si bien en algunos casos tendremos convergencia, muchas veces estos métodos son heurísticas. Los algoritmos mencionados en esta sección están disponibles en las principales bibliotecas de \textit{python} como \href{https://pytorch.org/docs/stable/optim.html#algorithms}{pytorch}, \href{https://www.tensorflow.org/api_docs/python/tf/keras/optimizers}{tensorflow} y \href{https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/optimizer/index.html}{Apache MXNet}.
\subsubsubsection{Momentum}
El método momentum utiliza el gradiente de la etapa anterior mediante la siguiente recurrencia:
$$ \theta_{t+1} = \theta_t - \gamma_t m_{t+1} \,,$$
con
$$ m_{t+1}=\beta m_t+(1-\beta)\nabla_\theta f(\theta_t,X_{t+1}), \espacio m_0=0\espacio .$$
Esto mantiene algo de las direcciones de descenso anteriormente usadas (``olvido exponencial'').

\subsubsubsection{AdaGrad}  %1:05
AdaGrad consiste en dividir el paso por un factor proporcional a la norma de los gradientes acumulados hasta entonces.
$$ \theta_{t+1}=\theta_t-\displaystyle \frac{\gamma_t}{\sqrt{v_t+\epsilon}}\nabla_\theta f(\theta,X_{t+1}) \,,$$
para $\epsilon$ fijo y con
$$ v_t = \displaystyle \sum^{t-1}_{k=1}\|\nabla_\theta f(\theta,X_k)\|^2\,,\espacio v_1=0\,.$$
Este método va haciendo que los pasos sean más chicos si hemos dado pasos grandes, evitando salir de buenos mínimos locales. Análogamente, al llegar a una zona plana este factor aumentará, de modo que se pueda salir hacia mejores valores.

\subsubsubsection{Variante Estocástica (Stochastic Gradient Langevin Dynamics)}
La siguiente variante ``agrega estocasticidad'' para explorar de mejor manera el espacio y evitar mínimos locales. Tomamos
$$ \theta_{t+1}=\theta_t-\gamma_t\nabla_\theta f(\theta_t,X_{t+1}) + \beta_t \mathcal{N}_t(0,I_d)\,$$
donde las $ \beta_t\sim \mathcal{N}_t(0,I_d)$ son independientes y representas ruido exógeno. Podemos tomar $\beta_t$ tendiendo a cero (por ejemplo $\beta_t=\sqrt{2\gamma t}$) o bien $\beta_t=\beta$ constante. Con este último, siempre se estará agregando ruido, lo cual puede ser útil para recorrer más el espacio de soluciones.
% \newp Estos temas tienen mucha investigación actual. En particular acerca de algoritmos híbridos entre MCMC y Gradiente Estocástico.

\subsubsubsection{Otras variantes}
Existen más variantes, muchas de las cuales son variantes del método de gradiente determinista adaptadas a gradiente estocástico. Algunas de ellas son:
\begin{enumerate}
    \item RMSprop
    \item NAG (\textit{Nesterov Accelerated Gradient})
    \item AdaDelta
    \item Adam (\textit{Adaptive momentum estimation})
    \item Adamax
\end{enumerate}
Hay teoría que indica cuando podría ser mejor uno u otro. En la práctica se recomienda probar con varios para encontrar la mejor alternativa.

\subsection{Introducción a las Redes Neuronales}
% Motivación.
% \subsubsection{Partes básicas}
% \subsubsubsection{Perceptrón}
% \subsubsubsection{Funciones de activación}
% \subsubsection{Perceptrón}
% \subsubsubsection{Grafo computacional}
Recordemos el Ejemplo \ref{ejemplo:red_neuronal}. Queremos aproximar las etiquetas $y$ usando
% $$ \hat{y}(x,\theta) = \displaystyle \frac{1}{N}\sum^N_{j=1}a_j\sigma(w_j^Tx+b_j) $$
$$ \hat{y}(x,\theta) = \displaystyle \sum^N_{j=1}a_j\sigma(w_j^Tx+b_j) $$
Cada uno de los elementos $\sigma(w_j^Tx+b_j)$ define una función, que se le llama \textbf{perceptrón}. 

\newp El perceptrón tiene una motivación biológica: se inspira de las neuronas biológicas. En una red neuronal combinaremos la acción de varios perceptrones, de modo que la transmisión de información entre un perceptrón y otro simula la sinapsis entre neuronas del cerebro. En este caso, la señal transmitida será numérica, y el que un perceptrón esté o no activado se modelará usando la función de activación $\sigma$, similarmente a como se activaría una neurona. El ejemplo más clásico es una función de activación de Heaviside, que retorna $0$ cuando el input es $\leq 0$ y $1$ en caso contrario.

\newp A la función del ejemplo la llamamos una red de perceptrón de una capa. Analizaremos las capacidades aproximativas de esta arquitectura, que mostramos de manera visual en la figura \ref{fig:MLP}.
\begin{figure}
    \centering
    \includegraphics[scale=1.2]{img/MLP.png}
    \caption{Visualización de una red neuronal de una capa}
    \label{fig:MLP}
\end{figure}

% \subsubsection{Aspectos teóricos}
% \subsubsubsection{Teoremas de aproximación universal}
\subsubsection{Teoremas de aproximación universal}
La siguiente subsección está basada en el trabajo de Cybenko (1989) \cite{cybenko}. Para la demostración usaremos elementos de Análisis Funcional, como lo son el Teorema de Hahn-Banach y el Teorema de representación de Riesz. Por otro lado, una versión del mismo resultado demostrado en el mismo periodo por Hornik et al. usa el Teorema de Stone-Weirstrass \cite{hornik}.
\begin{definition}[Función sigmoide]
Decimos que una función $\sigma$ es sigmoide si cumple
$$ \sigma(t) = \begin{cases}
1 & \text{ cuando }t\to\infty \\
0 & \text{ cuando }t\to-\infty
\end{cases}$$
\end{definition}
\begin{example}[Función logística]
A la función $\sigma:\R\mapsto (0,1)$
$$ \sigma(t):= \displaystyle\frac{1}{1+e^{-t}}$$
se le llama función logística. Es el ejemplo más común de una función sigmoide continua.
\\ La activación de Heavisde es también una función sigmoide.
% observacion funcion softmax ???
\end{example}
Asumimos el siguiente resultado sin demostración. Está basado en el uso del teorema de convergencia dominada.
\begin{lemma}
\label{lemma:discrim}
Sea $\sigma$ una función sigmoide, medible y acotada, entonces cumple la siguiente propiedad:
$$ \int_{[0,1]^n}\sigma(y^Tx+\theta)d\mu(x)=0\espacio \forall y\in\R^n,\forall\theta \in\R\espacio \Longrightarrow \espacio  \mu\equiv 0$$
En particular esto es cierto para cualquier función sigmoide continua.
\end{lemma}
\begin{theorem}[Teorema de aproximación universal para activación sigmoide]
\label{teo:AU}
Sea $\sigma$ una función sigmoide continua. Entonces el conjunto de funciones de la forma
$$ G(x) = \displaystyle \sum^N_{j=1}a_j\sigma(w^T_jx+b_j)$$
es denso en $\mathcal{C}([0,1]^n)$, con $\b_j\in\R$, $a_j\in\R\espacio \forall j=1,\dots,N$ y $w_j\in\R^n\espacio\forall j=1,\dots,N$. Dicho de otro modo, para todo $f\in\mathcal{C}([0,1]^n)$ y $\epsilon>0$, $\exists G(x)$ de la forma anterior que cumple
$$ |G(x)-f(x)|<\epsilon\espacio\forall x\in [0,1]^n\,.$$
\end{theorem}
\begin{proof}
\gris
Sea $S\subset\mathcal{C}([0,1]^n)$ el conjunto de las funciones de la forma $\displaystyle \sum^N_{i=j}a_j\sigma(w^T_jx+b_j)$, con  $b_j\in\R$, $a_i\in\R\espacio \forall j=1,\dots,N$ y $w_j\in\R^n\espacio\forall j=1,\dots,N$, y notemos que es un subespacio lineal de $\mathcal{C}([0,1]^n)$. Veamos que $\bar S$ (la cerradura de $S$) es $\mathcal{C}([0,1]^n)$. Por contradicción, si asumimos lo contrario entonces $\bar S$ es un subespacio cerrado propio de $\mathcal{C}([0,1]^n)$. Luego por Hahn-Banach existe un funcional lineal $L$ en $\mathcal{C}([0,1]^n)$, no nulo, pero que se anula en $\bar S$ (y por ende en $S$).

\newp Por el teorema de representación de Riesz, el funcional $L$ debe ser de la forma
$$ L(h) = \displaystyle\int_{[0,1]^n}h(x)d\mu(x)$$
para $\mu$ medida en $[0,1]^n$ y $\forall h\in \mathcal{C}([0,1]^n)$. Luego en particular se cumple que
$$\int_{[0,1]^n}\sigma(y^Tx+\theta)d\mu(x)=0 \espacio \forall y\in\R^n,\forall\theta \in\R$$. Por lema \ref{lemma:discrim}, $\mu\equiv 0$, con lo cual el funcional lineal $L$ es nulo. Como esto es una contradicción, $\bar S = \mathcal{C}([0,1]^n)$, i.e., $S$ es denso en $\mathcal{C}([0,1]^n)$.\findem
\negro
\end{proof}
\begin{remark}
A las funciones que cumplen la propiedad del lema \ref{lemma:discrim} se les llama funciones discriminatorias. El resultado vale entonces para cualquier función discriminatoria continua. Además, se puede generalizar además a funciones sigmoide medibles y funciones sigmoide arbitrarias agregando ciertas restricciones adicionales.

Más aún, existen variados resultados que proponen cotas para el tamaño de la red neuronal dependiendo de la función a aproximar. La versión del Teorema de Aproximación Universal que hemos visto corresponde al caso de ancho arbitrario, donde ancho se refiere a la cantidad de perceptrones de la capa. Análogamente existen versiones de ancho acotado y profundidad arbitraria, donde profundidad se refiere a la cantidad de capas.
\end{remark}
% \begin{remark}[Generalizaciones del Teorema \ref{teo:AU}]
% ...
% \end{remark}

El siguiente resultado nos muestra que el Teorema de aproximación universal nos sirve para implementar un clasificador basado en separaciones del espacio con una red de una sola capa. Primero sea $P_1,\dots,P_k$, $k\in\N$ una partición de $[0,1]^n$, definiremos una función de decisión como un $f:[0,1]^n\mapsto \{1,\dots,k\}$ que cumple
$$ f(x) = j \espacio \ssi \espacio x\in P_j \,.$$
\begin{theorem}
Sea $\sigma$ una función sigmoidal continua. Sea $f$ una función de decisión para alguna partición $\mu$-medible y finita, donde $\mu$ es la medida de Lebesgue en $[0,1]^n$. Para cualquier $\epsilon>0$ existe una suma finita de la forma
$$ G(x) = \displaystyle \sum^N_{j=1}a_j\sigma(w^T_jx+b_j) \,,$$
y un conjunto $D\subset [0,1]^n$ tal que $\mu(D)\geq 1-\epsilon$ y que cumple
$$ |G(x)-f(x)|<\epsilon\espacio \text{ para }x\in D \,.$$
\end{theorem}
\begin{proof}
\gris
Recordemos el Teorema de Lusin, que nos dice que una función finita $\mu$-c.s. es medible si y solo si es una función continua en casi todo su dominio. Luego existe una función continua $h$ y un conjunto $D$ tal que $\mu(D)\geq 1-\epsilon$ de modo que 
$$ h(x)=f(x) \espacio \forall x \in D \,.$$
Como $h$ es continua, por Teorema \ref{teo:AU}, existe una función $G$ de la forma $$G(x)=\displaystyle \sum^N_{i=j}a_j\sigma(w^T_jx+b_j)\,,$$
con  $b_j\in\R$, $a_i\in\R\espacio \forall j=1,\dots,N$ y $w_j\in\R^n\espacio\forall j=1,\dots,N$ que satisface
$$ |G(x)-h(x)|<\epsilon \espacio \forall x\in [0,1]^n\,.$$
Entonces para $x\in D$ tendremos $|G(x)-f(x)|<\epsilon$\,. \findem
\negro
\end{proof}

\subsubsection{Entrenamiento de una red neuronal con Gradiente Estocástico}
Consideremos el caso en el que tengamos una sucesión de $n$ puntos de datos $(x_1,y_1),\dots,(x_n,y_n)\subset \R^d\times\R$. Sea además, $l:\R\times\R\mapsto\R$ una función de error, y denotemos:
$$ \mathcal{L}(\theta) = \displaystyle\frac{1}{N}\sum_{i=1}^n l(y_i,\hat y_i(x_i,\theta)) \,.$$ 
Usaremos el Algoritmo de Descenso de Gradiente Estocástico para minimizar esta función de pérdida. Sin embargo notemos que esto no es tarea fácil, pues involucra el cómputo del gradiente de $\mathcal{L}$, considerando que la dimensionalidad de $\theta$ corresponde al número de parámetros totales de la red neuronal (potencialmente muy grande). Estudiemos como se realiza esto en la práctica.
% \subsubsubsection{Funciones de error}
\subsubsubsection{Cálculo de gradiente con \textit{Back-propagation}}
% \subsubsubsection{Tensores}
Back-Propagation es un algoritmo para calcular derivadas en contextos generales, aunque es principalmente usada en el contexto de redes neuronales. Es particularmente útil cuando tenemos cálculos sucesivos, lo cual es el caso de las redes neuronales, más aún si constan de varias capas de profundidad. Veamos un ejemplo para inspirar su uso.

\newp Sean $\theta$ el conjunto de parámetros de una red neuronal que queremos ajustar. Sabemos que para Gradiente Estocástico tendremos que saber computar el gradiente en un punto para cualquier parámetro en $\theta$. Consideremos una pérdida cuadrática y una aproximación de un dato $y_i$ de la forma
$$ \hat y_i = \displaystyle \sum^N_{j=1} a_j\sigma(\omega^T_j x_i+b_j) $$
Tratemos de calcular la derivada de $l_i=(y_i-\hat y_i)^2$ con respecto a $a_k$, con $k\in\{1,\dots,N\}$:
$$ \frac{\partial l_i}{\partial a_k} = \frac{\partial (y_i-\hat y_i)^2}{\partial a_k} = 2(y_i-\hat y_i) \frac{\partial (y_i-\hat y_i)}{\partial a_k} \,.$$
Notemos que hasta este punto, sólo usamos la regla de la cadena. Ahora como $y_i$ es un punto fijo de dato que no depende de $a_k$, nos queda:
$$ \frac{\partial l_i}{\partial a_k} = 2(y_i-\hat y_i) \frac{\partial (y_i-\hat y_i)}{\partial a_k} =  -2(y_i-\hat y_i) \frac{\partial\hat y_i}{\partial a_k} =  -2(y_i-\hat y_i) \frac{\partial[\sum^N_{j=1} a_j\sigma(\omega^T_j x_i+b_j)]}{\partial a_k} = -2(y_i-\hat y_i)\sigma(\omega_k^Tx_i+b_k)$$
Denotaremos ahora las variables auxiliares siguientes. El objetivo será simplificar lo anterior:
\begin{itemize}
    % \item $l_i = l(y_i,\hat y_i)$
    \item $z_i = (y_i-\hat y_i)$
    \item $u^j_i = a_j\sigma(\omega^T_j x_i+b_j)$
    \item $v^j_i = \omega^T_j x_i+b_j$
\end{itemize}
Podemos reescribir el cálculo de la última linea usando las variables auxiliares:
$$ \frac{\partial l_i}{\partial a_k} = 2(y_i-\hat y_i) \frac{\partial[\sum^N_{j=1} a_j\sigma(\omega^T_j x_i+b_j)]}{\partial a_k} = 2z_i \frac{\partial[\sum^N_{j=1} u^j_i]}{\partial a_k} = 2z_i \frac{\partial u_i^k}{\partial a_k} \,.$$
En este punto podemos notar que 
$$ \frac{\partial u_i^k}{\partial a_k} = \frac{\partial[a_k \sigma(v_i^j)]}{\partial a_k} = \sigma(v_i^j) \espacio \therefore \espacio \frac{\partial l_i}{\partial a_k}=2z_i \sigma(v_i^j)$$
Si bien agregar las variables auxiliares nos hace pesada la notación, la idea es guardar la información paso por paso a medida que los valores ``avanzan'' en la red neuronal hasta llegar a la predicción. Esto a la larga será útil, pues se nos facilitan los cálculos, como vimos en derivar $\frac{\partial l_i}{\partial a_k}$. Por otro lado notemos que podemos llegar a $l_i$ de manera acumulativa desde los parámetros usando las variables auxiliares.
\begin{itemize}
    \item $l_i = z_i^2$
    \item $z_i = (y_i-\sum^N_{j=1}u_i^j)$
    \item $u^j_i = a_j\sigma(v_i^j)$
    \item $v_i^j = \omega^T_j x_i+b_j$
\end{itemize}
Luego usando la regla de la cadena podemos deducir que
$$ \frac{\partial l_i}{\partial a_k} = \frac{\partial l_i}{\partial z_i}\frac{\partial z_i}{\partial u_i^j}\frac{\partial u_i^j}{\partial a_k} = 2z_i \cdot 1 \cdot \sigma(v_i^j) \,$$
i.e., hicimos el mismo cálculo anterior pero basado en cálculos simples de derivadas usando las variables auxiliares. Veamos otro ejemplo. Calcularemos $\frac{\partial l_i}{\partial b_k}$ usando este principio:
$$ \frac{\partial l_i}{\partial b_k} = \frac{\partial l_i}{\partial z_i}\frac{\partial z_i}{\partial u_i^j}\frac{\partial u_i^j}{\partial v_i^j}\frac{\partial v_i^j}{\partial b_k} = 2z_i \cdot 1 \cdot a_k\sigma'(v_i^j)\cdot 1 = 2 (y_i-\sum^N_{j=1}u_i^j)a_k\sigma'(\omega^T_j x_i+b_j) \,$$
donde $\sigma'$ es la derivada de la función de activación $\sigma$ (unidimensional). \newp Notemos la facilidad con la que llegamos a la expresión final, y más aún, notemos que muchas de las derivadas parciales que usamos también se usaron para computar $\frac{\partial l_i}{\partial a_k}$. Se puede hacer un cálculo análogo para llegar a $\frac{\partial l_i}{\partial \omega_k}$, con lo cual tendríamos calculado el gradiente de $l_i$. 
\newp El algoritmo Back-Propagation consiste en el uso exhaustivo de la \textbf{regla de la cadena}. Calculamos derivadas parciales paso por paso y las guardamos para usarlos en cada parámetro. De este modo que sólo computaremos derivadas simples y guardarlos evitará hacer cálculos redundantes. %La única restricción que tenemos para usar Back-Propagation es que no hayan cálculos circulares.
En nuestro caso no era imposible calcular el gradiente, pero a medida que las redes neuronales sean más profundas, más profundas serán las dependencias entre variables y por ende más cálculos se ahorrarán si usamos regla de la cadena, además de simplificar el computo.

\newp Las bibliotecas que implementan redes neuronales, como \textit{pytorch} implementan este algoritmo implícitamente, de modo que al definir las operaciones entre variables, se guardan las derivadas parciales respectivas. Durante el entrenamiento, una vez que la información pasa a través de la red neuronal para computar una predicción, se computará un error (\textit{forward}). Enseguida, se computa el gradiente usando las operaciones que estaban guardadas, haciendo fluir la información en reversa hasta tener el valor de cada derivada parcial (\textit{backward}). Esto nos da una manera eficiente de usar descenso de gradiente estocástico.

\subsubsubsection{Problemas y prácticas usuales}
% El uso de redes neuronales ha ido evolucionando con el tiempo. Muchas de las innovaciones son resultado de experimentos que han funcionado bien en la práctica.
Hasta ahora hemos observado las capacidades teóricas de las redes neuronales de una sola capa. Si bien poder aproximar funciones continuas parece una capacidad muy valiosa, no siempre estaremos intentando aproximar funciones con buenas propiedades. Es más, muchas veces no tendremos ninguna garantía de que las funciones que estamos tratando de aproximar existan. Tener un cierto grado de profundidad nos ayuda a aumentar el poder de expresividad de los valores de salida. A su vez podremos disminuir el número de perceptrones en cada capa.
% figura
\newp Lamentablemente, un problema que se encuentra al entrenar redes neuronales profundas es el \textbf{desvanecimiento del gradiente}. Esto sucede cuando el valor de las derivadas parciales es pequeño, evitando que puedan actualizarse valores de capas iniciales usando descenso de gradiente. Dentro de las soluciones que se han explorado para evitar este problema está el uso de la función de activación \textit{ReLU} (\textit{rectified linear unit}), que está dada por:
$$ ReLU(t) = \begin{cases}
t & \text{ cuando }t\geq0 \\
0 & \text{ cuando }t<0
\end{cases}$$
Si bien esta función no es sigmoide y es no-acotada, es mucho más fácil de computar, es invariante a la escala. El problema de que no sea derivable en $0$ se arregla simplemente asignando $1$ o $0$ como valor de la derivada en aquel punto. Por último notar que podemos lograr funciones que si son sigmoides al acoplar dos funciones \textit{ReLU} o más. Un ejemplo de esto es la función $\sigma$ siguiente:
$$ \sigma(t) = \frac{1}{2}(ReLU(t+1)-1-(ReLU(t-1)-1))\,,$$
que es una función sigmoide continua, y por ende cumple con las hipótesis del Teorema \ref{teo:AU}. En la figura \ref{fig:fun_activ} se muestran algunas de las funciones de activación que se han mencionado en esta subsección.
\begin{images}[\label{fig:fun_activ}]{Ejemplos de funciones de activación}
	\addimage{RRNN_escalon}{width=6.5cm}{Activación de Heaviside}
	\addimage{RRNN_logistic}{width=6.5cm}{Logística}
	\imagesnewline
	\addimage{RRNN_ReLU}{width=6.5cm}{$ReLU(t)$}
	\addimage{RRNN_two_ReLUs}{width=6.5cm}{$\frac{1}{2}(ReLU(t+1)-1-(ReLU(t-1)-1))$}
\end{images}


% \subsubsection{Otros tipos de redes neuronales}
% \subsubsubsection{Redes convolucionales}
% \subsubsubsection{Redes recurrentes}

% \subsubsection{Aplicaciones en matemáticas}

\newpage
\section{Movimiento Browniano y difusiones}
\input{unidad_final}

\newpage
%%%%%%%%%%%%%% cosas recurrentes %%%%%%%%%%%%%%%%%%%
%Sea $X:\Omega \longrightarrow E $ v.a., 
% $f:(E,\Sigma)\longrightarrow (\mathbb{R},\mathcal{B(\mathbb{R})})$
% $f\in L^1(E,\Sigma,\mu)$
% $$ \mathbb{E}(f(X)) = \langle \mu,f \rangle, \forall f \in L^1(\mu)$$
% Sea $(\Omega,\mathcal{F},\mathbb{P})$ e.d.p.
% $\mu_n \mbox{ }\substack{\Longrightarrow \\n \to \infty}\mbox{ } \mu$ convergencia débil
% \mbox{ }\substack{\longrightarrow \\ n\to\infty}\mbox{ } convergencia normal
% $X_n\mbox{ }\overset{ley}{\substack{\longrightarrow \\n \to \infty}}\mbox{ }X$ conv en ley
% \color{red}RELLENAR\color{mygray}
\begin{references}
\bibitem{billing} Dudley, R. M. (1971). P. Billingsley, Convergence of probability measures. Bulletin of the American Mathematical Society, 77(1), 25-27.
\bibitem{glass} Glasserman, P. (2013). Monte Carlo methods in financial engineering (Vol. 53). Springer Science \& Business Media.
\bibitem{pardoux} Pardoux, É. (2006). Processus de Markov et applications. Algorithmes, Réseaux, Génome et Finance.
\bibitem{asm} Asmussen, S., \& Glynn, P. W. (2007). Stochastic simulation: algorithms and analysis (Vol. 57). Springer Science \& Business Media.
\bibitem{norris} Norris, J. R. (1998). Markov chains (No. 2). Cambridge university press.
\bibitem{metro} Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., \& Teller, E. (1953). Equation of state calculations by fast computing machines. The journal of chemical physics, 21(6), 1087-1092.
\bibitem{robbins} Robbins, H., \& Monro, S. (1951). A Stochastic Approximation Method. The Annals of Mathematical Statistics, 22(3), 400–407.
\bibitem{bottou} Bottou, L. (1998). On-line learning and stochastic approximations. In On-Line Learning in Neural Networks, 9–42.
\bibitem{cybenko} ] G. Cybenko. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4), 303–314.
\bibitem{hornik} K. Hornik, M. Stinchcombe, \& H. White. (1989). Multilayer feedforward networks are universal approximators. Neural networks, 2(5), 359–366.
\bibitem{karatzas} Karatzas, I., \& Shreve, S. (2012). Brownian motion and stochastic calculus (Vol. 113). Springer Science & Business Media.
\end{references}

\newpage
\begin{anexo}

\section{Laboratorios}
\subsection[Laboratorio 1 - Monte Carlo y eficiencia de simulación]{Laboratorio 1}
\input{laboratorios/laboratorio_1}

\newpage
\subsection[Laboratorio 2 - Reducción de varianza y cadenas de Markov]{Laboratorio 2}
\input{laboratorios/laboratorio_2}

\newpage
\subsection[Laboratorio 3 - Algoritmos estocásticos usando cadenas de Markov]{Laboratorio 3}
\input{laboratorios/laboratorio_3}

\newpage
\subsection[Laboratorio 4.1 - Descenso de gradiente estocástico y aplicaciones]{Laboratorio 4.1}
\input{laboratorios/laboratorio_4.1}

\newpage
\subsection[Laboratorio 4.2 - Integral estocástica y EDEs]{Laboratorio 4.2}
\input{laboratorios/laboratorio_4.2}
\end{anexo}

% FIN DEL DOCUMENTO
\end{document}